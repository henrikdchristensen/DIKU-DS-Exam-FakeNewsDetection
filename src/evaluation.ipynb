{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FakeNewsCorpus test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y_true, name=None):\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"test_acc\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred),\n",
    "            \"recall\": recall_score(y_true, y_pred),\n",
    "            \"f1\": f1_score(y_true, y_pred), \n",
    "        })\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to csv\n",
    "column_names_list = ['id', 'type', 'content', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', \n",
    "            'barely true counts', 'false counts', 'half true counts', 'mostly true counts', 'pants on fire counts', 'context']\n",
    "df = pd.read_table(\"../datasets/liar_dataset/train.tsv\", header=None, names = column_names_list)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.to_csv(\"../datasets/liar_dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:01<00:00, 8996.04it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 283271.82it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 34601.03it/s]\n",
      "100%|██████████| 10240/10240 [00:01<00:00, 5155.23it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 812717.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 3.744737148284912\n"
     ]
    }
   ],
   "source": [
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "Clean_data(\"../datasets/liar_dataset/train.csv\", \"../datasets/liar_dataset/train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 1089236.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.2721900939941406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/liar_dataset/train_cleaned.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/liar_dataset/train_cleaned_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 23857.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.5099701881408691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:14<00:00, 711.57it/s]\n",
      "/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "100%|██████████| 10240/10240 [00:09<00:00, 1027.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 29.30839991569519\n"
     ]
    }
   ],
   "source": [
    "#Vectorisation \n",
    "\n",
    "importlib.reload(pp)\n",
    "\n",
    "def Get_unique_words(file):\n",
    "    unique_words = pp.Generate_unique_word_list()\n",
    "    pp.apply_pipeline(file, [(unique_words, None)], progress_bar=True)\n",
    "    return unique_words\n",
    "\n",
    "unique_words = Get_unique_words(\"../datasets/liar_dataset/train_cleaned_bin.csv\")\n",
    "\n",
    "unique_words_list = unique_words.get_unique_words(0,1)\n",
    "\n",
    "def Vectorize_content(file, new_file, unique_words):\n",
    "    pp.apply_pipeline(file, [\n",
    "            (pp.Create_word_vector(unique_words), \"content\"),\n",
    "            (pp.Save_numpy_arr(), \"content\")\n",
    "        ], \n",
    "        new_file=new_file,\n",
    "        progress_bar=True)\n",
    "\n",
    "Vectorize_content(\"../datasets/liar_dataset/train_cleaned_bin.csv\", \"../datasets/liar_dataset/train_vectorized.csv\", unique_words_list)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d59e2650f512131a22a150c5c14fd943a8bb8eb74e25536a1fe4b78e0dd08d99"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 ('fake_news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
