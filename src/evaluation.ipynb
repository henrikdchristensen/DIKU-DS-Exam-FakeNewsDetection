{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "from tqdm import tqdm\n",
    "import importlib\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y_true, name=None):\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred),\n",
    "            \"recall\": recall_score(y_true, y_pred),\n",
    "            \"f1\": f1_score(y_true, y_pred), \n",
    "        })\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FakeNewsCorpus test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to csv\n",
    "column_names_list = ['id', 'type', 'content', 'subject', 'speaker', 'speaker job title', 'state info', 'party affiliation', \n",
    "            'barely true counts', 'false counts', 'half true counts', 'mostly true counts', 'pants on fire counts', 'context']\n",
    "df = pd.read_table(\"../datasets/liar_dataset/train.tsv\", header=None, names = column_names_list)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.to_csv(\"../datasets/liar_dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:01<00:00, 9959.79it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 331156.50it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 36975.63it/s]\n",
      "100%|██████████| 10240/10240 [00:01<00:00, 5444.37it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 792619.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 3.46828293800354\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "Clean_data(\"../datasets/liar_dataset/train.csv\", \"../datasets/liar_dataset/train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 1212411.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.19042396545410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/liar_dataset/train_cleaned.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/liar_dataset/train_cleaned_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 23302.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.4933781623840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:16<00:00, 603.36it/s]\n",
      "/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "100%|██████████| 10240/10240 [00:07<00:00, 1288.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 29.131235122680664\n"
     ]
    }
   ],
   "source": [
    "#Vectorisation \n",
    "\n",
    "importlib.reload(pp)\n",
    "\n",
    "def Get_unique_words(file):\n",
    "    unique_words = pp.Generate_unique_word_list()\n",
    "    pp.apply_pipeline(file, [(unique_words, None)], progress_bar=True)\n",
    "    return unique_words\n",
    "\n",
    "unique_words = Get_unique_words(\"../datasets/liar_dataset/train_cleaned_bin.csv\")\n",
    "\n",
    "unique_words_list = unique_words.get_unique_words(0,1)\n",
    "\n",
    "def Vectorize_content(file, new_file, unique_words):\n",
    "    pp.apply_pipeline(file, [\n",
    "            (pp.Create_word_vector(unique_words), \"content\"),\n",
    "            (pp.Save_numpy_arr(), \"content\")\n",
    "        ], \n",
    "        new_file=new_file,\n",
    "        progress_bar=True)\n",
    "\n",
    "Vectorize_content(\"../datasets/liar_dataset/train_cleaned_bin.csv\", \"../datasets/liar_dataset/train_vectorized.csv\", unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d59e2650f512131a22a150c5c14fd943a8bb8eb74e25536a1fe4b78e0dd08d99"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 ('fake_news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
