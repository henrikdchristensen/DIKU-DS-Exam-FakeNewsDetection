{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/_C.cpython-311-darwin.so, 0x0002): Library not loaded: @loader_path/libtorch_cpu.dylib\n  Referenced from: <BB83D0BC-DB63-39D8-A478-116ADDABD1B1> /Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_python.dylib\n  Reason: tried: '/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS@loader_path/libtorch_cpu.dylib' (no such file), '/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/usr/local/lib/libtorch_cpu.dylib' (no such file), '/usr/lib/libtorch_cpu.dylib' (no such file, not in dyld cache)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpipeline\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mml\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#from tqdm import tqdm\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimportlib\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/FakeNews/src/models.py:17\u001b[0m\n\u001b[1;32m     <a href='file:///Users/linneaandersen/Desktop/FakeNews/src/models.py?line=13'>14</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m GridSearchCV\n\u001b[1;32m     <a href='file:///Users/linneaandersen/Desktop/FakeNews/src/models.py?line=15'>16</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mppb\u001b[39;00m  \u001b[39m# pytorch-pretrained-bert\u001b[39;00m\n\u001b[0;32m---> <a href='file:///Users/linneaandersen/Desktop/FakeNews/src/models.py?line=16'>17</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/linneaandersen/Desktop/FakeNews/src/models.py?line=18'>19</a>\u001b[0m \u001b[39m# The number of rows to train the model\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/linneaandersen/Desktop/FakeNews/src/models.py?line=19'>20</a>\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m1000000\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py:229\u001b[0m\n\u001b[1;32m    <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=226'>227</a>\u001b[0m     \u001b[39mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=227'>228</a>\u001b[0m         _load_global_deps()\n\u001b[0;32m--> <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=228'>229</a>\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=230'>231</a>\u001b[0m \u001b[39m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=231'>232</a>\u001b[0m \u001b[39m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/__init__.py?line=232'>233</a>\u001b[0m \u001b[39mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/_C.cpython-311-darwin.so, 0x0002): Library not loaded: @loader_path/libtorch_cpu.dylib\n  Referenced from: <BB83D0BC-DB63-39D8-A478-116ADDABD1B1> /Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_python.dylib\n  Reason: tried: '/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS@loader_path/libtorch_cpu.dylib' (no such file), '/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib' (no such file), '/usr/local/lib/libtorch_cpu.dylib' (no such file), '/usr/lib/libtorch_cpu.dylib' (no such file, not in dyld cache)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "#from tqdm import tqdm\n",
    "import importlib\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(models, X, y_true, name=None):\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"accuracy\": accuracy_score(y_true, y_pred),\n",
    "            \"precision\": precision_score(y_true, y_pred),\n",
    "            \"recall\": recall_score(y_true, y_pred),\n",
    "            \"f1\": f1_score(y_true, y_pred), \n",
    "        })\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FakeNewsCorpus test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIAR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to csv\n",
    "column_names_list = ['id', 'type', 'content', 'subjects', 'speaker', 'speaker job title', 'state info', 'party', \n",
    "            'barely true counts', 'false counts', 'half true counts', 'mostly true counts', 'pants on fire counts', 'context']\n",
    "df = pd.read_table(\"../datasets/liar_dataset/train.tsv\", header=None, names = column_names_list)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "df.to_csv(\"../datasets/liar_dataset/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:01<00:00, 9959.79it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 331156.50it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 36975.63it/s]\n",
      "100%|██████████| 10240/10240 [00:01<00:00, 5444.37it/s]\n",
      "100%|██████████| 10240/10240 [00:00<00:00, 792619.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 3.46828293800354\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "Clean_data(\"../datasets/liar_dataset/train.csv\", \"../datasets/liar_dataset/train_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to binary labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 1212411.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.19042396545410156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/liar_dataset/train_cleaned.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/liar_dataset/train_cleaned_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:00<00:00, 23302.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 0.4933781623840332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10240/10240 [00:16<00:00, 603.36it/s]\n",
      "/Users/linneaandersen/miniconda3/envs/fake_news/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:75: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n",
      "100%|██████████| 10240/10240 [00:07<00:00, 1288.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 29.131235122680664\n"
     ]
    }
   ],
   "source": [
    "#Vectorisation \n",
    "\n",
    "importlib.reload(pp)\n",
    "\n",
    "def Get_unique_words(file):\n",
    "    unique_words = pp.Generate_unique_word_list()\n",
    "    pp.apply_pipeline(file, [(unique_words, None)], progress_bar=True)\n",
    "    return unique_words\n",
    "\n",
    "unique_words = Get_unique_words(\"../datasets/liar_dataset/train_cleaned_bin.csv\")\n",
    "\n",
    "unique_words_list = unique_words.get_unique_words(0,1)\n",
    "\n",
    "def Vectorize_content(file, new_file, unique_words):\n",
    "    pp.apply_pipeline(file, [\n",
    "            (pp.Create_word_vector(unique_words), \"content\"),\n",
    "            (pp.Save_numpy_arr(), \"content\")\n",
    "        ], \n",
    "        new_file=new_file,\n",
    "        progress_bar=True)\n",
    "\n",
    "Vectorize_content(\"../datasets/liar_dataset/train_cleaned_bin.csv\", \"../datasets/liar_dataset/train_vectorized.csv\", unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add features "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d59e2650f512131a22a150c5c14fd943a8bb8eb74e25536a1fe4b78e0dd08d99"
  },
  "kernelspec": {
   "display_name": "Python 3.11.0 ('fake_news')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
