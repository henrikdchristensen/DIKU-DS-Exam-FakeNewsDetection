{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 14:44:16.356131: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import preprocessing\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.models import Model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GlobalAveragePooling1D\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "# from transformers import_version_; print(_version_)\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, BertForMaskedLM\n",
    "import torch\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate dataset and apply binary labels to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 1177579.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20000 rows\n",
      "finish time: 0.402724027633667\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "\n",
    "data = pp.apply_pipeline(\n",
    "    '../datasets/sample/dataset.csv',\n",
    "    [(pp.Binary_labels(), 'type', 'typeContent') ],\n",
    "    # classes=[True, False],\n",
    "    total_rows=5000,\n",
    "    new_file='../datasets/sample/dataset_bin_raw.csv',\n",
    "    progress_bar=True,\n",
    ")\n",
    "\n",
    "# write panda frame data to ../datasets/big/dataset_bin.csv\"\n",
    "\n",
    "raw_sample_file = '../datasets/sample/dataset_bin_raw.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>scraped_at</th>\n",
       "      <th>inserted_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>keywords</th>\n",
       "      <th>meta_keywords</th>\n",
       "      <th>meta_description</th>\n",
       "      <th>tags</th>\n",
       "      <th>summary</th>\n",
       "      <th>source</th>\n",
       "      <th>typeContent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6301449</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>\\n\\nTo the Editor:\\n\\nRe ''Drop Out of the Col...</td>\n",
       "      <td>2018-02-11 00:40:10.316783</td>\n",
       "      <td>2018-02-11 00:14:20.346838</td>\n",
       "      <td>2018-02-11 00:14:20.346871</td>\n",
       "      <td>Time to Scrap the Electoral College?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Presidential Elections (US)', 'Electoral Col...</td>\n",
       "      <td>&lt;br&gt;To the Editor:\\n&lt;p&gt;\\n  Re ''Drop Out of th...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3676064</td>\n",
       "      <td>nationalreview.com</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.nationalreview.com/postmodern-conse...</td>\n",
       "      <td>So I’ve written for another channel my advice ...</td>\n",
       "      <td>2017-11-27T01:14:42.983556</td>\n",
       "      <td>2018-02-08 19:18:34.468038</td>\n",
       "      <td>2018-02-08 19:18:34.468066</td>\n",
       "      <td>Antonin Scalia, Donald Trump, Tyler Cowen &amp; Co...</td>\n",
       "      <td>Peter Augustine Lawler</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['Peter Augustine Lawler']</td>\n",
       "      <td>Senate Republicans should enter into the battl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5192554</td>\n",
       "      <td>infowars.com</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>https://www.infowars.com/soldiers-nearly-kille...</td>\n",
       "      <td>David Gutierrez\\n\\nNatural News\\n\\nNovember 3,...</td>\n",
       "      <td>2017-12-09T22:10:08.302997</td>\n",
       "      <td>2018-02-08 19:18:34.468038</td>\n",
       "      <td>2018-02-08 19:18:34.468066</td>\n",
       "      <td>Soldiers Nearly Killed with Military’s Bioterr...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1136409</td>\n",
       "      <td>redstate.com</td>\n",
       "      <td>political</td>\n",
       "      <td>https://www.redstate.com/diary/Erick/2010/10/2...</td>\n",
       "      <td>Colorado should not be too close to call right...</td>\n",
       "      <td>2017-11-10T11:18:44.524042</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Colorado Should Not Be Too Close To Call</td>\n",
       "      <td>Erick Erickson, Redstate Insider, Susan Wright...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>Colorado Should Not Be Too Close To Call</td>\n",
       "      <td>Michael Bennet, Colorado, ken buck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>62174</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/survival/2015/06/lett...</td>\n",
       "      <td>Letter Re: Does the Number of the Beast Have a...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Letter Re: Does the Number of the Beast Have a...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4692181</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/spirit/2016/11/thane-...</td>\n",
       "      <td>Two remote villages in neighbouring Thane dist...</td>\n",
       "      <td>2017-11-27T01:14:08.7454</td>\n",
       "      <td>2018-02-08 19:18:34.468038</td>\n",
       "      <td>2018-02-08 19:18:34.468066</td>\n",
       "      <td>Thane villages get rid of darkness this Diwali</td>\n",
       "      <td>Wisdom Blog Of Art Of Living</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>1308332</td>\n",
       "      <td>chroniclesmagazine.org</td>\n",
       "      <td>political</td>\n",
       "      <td>http://www.chroniclesmagazine.org/1998/Februar...</td>\n",
       "      <td>Peter LaBabera is publisher of the Lambda Repo...</td>\n",
       "      <td>2017-11-10T11:18:44.524042</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Chronicles Magazine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>6505182</td>\n",
       "      <td>nytimes.com</td>\n",
       "      <td>reliable</td>\n",
       "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
       "      <td>SAINT-SA?: PIANO CONCERTOS NOS. 2 &amp; 5; FRANCK:...</td>\n",
       "      <td>2018-02-11 00:42:12.085863</td>\n",
       "      <td>2018-02-11 00:14:20.346838</td>\n",
       "      <td>2018-02-11 00:14:20.346871</td>\n",
       "      <td>Classical Recordings: Ambient Haze, Romanticis...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>SAINT-SA�NS: PIANO CONCERTOS NOS. 2 &amp; 5; FRANC...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nytimes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>73138</td>\n",
       "      <td>beforeitsnews.com</td>\n",
       "      <td>fake</td>\n",
       "      <td>http://beforeitsnews.com/opinion-conservative/...</td>\n",
       "      <td>Barack and the Terrible, Horrible, No Good, Ve...</td>\n",
       "      <td>2018-01-25 20:13:50.426130</td>\n",
       "      <td>2018-02-02 01:19:41.756632</td>\n",
       "      <td>2018-02-02 01:19:41.756664</td>\n",
       "      <td>Barack and the Terrible, Horrible, No Good, Ve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>1935828</td>\n",
       "      <td>jihadwatch.org</td>\n",
       "      <td>conspiracy</td>\n",
       "      <td>https://www.jihadwatch.org/2009/05/sheikh-al-a...</td>\n",
       "      <td>Yet once again he doesn’t explain how exactly ...</td>\n",
       "      <td>2017-11-18T20:01:27.400599</td>\n",
       "      <td>2018-02-07 23:39:33.852671</td>\n",
       "      <td>2018-02-07 23:39:33.852696</td>\n",
       "      <td>Sheikh Al-Azhar urges jihad against terrorists</td>\n",
       "      <td>Robert Spencer, Michael Copeland</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                  domain        type  \\\n",
       "0     6301449             nytimes.com    reliable   \n",
       "1     3676064      nationalreview.com   political   \n",
       "2     5192554            infowars.com  conspiracy   \n",
       "3     1136409            redstate.com   political   \n",
       "4       62174       beforeitsnews.com        fake   \n",
       "...       ...                     ...         ...   \n",
       "4995  4692181       beforeitsnews.com        fake   \n",
       "4996  1308332  chroniclesmagazine.org   political   \n",
       "4997  6505182             nytimes.com    reliable   \n",
       "4998    73138       beforeitsnews.com        fake   \n",
       "4999  1935828          jihadwatch.org  conspiracy   \n",
       "\n",
       "                                                    url  \\\n",
       "0     https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "1     http://www.nationalreview.com/postmodern-conse...   \n",
       "2     https://www.infowars.com/soldiers-nearly-kille...   \n",
       "3     https://www.redstate.com/diary/Erick/2010/10/2...   \n",
       "4     http://beforeitsnews.com/survival/2015/06/lett...   \n",
       "...                                                 ...   \n",
       "4995  http://beforeitsnews.com/spirit/2016/11/thane-...   \n",
       "4996  http://www.chroniclesmagazine.org/1998/Februar...   \n",
       "4997  https://query.nytimes.com/gst/fullpage.html?re...   \n",
       "4998  http://beforeitsnews.com/opinion-conservative/...   \n",
       "4999  https://www.jihadwatch.org/2009/05/sheikh-al-a...   \n",
       "\n",
       "                                                content  \\\n",
       "0     \\n\\nTo the Editor:\\n\\nRe ''Drop Out of the Col...   \n",
       "1     So I’ve written for another channel my advice ...   \n",
       "2     David Gutierrez\\n\\nNatural News\\n\\nNovember 3,...   \n",
       "3     Colorado should not be too close to call right...   \n",
       "4     Letter Re: Does the Number of the Beast Have a...   \n",
       "...                                                 ...   \n",
       "4995  Two remote villages in neighbouring Thane dist...   \n",
       "4996  Peter LaBabera is publisher of the Lambda Repo...   \n",
       "4997  SAINT-SA?: PIANO CONCERTOS NOS. 2 & 5; FRANCK:...   \n",
       "4998  Barack and the Terrible, Horrible, No Good, Ve...   \n",
       "4999  Yet once again he doesn’t explain how exactly ...   \n",
       "\n",
       "                      scraped_at                 inserted_at  \\\n",
       "0     2018-02-11 00:40:10.316783  2018-02-11 00:14:20.346838   \n",
       "1     2017-11-27T01:14:42.983556  2018-02-08 19:18:34.468038   \n",
       "2     2017-12-09T22:10:08.302997  2018-02-08 19:18:34.468038   \n",
       "3     2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
       "4     2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "...                          ...                         ...   \n",
       "4995    2017-11-27T01:14:08.7454  2018-02-08 19:18:34.468038   \n",
       "4996  2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
       "4997  2018-02-11 00:42:12.085863  2018-02-11 00:14:20.346838   \n",
       "4998  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
       "4999  2017-11-18T20:01:27.400599  2018-02-07 23:39:33.852671   \n",
       "\n",
       "                      updated_at  \\\n",
       "0     2018-02-11 00:14:20.346871   \n",
       "1     2018-02-08 19:18:34.468066   \n",
       "2     2018-02-08 19:18:34.468066   \n",
       "3     2018-02-07 23:39:33.852696   \n",
       "4     2018-02-02 01:19:41.756664   \n",
       "...                          ...   \n",
       "4995  2018-02-08 19:18:34.468066   \n",
       "4996  2018-02-07 23:39:33.852696   \n",
       "4997  2018-02-11 00:14:20.346871   \n",
       "4998  2018-02-02 01:19:41.756664   \n",
       "4999  2018-02-07 23:39:33.852696   \n",
       "\n",
       "                                                  title  \\\n",
       "0                  Time to Scrap the Electoral College?   \n",
       "1     Antonin Scalia, Donald Trump, Tyler Cowen & Co...   \n",
       "2     Soldiers Nearly Killed with Military’s Bioterr...   \n",
       "3              Colorado Should Not Be Too Close To Call   \n",
       "4     Letter Re: Does the Number of the Beast Have a...   \n",
       "...                                                 ...   \n",
       "4995     Thane villages get rid of darkness this Diwali   \n",
       "4996                                Chronicles Magazine   \n",
       "4997  Classical Recordings: Ambient Haze, Romanticis...   \n",
       "4998  Barack and the Terrible, Horrible, No Good, Ve...   \n",
       "4999     Sheikh Al-Azhar urges jihad against terrorists   \n",
       "\n",
       "                                                authors  keywords  \\\n",
       "0                                                   NaN       NaN   \n",
       "1                                Peter Augustine Lawler       NaN   \n",
       "2                                                   NaN       NaN   \n",
       "3     Erick Erickson, Redstate Insider, Susan Wright...       NaN   \n",
       "4                                                   NaN       NaN   \n",
       "...                                                 ...       ...   \n",
       "4995                       Wisdom Blog Of Art Of Living       NaN   \n",
       "4996                                                NaN       NaN   \n",
       "4997                                                NaN       NaN   \n",
       "4998                                                NaN       NaN   \n",
       "4999                   Robert Spencer, Michael Copeland       NaN   \n",
       "\n",
       "                                          meta_keywords  \\\n",
       "0     ['Presidential Elections (US)', 'Electoral Col...   \n",
       "1                            ['Peter Augustine Lawler']   \n",
       "2                                                  ['']   \n",
       "3                                                  ['']   \n",
       "4                                                  ['']   \n",
       "...                                                 ...   \n",
       "4995                                               ['']   \n",
       "4996                                               ['']   \n",
       "4997                                               ['']   \n",
       "4998                                               ['']   \n",
       "4999                                               ['']   \n",
       "\n",
       "                                       meta_description  \\\n",
       "0     <br>To the Editor:\\n<p>\\n  Re ''Drop Out of th...   \n",
       "1     Senate Republicans should enter into the battl...   \n",
       "2                                                   NaN   \n",
       "3              Colorado Should Not Be Too Close To Call   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4995                                                NaN   \n",
       "4996                                                NaN   \n",
       "4997  SAINT-SA�NS: PIANO CONCERTOS NOS. 2 & 5; FRANC...   \n",
       "4998                                                NaN   \n",
       "4999                                                NaN   \n",
       "\n",
       "                                    tags  summary   source  typeContent  \n",
       "0                                    NaN      NaN  nytimes         True  \n",
       "1                                    NaN      NaN      NaN         True  \n",
       "2                                    NaN      NaN      NaN        False  \n",
       "3     Michael Bennet, Colorado, ken buck      NaN      NaN         True  \n",
       "4                                    NaN      NaN      NaN        False  \n",
       "...                                  ...      ...      ...          ...  \n",
       "4995                                 NaN      NaN      NaN        False  \n",
       "4996                                 NaN      NaN      NaN         True  \n",
       "4997                                 NaN      NaN  nytimes         True  \n",
       "4998                                 NaN      NaN      NaN        False  \n",
       "4999                                 NaN      NaN      NaN        False  \n",
       "\n",
       "[5000 rows x 17 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../datasets/sample/dataset_bin_raw.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:08<00:00, 571.16it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 32861.04it/s]\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1668.35it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 268.91it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 145619.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20000 rows\n",
      "finish time: 31.504063844680786\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file, content=\"content\"):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), content, \"content_cleaned\"),\n",
    "            (pp.Tokenizer(), \"content_cleaned\", \"content_tokenized\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content_tokenized\"),\n",
    "            (pp.Stem(), \"content_tokenized\"),\n",
    "            (pp.Combine_Content(), \"content_tokenized\", \"content_combined\"), # join all the words in the list to a string\n",
    "            # Clean authors\n",
    "            # (pp.Clean_author(), \"authors\"),\n",
    "            # # Clean title\n",
    "            # (pp.Clean_data(), 'title'),\n",
    "            # (pp.Tokenizer(), \"title\"),\n",
    "            # (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            # (pp.Stem(), \"title\"),\n",
    "            # (pp.Combine_Content(), \"title\"),\n",
    "            # # Clean domain\n",
    "            # (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            # (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            # (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            # (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            # (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "Clean_data(raw_sample_file, \"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "cleaned_content_list = \"content_tokenized\"\n",
    "cleaned_content_string = \"content_combined\"\n",
    "# cleaned_data = pd.read_csv(\"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data to training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 891115.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries to read: 0\n",
      "entries read: 20000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fromfile = \"../datasets/big/dataset_unbalanced_cleaned.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(fromfile, 5000, [0.8,0.1,0.1], [False, False, False], \n",
    "                                   out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)\n",
    "\n",
    "cleaned_data = pd.read_csv(\"../datasets/sample/dataset_unbalanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the data (optional)\n",
    "\n",
    "To find underlying clusters in the data, we use the KMeans algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the corpus using TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(cleaned_data['content_combined'].values.astype('U'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the elbow curve to determine the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Calculate SSE for different values of k\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "models = []\n",
    "\n",
    "sse = {}\n",
    "for k in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=k, max_iter=1000, n_init=10).fit(tfidf)\n",
    "    models.append(kmeans)\n",
    "    sse[k] = kmeans.inertia_ \n",
    "\n",
    "# Plot SSE against k\n",
    "plt.figure()\n",
    "plt.plot(list(sse.keys()), list(sse.values()))\n",
    "plt.xlabel(\"Number of clusters\")\n",
    "plt.ylabel(\"SSE\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KMeans(max_iter=100, n_clusters=5, n_init=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KMeans</label><div class=\"sk-toggleable__content\"><pre>KMeans(max_iter=100, n_clusters=5, n_init=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "KMeans(max_iter=100, n_clusters=5, n_init=10)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_clusters = 5\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=10)\n",
    "kmeans.fit(tfidf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the clusters using a a scatterplot in 2d with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_color = {0: 'red', 1: 'blue', 2: 'green', 3: 'violet', 4: \"yellow\", 5: \"orange\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce the dimensions of the data using PCA\n",
    "pca = PCA(n_components=2)\n",
    "tfidf_2d = pca.fit_transform(tfidf.toarray())\n",
    "\n",
    "colors = [label_to_color[label] for label in kmeans.labels_]\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(tfidf_2d[:, 0], tfidf_2d[:, 1], \n",
    "            c=colors, \n",
    "            cmap='rainbow',\n",
    "            )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster red terms: num, date, url, said, new, time, mr, peopl, state, year, \n",
      "Cluster blue terms: updat, bookmark, sputnik, pleas, dear, excit, radio, whitelist, adblock, voic, \n",
      "Cluster green terms: tor, tail, anonymis, usb, browser, dvd, submiss, stick, comput, internet, \n",
      "Cluster violet terms: rec, num, trump, republican, moor, date, diari, roy, senat, thread, \n",
      "Cluster yellow terms: iran, iranian, nuclear, suprem, islam, minist, leader, tehran, ali, republ, \n"
     ]
    }
   ],
   "source": [
    "# Print the top terms in each cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1] # sort the centroids by their distance to the origin\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {label_to_color[i]} terms: \", end='')\n",
    "    for j in order_centroids[i, :10]:\n",
    "        print(f\"{terms[j]}, \", end='')\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add the cluster labels to the data as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['cluster'] = kmeans.labels_\n",
    "# add the cluster label to the content\n",
    "\n",
    "# concat the cluster label to the content\n",
    "\n",
    "# create a new column 'cluster_color' that maps cluster labels to color codes\n",
    "cleaned_data['cluster_color'] = cleaned_data['cluster'].map(label_to_color)\n",
    "\n",
    "# create a new column 'cluster_string' that concatenates the color code and cluster label string\n",
    "cleaned_data['cluster_string'] = cleaned_data['cluster_color'] + 'Cluster'\n",
    "\n",
    "cleaned_data['content_with_cluster'] = cleaned_data['content_combined'] + ' ' + cleaned_data['cluster_string'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'editor drop colleg editori march <num> written favor elector colleg reform sinc <date> would prefer popular elect presid gimmick endors individu state bind elector vote nation popularvot winner seem problemat one basic reason stop state legislatur strong partisan loyalti abandon agreement urgent calcul parti advantag come fore ? one legislatur anoth undo end difficult might presidentialelect reform depend take amend process serious that exampl editori cite conclus demonstr although individu state set legisl preced grant suffrag africanamerican women also popular elect senat end right entrench constitut amend left unstabl legisl gimmickri jack rakov stanford calif march <date> writer professor histori american studi polit scienc stanford univers draw draw thoma fuch redCluster'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data['content_with_cluster'][0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add top termas as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per cluster:\n",
      "Cluster red terms: num date url said new time mr peopl state year\n",
      "Cluster blue terms: updat bookmark sputnik pleas dear excit radio whitelist adblock voic\n",
      "Cluster green terms: tor tail anonymis usb browser dvd submiss stick comput internet\n",
      "Cluster violet terms: rec num trump republican moor date diari roy senat thread\n",
      "Cluster yellow terms: iran iranian nuclear suprem islam minist leader tehran ali republ\n"
     ]
    }
   ],
   "source": [
    "# Print the top terms in each cluster\n",
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1] # sort the centroids by their distance to the origin\n",
    "terms = tfidf_vectorizer.get_feature_names_out()\n",
    "for i in range(num_clusters):\n",
    "    print(f\"Cluster {label_to_color[i]} terms: \", end='')\n",
    "    top_terms = ', '.join([terms[j] for j in order_centroids[i, :10]]).replace(',', '')\n",
    "    print(top_terms)\n",
    "    cleaned_data.loc[cleaned_data['cluster'] == i, 'content_with_top_terms'] = cleaned_data.loc[cleaned_data['cluster'] == i, 'content_combined'] + ' ' + top_terms\n",
    "\n",
    "# write data to csv\n",
    "# cleaned_data.to_csv(\"../datasets/sample/dataset_unbalanced_with_clusters.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:08<00:00, 573.67it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 36078.17it/s]\n",
      "100%|██████████| 5000/5000 [00:03<00:00, 1628.07it/s]\n",
      "100%|██████████| 5000/5000 [00:18<00:00, 270.21it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 160401.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 20000 rows\n",
      "finish time: 33.5052752494812\n"
     ]
    }
   ],
   "source": [
    "# Clean_data(\"../datasets/sample/dataset_unbalanced_with_clusters.csv\", \"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "# cleaned_content_list = \"content_tokenized\"\n",
    "# cleaned_content_string = \"content_combined\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'editor drop colleg editori march <num> written favor elector colleg reform sinc <date> would prefer popular elect presid gimmick endors individu state bind elector vote nation popularvot winner seem problemat one basic reason stop state legislatur strong partisan loyalti abandon agreement urgent calcul parti advantag come fore ? one legislatur anoth undo end difficult might presidentialelect reform depend take amend process serious that exampl editori cite conclus demonstr although individu state set legisl preced grant suffrag africanamerican women also popular elect senat end right entrench constitut amend left unstabl legisl gimmickri jack rakov stanford calif march <date> writer professor histori american studi polit scienc stanford univers draw draw thoma fuch num date url said new time mr peopl state year'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cleaned_data.head(1)\n",
    "cleaned_data.iloc[0]['content_with_top_terms']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get balanced or unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 961026.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries to read: 0\n",
      "entries read: 20000\n"
     ]
    }
   ],
   "source": [
    "# importlib.reload(pp)\n",
    "# from_file = '../datasets/sample/dataset_bin_raw.csv'\n",
    "# # from_file = '../datasets/sample/news_sample_cleaned_num_100k.csv'\n",
    "# TOTALSIZE = 5000\n",
    "\n",
    "# pp.get_dataframe_with_distribution(\n",
    "#         from_file, \n",
    "#         total_size = TOTALSIZE, \n",
    "#         splits=[0.8,0.1,0.1], \n",
    "#         balanced=[False, False, False], \n",
    "#         out_file=\"../datasets/big/dataset_unbalanced.csv\", \n",
    "#         get_frame=False\n",
    "#     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['editor', 'drop', 'colleg', 'editori', 'march', '<num>', 'written', 'favor', 'elector', 'colleg', 'reform', 'sinc', '<date>', 'would', 'prefer', 'popular', 'elect', 'presid', 'gimmick', 'endors', 'individu', 'state', 'bind', 'elector', 'vote', 'nation', 'popularvot', 'winner', 'seem', 'problemat', 'one', 'basic', 'reason', 'stop', 'state', 'legislatur', 'strong', 'partisan', 'loyalti', 'abandon', 'agreement', 'urgent', 'calcul', 'parti', 'advantag', 'come', 'fore', '?', 'one', 'legislatur', 'anoth', 'undo', 'end', 'difficult', 'might', 'presidentialelect', 'reform', 'depend', 'take', 'amend', 'process', 'serious', 'that', 'exampl', 'editori', 'cite', 'conclus', 'demonstr', 'although', 'individu', 'state', 'set', 'legisl', 'preced', 'grant', 'suffrag', 'africanamerican', 'women', 'also', 'popular', 'elect', 'senat', 'end', 'right', 'entrench', 'constitut', 'amend', 'left', 'unstabl', 'legisl', 'gimmickri', 'jack', 'rakov', 'stanford', 'calif', 'march', '<date>', 'writer', 'professor', 'histori', 'american', 'studi', 'polit', 'scienc', 'stanford', 'univers', 'draw', 'draw', 'thoma', 'fuch', 'redclust', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama', 'num', 'said', 'mr', 'date', 'trump', 'state', 'url', 'new', 'peopl', 'obama']\""
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"../datasets/big/dataset_unbalanced_cleaned.csv\").iloc[0][\"content_tokenized\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/sample/dataset_unbalanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content'].values\n",
    "y = df['type_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 35s 257ms/step - loss: 0.5595 - accuracy: 0.7103 - val_loss: 0.4562 - val_accuracy: 0.7710\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.3328 - accuracy: 0.8580 - val_loss: 0.4303 - val_accuracy: 0.8040\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 0.1814 - accuracy: 0.9283 - val_loss: 0.5382 - val_accuracy: 0.7870\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0855 - accuracy: 0.9728 - val_loss: 0.6164 - val_accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 0.8345 - val_accuracy: 0.7890\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 38s 308ms/step - loss: 0.0619 - accuracy: 0.9790 - val_loss: 0.6924 - val_accuracy: 0.7640\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.8416 - val_accuracy: 0.7700\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.9763 - val_accuracy: 0.7700\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 1.0699 - val_accuracy: 0.7670\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 1.1170 - val_accuracy: 0.7830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd46c0a8f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data and make sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "# df = pd.read_csv(\"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "\n",
    "# working_file = \"../datasets/big/dataset_unbalanced_cleaned.csv\"\n",
    "# pd.read_csv(working_file).head()\n",
    "\n",
    "df = cleaned_data\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "# content cleaned\n",
    "# tokenizer.fit_on_texts(df[df[\"set\"] == 0][\"content_cleaned\"])\n",
    "# sequences = tokenizer.texts_to_sequences(df[\"content_cleaned\"])\n",
    "# df[\"padded_sequences\"] = pad_sequences(sequences, maxlen=1000, truncating=\"post\").tolist()\n",
    "\n",
    "# col = \"content_with_top_terms\"\n",
    "col = \"content_with_cluster\"\n",
    "\n",
    "# col = \"content_combined\"\n",
    "\n",
    "\n",
    "# all cleaned\n",
    "tokenizer.fit_on_texts(df[df[\"set\"] == 0][col])\n",
    "sequences = tokenizer.texts_to_sequences(df[col])\n",
    "df[\"padded_sequences_all\"] = pad_sequences(sequences, maxlen=1000, truncating=\"post\").tolist()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CSR (Compressed Sparse Row) matrix to use in models\n",
    "\n",
    "The function first splits the input data into three sets based on the values in the set column: a training set (where set=0), a validation set (where set=1), and a test set (where set=2).\n",
    "\n",
    "Next, the function applies the stack_func stacking function to the feature column of each set of data to convert it into a CSR (Compressed Sparse Row) matrix. A CSR matrix is a sparse matrix format that stores data in a compressed way, which can be more memory-efficient than other formats for sparse data.\n",
    "\n",
    "The function also converts the y column of each set of data into an integer type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csr_data(data: pd.DataFrame, feature: str, y, stack_func, set=\"set\", get_val=True):\n",
    "    # train = data[data[set] == 0]\n",
    "    # val = data[data[set] == 1]\n",
    "    # test = data[data[set] == 2]\n",
    "    \n",
    "    train = data[data[set] == 0]\n",
    "    val = data[data[set] == 1]\n",
    "    test = data[data[set] == 2]\n",
    "\n",
    "    X_train, y_train = stack_func(train[feature]), train[y].astype(int)\n",
    "    X_val, y_val = stack_func(val[feature]), val[y].astype(int)\n",
    "    X_test, y_test = stack_func(test[feature]), test[y].astype(int)\n",
    "    if not get_val:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stacked, X_val_stacked, X_test_stacked, y_train_stacked, y_val_stacked, y_test_stacked = split_csr_data(\n",
    "    df, \n",
    "    \"padded_sequences_all\", \"typeContent\", \n",
    "    lambda x: np.array(x.tolist()), \n",
    "    get_val=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model-A\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.6885 - accuracy: 0.5717 - precision: 0.5683 - recall: 0.6601 - val_loss: 0.6852 - val_accuracy: 0.5560 - val_precision: 0.7143 - val_recall: 0.3819\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.6641 - accuracy: 0.6392 - precision: 0.6878 - recall: 0.5334 - val_loss: 0.6630 - val_accuracy: 0.5840 - val_precision: 0.7532 - val_recall: 0.4132\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.6169 - accuracy: 0.6615 - precision: 0.7153 - recall: 0.5565 - val_loss: 0.6068 - val_accuracy: 0.6580 - val_precision: 0.7940 - val_recall: 0.5486\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.5508 - accuracy: 0.7355 - precision: 0.8135 - recall: 0.6233 - val_loss: 0.5654 - val_accuracy: 0.6740 - val_precision: 0.8882 - val_recall: 0.4965\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4886 - accuracy: 0.7828 - precision: 0.8787 - recall: 0.6650 - val_loss: 0.5071 - val_accuracy: 0.7720 - val_precision: 0.8508 - val_recall: 0.7326\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4393 - accuracy: 0.8195 - precision: 0.8953 - recall: 0.7308 - val_loss: 0.4821 - val_accuracy: 0.7980 - val_precision: 0.8375 - val_recall: 0.8056\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.4023 - accuracy: 0.8405 - precision: 0.8958 - recall: 0.7770 - val_loss: 0.4714 - val_accuracy: 0.7800 - val_precision: 0.8771 - val_recall: 0.7188\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3644 - accuracy: 0.8565 - precision: 0.9211 - recall: 0.7854 - val_loss: 0.4724 - val_accuracy: 0.7640 - val_precision: 0.8864 - val_recall: 0.6771\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3339 - accuracy: 0.8705 - precision: 0.9250 - recall: 0.8114 - val_loss: 0.4561 - val_accuracy: 0.7860 - val_precision: 0.8577 - val_recall: 0.7535\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 0s 3ms/step - loss: 0.3052 - accuracy: 0.8892 - precision: 0.9384 - recall: 0.8374 - val_loss: 0.4535 - val_accuracy: 0.7860 - val_precision: 0.8606 - val_recall: 0.7500\n",
      "16/16 [==============================] - 0s 1ms/step - loss: 0.4389 - accuracy: 0.7860 - precision: 0.8400 - recall: 0.7269\n",
      "Results: [0.4389077126979828, 0.7860000133514404, 0.8399999737739563, 0.7269230484962463]\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "models = [\n",
    "    (tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]), \"Model-A\"),\n",
    "    # Results: [0.4473037123680115, 0.777999997138977]\n",
    "    # Results: [0.4245592951774597, 0.7940000295639038]\n",
    "    # Results: [0.4389077126979828, 0.7860000133514404,\n",
    "\n",
    "    (tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000), # 10000 words, 64 dimensions\n",
    "        tf.keras.layers.LSTM(units=16, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ]), \"Model-B\"),\n",
    "    # Results: [0.6940779685974121, 0.7940000295639038]\n",
    "\n",
    "    # (tf.keras.Sequential([\n",
    "    #     tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000), # 10000 words, 64 dimensions\n",
    "    #     tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=16, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    #     tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    # ]), \"Model-C\"), bad\n",
    "\n",
    "\n",
    "    (tf.keras.Sequential([\n",
    "            tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000), # 10000 words, 64 dimensions\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.LSTM(100),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.3),\n",
    "            tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ]), \"Model-D\")\n",
    "    # Results: [0.9893674254417419, 0.7720000147819519]\n",
    "\n",
    "]\n",
    "\n",
    "modelIndex = 0\n",
    "# Compile the model\n",
    "# for model, name in models:\n",
    "for model, name in [models[modelIndex]]:\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\", tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "\n",
    "\n",
    "\n",
    "# for feature in [\"padded_sequences\", \"padded_sequences_all\"]:\n",
    "# print(\"Training model with feature: {}\".format(feature))\n",
    "# X_train, X_val, X_test, y_train, y_val, y_test = split_csr_data(df, \"padded_sequences_all\", \"typeContent\", lambda x: np.array(x.tolist()), get_val=True)\n",
    "\n",
    "# for model, name in models:\n",
    "for model, name in [models[modelIndex]]:\n",
    "    print(f\"Training {name}\")\n",
    "    # Train the model\n",
    "    model.fit(X_train_stacked, y_train_stacked, epochs=10, validation_data=(X_val_stacked, y_val_stacked))\n",
    "\n",
    "    # Evaluate the model\n",
    "    print(f\"Results:\", model.evaluate(X_test_stacked, y_test_stacked))\n",
    "\n",
    "# for feature in [\"padded_sequences\", \"padded_sequences_all\"]:\n",
    "#     print(\"Training model with feature: {}\".format(feature))\n",
    "#     X_train, X_val, X_test, y_train, y_val, y_test = split_csr_data(df, feature, \"typeContent\", lambda x: np.array(x.tolist()), get_val=True)\n",
    "\n",
    "#     for model, name in models:\n",
    "#         print(f\"Training {name}\")\n",
    "#         # Train the model\n",
    "#         model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "#         # Evaluate the model\n",
    "#         print(f\"Results:\", model.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Sequential' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[89], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mResults:\u001b[39m\u001b[39m\"\u001b[39m, model[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39msummary())\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Sequential' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most successful model from list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 41s 318ms/step - loss: 0.6214 - accuracy: 0.6715 - val_loss: 0.5209 - val_accuracy: 0.7740\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.4058 - accuracy: 0.8257 - val_loss: 0.4422 - val_accuracy: 0.7960\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 39s 308ms/step - loss: 0.2533 - accuracy: 0.9118 - val_loss: 0.5376 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 39s 309ms/step - loss: 0.1646 - accuracy: 0.9433 - val_loss: 0.5695 - val_accuracy: 0.7920\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 39s 310ms/step - loss: 0.1021 - accuracy: 0.9695 - val_loss: 0.6770 - val_accuracy: 0.7760\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 39s 313ms/step - loss: 0.0771 - accuracy: 0.9775 - val_loss: 0.8155 - val_accuracy: 0.7800\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 39s 316ms/step - loss: 0.0543 - accuracy: 0.9845 - val_loss: 0.6902 - val_accuracy: 0.7980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 39s 314ms/step - loss: 0.0383 - accuracy: 0.9905 - val_loss: 0.8368 - val_accuracy: 0.7960\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 39s 313ms/step - loss: 0.0338 - accuracy: 0.9902 - val_loss: 0.9150 - val_accuracy: 0.7680\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 39s 311ms/step - loss: 0.0386 - accuracy: 0.9880 - val_loss: 1.0217 - val_accuracy: 0.7740\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[84], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m model\u001b[39m.\u001b[39mfit(X_train_stacked, y_train_stacked, epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, validation_data\u001b[39m=\u001b[39m(X_val_stacked, y_val_stacked))\n\u001b[1;32m     20\u001b[0m \u001b[39m# Evaluate the model\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m model\u001b[39m.\u001b[39;49mevaluate(X_test, y_test)\n\u001b[1;32m     23\u001b[0m \u001b[39m# [0.23643802106380463, 0.9100000262260437]\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fakenews/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fakenews/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the model architecture\n",
    "# model = tf.keras.Sequential([\n",
    "#     Embedding(input_dim=5000, output_dim=64, input_length=1000),\n",
    "#     LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "#     Dense(units=1, activation='sigmoid')\n",
    "# ])\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000), # 10000 words, 64 dimensions\n",
    "    tf.keras.layers.LSTM(units=16, dropout=0.2, recurrent_dropout=0.2),\n",
    "    tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_stacked, y_train_stacked, epochs=10, validation_data=(X_val_stacked, y_val_stacked))\n",
    "\n",
    "# Evaluate the model\n",
    "# model.evaluate(X_test, y_test)\n",
    "\n",
    "model.evaluate(X_test_stacked, y_test_stacked)\n",
    "\n",
    "\n",
    "# [0.23643802106380463, 0.9100000262260437]\n",
    "\n",
    "\n",
    "# [0.8935213685035706, 0.7739999890327454]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 61ms/step - loss: 0.8279 - accuracy: 0.7780\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8278565406799316, 0.777999997138977]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test_stacked, y_test_stacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m classification_report\n\u001b[1;32m      4\u001b[0m \u001b[39m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mround(model\u001b[39m.\u001b[39;49mpredict(X_test))\n\u001b[1;32m      7\u001b[0m \u001b[39m# Print classification report\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fakenews/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fakenews/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "def print_scores(model, X_test, y_test, X_train, y_train): # add model, or filepath (and load) here?\n",
    "    #model = keras.models.load_model(best_model_file_name)\n",
    "    y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
    "    y_pred_train = (model.predict(X_train) > 0.5).astype(\"int32\")\n",
    "    print(\"Scores for \", model)\n",
    "    print('Accuracy Train: ', accuracy_score(y_train, y_pred_train))\n",
    "    print('Accuracy Test: ', accuracy_score(y_test, y_pred))\n",
    "    print('Precision: ', precision_score(y_test, y_pred))\n",
    "    print('Recall: ', recall_score(y_test, y_pred))\n",
    "    print('F1 Score: ', f1_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_11 (Embedding)    (None, 1000, 16)          160000    \n",
      "                                                                 \n",
      " lstm_7 (LSTM)               (None, 16)                2112      \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 162,129\n",
      "Trainable params: 162,129\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model has 162,129 parameters to train which means 162,129 partial gradients to calculate and optimize for each epoch. Clearly, the optimization problem is is going to have numerous local minima and look something like this:\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support vector machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_csr_data(df, \"padded_sequences_all\", \"typeContent\", lambda x: np.array(x.tolist()), get_val=True)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(X_train['content_domain_authors_title'])\n",
    "X_test = vectorizer.transform(X_test['content_domain_authors_title'])\n",
    "\n",
    "\n",
    "# create SVM model\n",
    "clf = SVC(kernel='linear')\n",
    "\n",
    "# fit the model to the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the testing data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# print the accuracy score\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 15719.50it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf1 = pd.read_csv(\"../datasets/sample/dataset_unbalanced.csv\")\n",
    "pdf2 = pp.apply_pipeline_pd_tqdm(pdf1, [(pp.Tokenizer(), \"content\", \"tokenized\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "_ = pdf2[\"tokenized\"].apply(lambda x: list.extend(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'the',\n",
       " 'Editor:',\n",
       " 'Re',\n",
       " \"''Drop\",\n",
       " 'Out',\n",
       " 'of',\n",
       " 'the',\n",
       " \"College''\",\n",
       " '(editorial,',\n",
       " 'March',\n",
       " '14):',\n",
       " 'Having',\n",
       " 'written',\n",
       " 'in',\n",
       " 'favor',\n",
       " 'of',\n",
       " 'Electoral',\n",
       " 'College',\n",
       " 'reform',\n",
       " 'since',\n",
       " '2000,',\n",
       " 'I,',\n",
       " 'too,',\n",
       " 'would',\n",
       " 'prefer',\n",
       " 'the',\n",
       " 'popular',\n",
       " 'election',\n",
       " 'of',\n",
       " 'the',\n",
       " 'president.',\n",
       " 'But',\n",
       " 'the',\n",
       " 'gimmick',\n",
       " 'you',\n",
       " 'endorse,',\n",
       " 'of',\n",
       " 'having',\n",
       " 'individual',\n",
       " 'states',\n",
       " 'bind',\n",
       " 'their',\n",
       " 'electors',\n",
       " 'to',\n",
       " 'vote',\n",
       " 'for',\n",
       " 'the',\n",
       " 'national',\n",
       " 'popular-vote',\n",
       " 'winner,',\n",
       " 'seems',\n",
       " 'problematic',\n",
       " 'for',\n",
       " 'one',\n",
       " 'basic',\n",
       " 'reason.',\n",
       " 'What',\n",
       " 'is',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'state',\n",
       " 'legislatures',\n",
       " 'with',\n",
       " 'strong',\n",
       " 'partisan',\n",
       " 'loyalties',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " 'from',\n",
       " 'abandoning',\n",
       " 'such',\n",
       " 'an',\n",
       " 'agreement',\n",
       " 'when',\n",
       " 'urgent',\n",
       " 'calculations',\n",
       " 'of',\n",
       " 'party',\n",
       " 'advantage',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fore?',\n",
       " 'What',\n",
       " 'one',\n",
       " 'legislature',\n",
       " 'can',\n",
       " 'do,',\n",
       " 'another',\n",
       " 'can',\n",
       " 'undo.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'end,',\n",
       " 'difficult',\n",
       " 'as',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be,',\n",
       " 'presidential-election',\n",
       " 'reform',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'process',\n",
       " 'seriously.',\n",
       " 'And',\n",
       " \"that's\",\n",
       " 'what',\n",
       " 'the',\n",
       " 'examples',\n",
       " 'your',\n",
       " 'editorial',\n",
       " 'cites',\n",
       " 'in',\n",
       " 'conclusion',\n",
       " 'demonstrate.',\n",
       " 'Although',\n",
       " 'individual',\n",
       " 'states',\n",
       " 'did',\n",
       " 'set',\n",
       " 'legislative',\n",
       " 'precedents',\n",
       " 'for',\n",
       " 'granting',\n",
       " 'the',\n",
       " 'suffrage',\n",
       " 'to',\n",
       " 'African-Americans',\n",
       " 'and',\n",
       " 'to',\n",
       " 'women,',\n",
       " 'and',\n",
       " 'also',\n",
       " 'for',\n",
       " 'the',\n",
       " 'popular',\n",
       " 'election',\n",
       " 'of',\n",
       " 'senators,',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " 'these',\n",
       " 'rights',\n",
       " 'were',\n",
       " 'entrenched',\n",
       " 'through',\n",
       " 'constitutional',\n",
       " 'amendments,',\n",
       " 'not',\n",
       " 'left',\n",
       " 'to',\n",
       " 'unstable',\n",
       " 'legislative',\n",
       " 'gimmickry.',\n",
       " 'Jack',\n",
       " 'Rakove',\n",
       " 'Stanford,',\n",
       " 'Calif.,',\n",
       " 'March',\n",
       " '14,',\n",
       " '2006',\n",
       " 'The',\n",
       " 'writer',\n",
       " 'is',\n",
       " 'a',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'history,',\n",
       " 'American',\n",
       " 'studies',\n",
       " 'and',\n",
       " 'political',\n",
       " 'science',\n",
       " 'at',\n",
       " 'Stanford',\n",
       " 'University.',\n",
       " 'Drawing',\n",
       " '(Drawing',\n",
       " 'by',\n",
       " 'Thomas',\n",
       " 'Fuchs)',\n",
       " 'So',\n",
       " 'I’ve',\n",
       " 'written',\n",
       " 'for',\n",
       " 'another',\n",
       " 'channel',\n",
       " 'my',\n",
       " 'advice',\n",
       " 'for',\n",
       " 'how',\n",
       " 'Republicans',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'should',\n",
       " 'scrupulously',\n",
       " 'adhere',\n",
       " 'to',\n",
       " 'constitutional',\n",
       " 'forms',\n",
       " 'and',\n",
       " 'Scalia’s',\n",
       " 'constitutionalism',\n",
       " 'in',\n",
       " 'entering',\n",
       " 'into',\n",
       " 'the',\n",
       " 'battle',\n",
       " 'to',\n",
       " 'replace',\n",
       " 'the',\n",
       " 'irreplaceable',\n",
       " 'justice.',\n",
       " 'I',\n",
       " 'agree',\n",
       " 'with',\n",
       " 'Pete',\n",
       " 'below',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Republicans',\n",
       " 'have',\n",
       " 'become',\n",
       " 'an',\n",
       " 'ideological',\n",
       " 'disaster.',\n",
       " 'Tyler',\n",
       " 'Cowen,',\n",
       " 'a',\n",
       " 'very',\n",
       " 'thoughtful',\n",
       " 'and',\n",
       " 'erudite,',\n",
       " 'fairly',\n",
       " 'libertarian',\n",
       " 'economist,',\n",
       " 'reflects',\n",
       " 'on',\n",
       " 'what',\n",
       " 'libertarians',\n",
       " 'should',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'the',\n",
       " 'unexpected',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'Trump.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'his',\n",
       " 'takeaways:',\n",
       " 'True',\n",
       " 'cosmopolitans',\n",
       " 'are',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'find.',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'that’s',\n",
       " 'true,',\n",
       " 'although',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'what',\n",
       " 'you',\n",
       " 'mean',\n",
       " 'by',\n",
       " 'cosmopolitan.',\n",
       " 'From',\n",
       " 'a',\n",
       " 'libertarian',\n",
       " 'economist’s',\n",
       " 'view',\n",
       " '(as',\n",
       " 'I’ve',\n",
       " 'learned',\n",
       " 'from',\n",
       " 'Cowen’s',\n",
       " 'books),',\n",
       " 'a',\n",
       " 'cosmopolitan',\n",
       " 'is',\n",
       " 'a',\n",
       " 'free',\n",
       " 'individual',\n",
       " 'who’s',\n",
       " 'been',\n",
       " 'productive',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'enjoy,',\n",
       " 'from',\n",
       " 'a',\n",
       " 'detached',\n",
       " 'or',\n",
       " 'abstracted',\n",
       " 'and',\n",
       " 'displaced',\n",
       " 'or',\n",
       " 'multicultural',\n",
       " 'point',\n",
       " 'of',\n",
       " 'view,',\n",
       " 'all',\n",
       " 'that',\n",
       " 'the',\n",
       " 'various',\n",
       " 'cultures',\n",
       " 'and',\n",
       " 'peoples',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'have',\n",
       " 'to',\n",
       " 'offer.',\n",
       " 'The',\n",
       " 'foodie',\n",
       " 'Cowen,',\n",
       " 'for',\n",
       " 'example,',\n",
       " 'can',\n",
       " 'enjoy',\n",
       " 'French',\n",
       " 'or',\n",
       " 'Ethiopian',\n",
       " 'food',\n",
       " 'without',\n",
       " 'all',\n",
       " 'the',\n",
       " 'repressive',\n",
       " 'baggage',\n",
       " 'of',\n",
       " 'actually',\n",
       " 'being',\n",
       " 'French',\n",
       " 'or',\n",
       " 'Ethiopian.',\n",
       " 'And',\n",
       " 'it’s',\n",
       " 'true',\n",
       " 'enough,',\n",
       " 'after',\n",
       " 'all,',\n",
       " 'that',\n",
       " 'those',\n",
       " 'people',\n",
       " 'free',\n",
       " 'enough',\n",
       " 'through',\n",
       " 'habits',\n",
       " 'of',\n",
       " 'abstract',\n",
       " 'thinking',\n",
       " 'to',\n",
       " 'be',\n",
       " 'facile',\n",
       " 'role',\n",
       " 'players',\n",
       " 'deploying',\n",
       " 'their',\n",
       " 'technical',\n",
       " 'skills',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'settings',\n",
       " 'are',\n",
       " 'those',\n",
       " 'most',\n",
       " 'prepared',\n",
       " 'to',\n",
       " 'flourish',\n",
       " 'in',\n",
       " 'the',\n",
       " '21st-century',\n",
       " 'global',\n",
       " 'competitive',\n",
       " 'marketplace.',\n",
       " 'Still,',\n",
       " 'someone',\n",
       " 'might',\n",
       " 'say,',\n",
       " 'that',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'detachment',\n",
       " 'isn’t',\n",
       " 'true',\n",
       " 'cosmopolitanism.',\n",
       " 'To',\n",
       " 'be',\n",
       " 'a',\n",
       " 'genuine',\n",
       " 'citizen',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world,',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'community',\n",
       " 'of',\n",
       " 'philosophers',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'pursuing',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'we',\n",
       " 'all',\n",
       " 'share.',\n",
       " 'Or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'God,',\n",
       " 'that',\n",
       " 'loving',\n",
       " 'and',\n",
       " 'relational',\n",
       " 'personal',\n",
       " 'community',\n",
       " 'that',\n",
       " 'includes',\n",
       " 'us',\n",
       " 'all.',\n",
       " 'Or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Stoic',\n",
       " 'community',\n",
       " 'of',\n",
       " 'rational',\n",
       " 'and',\n",
       " 'virtuous',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'across',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'who',\n",
       " 'know',\n",
       " 'one',\n",
       " 'another',\n",
       " '—',\n",
       " 'Pericles,',\n",
       " 'Marcus',\n",
       " 'Aurelius,',\n",
       " 'and',\n",
       " 'Robert',\n",
       " 'E.',\n",
       " 'Lee',\n",
       " '—',\n",
       " 'as',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'kind.',\n",
       " 'I',\n",
       " 'could',\n",
       " 'go',\n",
       " 'on,',\n",
       " 'but',\n",
       " 'a',\n",
       " 'true',\n",
       " 'cosmopolitan',\n",
       " 'isn’t',\n",
       " 'merely',\n",
       " 'free',\n",
       " 'from',\n",
       " 'relational',\n",
       " 'prejudices',\n",
       " 'to',\n",
       " 'maximize',\n",
       " 'his',\n",
       " 'personal',\n",
       " 'productivity',\n",
       " 'in',\n",
       " 'the',\n",
       " 'service',\n",
       " 'of',\n",
       " 'random',\n",
       " 'personal',\n",
       " 'preferences.',\n",
       " 'It',\n",
       " 'really',\n",
       " 'is',\n",
       " 'true,',\n",
       " 'I',\n",
       " 'agree,',\n",
       " 'that',\n",
       " 'Trumpism',\n",
       " 'and',\n",
       " 'so',\n",
       " 'forth',\n",
       " 'are,',\n",
       " 'at',\n",
       " 'some',\n",
       " 'level',\n",
       " 'and',\n",
       " 'to',\n",
       " 'some',\n",
       " 'extent,',\n",
       " 'rebellions',\n",
       " 'against',\n",
       " 'the',\n",
       " 'emptiness',\n",
       " 'or',\n",
       " 'irresponsibility',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cosmopolitanism',\n",
       " 'of',\n",
       " 'a',\n",
       " 'merely',\n",
       " 'cognitive',\n",
       " 'elite,',\n",
       " 'one',\n",
       " 'distinguished',\n",
       " 'only',\n",
       " 'by',\n",
       " 'its',\n",
       " 'marvelously',\n",
       " 'unprecedented',\n",
       " 'capacity',\n",
       " 'for',\n",
       " 'almost',\n",
       " 'infinitely',\n",
       " 'productive',\n",
       " 'mental',\n",
       " 'labor.',\n",
       " 'Their',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'faux',\n",
       " 'cosmopolitanism',\n",
       " 'is',\n",
       " 'an',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'a',\n",
       " 'privileged',\n",
       " 'few,',\n",
       " 'one',\n",
       " 'not',\n",
       " 'as',\n",
       " 'accompanied',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'by',\n",
       " 'corresponding',\n",
       " 'responsibilities.',\n",
       " 'So',\n",
       " 'one',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'some',\n",
       " 'libertarians',\n",
       " 'is',\n",
       " 'not',\n",
       " 'thinking',\n",
       " 'of',\n",
       " 'citizenship',\n",
       " 'as',\n",
       " 'a',\n",
       " 'real',\n",
       " 'and',\n",
       " 'ennobling',\n",
       " 'human',\n",
       " 'experience.',\n",
       " 'It’s',\n",
       " 'not',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'shared',\n",
       " 'all',\n",
       " 'that',\n",
       " 'much',\n",
       " 'by',\n",
       " 'even',\n",
       " 'Augustinian',\n",
       " 'Christians',\n",
       " 'who',\n",
       " 'don’t',\n",
       " 'think',\n",
       " 'that',\n",
       " 'the',\n",
       " 'political',\n",
       " 'diversity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'Man',\n",
       " 'is',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'be',\n",
       " 'displaced',\n",
       " 'by',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'God.',\n",
       " 'Most',\n",
       " 'people',\n",
       " 'find',\n",
       " 'personal',\n",
       " 'fulfillment',\n",
       " 'through',\n",
       " 'love',\n",
       " 'and',\n",
       " 'work',\n",
       " 'in',\n",
       " 'institutions',\n",
       " 'that',\n",
       " 'correspond',\n",
       " 'to',\n",
       " 'the',\n",
       " 'limitations',\n",
       " 'of',\n",
       " 'our',\n",
       " 'existence',\n",
       " 'as',\n",
       " 'embodied',\n",
       " 'animals',\n",
       " 'with',\n",
       " 'relational',\n",
       " 'longings.',\n",
       " 'And',\n",
       " 'those',\n",
       " 'limitations',\n",
       " 'aren’t',\n",
       " 'even',\n",
       " 'limitations',\n",
       " 'exactly;',\n",
       " 'they',\n",
       " 'shape',\n",
       " 'our',\n",
       " 'experiences',\n",
       " 'of',\n",
       " 'who',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'is',\n",
       " 'and',\n",
       " 'what',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'is',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'do.',\n",
       " 'Before',\n",
       " 'we',\n",
       " 'can',\n",
       " 'be',\n",
       " 'citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world,',\n",
       " 'someone',\n",
       " 'might',\n",
       " 'say,',\n",
       " 'we',\n",
       " 'must',\n",
       " 'be',\n",
       " 'citizens',\n",
       " 'of',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'country.',\n",
       " 'And',\n",
       " 'in',\n",
       " 'the',\n",
       " 'most',\n",
       " 'truthfully',\n",
       " 'self-conscious',\n",
       " 'cases,',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'will',\n",
       " 'be',\n",
       " 'both.',\n",
       " 'It’s',\n",
       " 'through',\n",
       " 'our',\n",
       " 'particular',\n",
       " 'experiences',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'limited',\n",
       " 'number',\n",
       " 'of',\n",
       " 'places',\n",
       " 'that',\n",
       " 'we',\n",
       " 'get',\n",
       " 'what',\n",
       " 'access',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'the',\n",
       " 'universal',\n",
       " 'truth',\n",
       " 'and',\n",
       " 'the',\n",
       " 'loving',\n",
       " 'personal',\n",
       " 'God.',\n",
       " 'David',\n",
       " 'Gutierrez',\n",
       " 'Natural',\n",
       " 'News',\n",
       " 'November',\n",
       " '3,',\n",
       " '2009',\n",
       " 'Awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'risks',\n",
       " 'over',\n",
       " 'the',\n",
       " 'smallpox',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'prevented',\n",
       " 'the',\n",
       " 'government',\n",
       " 'from',\n",
       " 'requiring',\n",
       " 'vaccination',\n",
       " 'of',\n",
       " 'civilians.',\n",
       " 'Approximately',\n",
       " '200',\n",
       " 'soldiers',\n",
       " 'have',\n",
       " 'suffered',\n",
       " 'from',\n",
       " 'serious',\n",
       " 'and',\n",
       " 'even',\n",
       " 'life-threatening',\n",
       " 'complications',\n",
       " 'from',\n",
       " 'the',\n",
       " 'government-mandated',\n",
       " 'smallpox',\n",
       " 'vaccine,',\n",
       " 'and',\n",
       " 'one',\n",
       " 'has',\n",
       " 'even',\n",
       " 'died.',\n",
       " 'Starting',\n",
       " 'in',\n",
       " '2002,',\n",
       " 'fears',\n",
       " 'over',\n",
       " 'a',\n",
       " 'bioterrorist',\n",
       " 'attack',\n",
       " 'have',\n",
       " 'led',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'government',\n",
       " 'to',\n",
       " 'require',\n",
       " 'that',\n",
       " 'all',\n",
       " 'of',\n",
       " 'its',\n",
       " 'military',\n",
       " 'servicepeople',\n",
       " 'receive',\n",
       " 'vaccination',\n",
       " 'against',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'diseases',\n",
       " 'before',\n",
       " 'deployment,',\n",
       " 'including',\n",
       " 'anthrax',\n",
       " 'and',\n",
       " 'smallpox.',\n",
       " 'An',\n",
       " 'estimated',\n",
       " '1.7',\n",
       " 'million',\n",
       " 'have',\n",
       " 'been',\n",
       " 'vaccinated',\n",
       " 'against',\n",
       " 'smallpox',\n",
       " 'since',\n",
       " 'then.',\n",
       " 'Yet',\n",
       " 'in',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'cases,',\n",
       " 'the',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'led',\n",
       " 'to',\n",
       " 'severe',\n",
       " 'complications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'inflammations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'brain',\n",
       " 'or',\n",
       " 'heart.',\n",
       " 'In',\n",
       " '2003,',\n",
       " 'two',\n",
       " 'expert',\n",
       " 'panels',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'Army',\n",
       " 'Specialist',\n",
       " 'Rachel',\n",
       " 'Ray',\n",
       " 'died',\n",
       " 'in',\n",
       " 'part',\n",
       " 'due',\n",
       " 'to',\n",
       " 'complications',\n",
       " 'from',\n",
       " 'the',\n",
       " 'deployment',\n",
       " 'vaccines',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'been',\n",
       " 'given.',\n",
       " '“The',\n",
       " 'reality',\n",
       " 'is,',\n",
       " 'we’re',\n",
       " 'never',\n",
       " 'going',\n",
       " 'to',\n",
       " 'have',\n",
       " 'zero',\n",
       " 'risk',\n",
       " 'on',\n",
       " 'a',\n",
       " 'vaccine,”',\n",
       " 'said',\n",
       " 'Dr.',\n",
       " 'Michael',\n",
       " 'Kilpatrick',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Military',\n",
       " 'Health',\n",
       " 'System.',\n",
       " '“There’s',\n",
       " 'always',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'that',\n",
       " 'individual',\n",
       " 'that',\n",
       " 'has',\n",
       " 'some',\n",
       " 'untoward',\n",
       " 'event',\n",
       " 'that',\n",
       " 'would',\n",
       " 'occur.”',\n",
       " 'Awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'risks',\n",
       " 'over',\n",
       " 'the',\n",
       " 'smallpox',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'prevented',\n",
       " 'the',\n",
       " 'government',\n",
       " 'from',\n",
       " 'requiring',\n",
       " 'vaccination',\n",
       " 'of',\n",
       " 'civilians.',\n",
       " 'One',\n",
       " 'potential',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'is',\n",
       " 'infection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'vaccine,',\n",
       " 'a',\n",
       " 'condition',\n",
       " 'known',\n",
       " 'as',\n",
       " 'progressive',\n",
       " 'vaccinia.',\n",
       " 'Back',\n",
       " 'when',\n",
       " 'smallpox',\n",
       " 'vaccination',\n",
       " 'was',\n",
       " 'widespread,',\n",
       " 'the',\n",
       " 'infection',\n",
       " 'had',\n",
       " 'a',\n",
       " '15',\n",
       " 'percent',\n",
       " 'fatality',\n",
       " 'rate.',\n",
       " 'Read',\n",
       " 'entire',\n",
       " 'article',\n",
       " 'Colorado',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'too',\n",
       " 'close',\n",
       " 'to',\n",
       " 'call',\n",
       " 'right',\n",
       " 'now.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'is.',\n",
       " 'Unions',\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hej', 'med', 'dig', 'hej', 'med', 'dig']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"hej\", \"med\", \"dig\"] + [\"hej\", \"med\", \"dig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model C - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'like': 1, 'apples': 2, 'love': 3, 'oranges': 4, 'She': 5, 'hates': 6, 'pears': 7, 'He': 8, 'dislikes': 9, 'bananas': 10}\n",
      "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "I [ 0.06881227 -0.13503245  0.13067168  0.16277544  0.1175928   0.14362365\n",
      " -0.05564805  0.05248439  0.08392346 -0.15577602]\n",
      "like [-0.14159463  0.15149534 -0.14801809 -0.1548039  -0.07514571 -0.1058769\n",
      "  0.05275943 -0.07659105 -0.15096149  0.1543007 ]\n",
      "apples [-0.0305975  -0.04509356  0.03028085  0.01999826 -0.00269739 -0.00310541\n",
      "  0.02410186 -0.04725834  0.02400244  0.04329363]\n",
      "love [-0.04896925 -0.00796487 -0.04935968 -0.01651973 -0.0330081  -0.00728096\n",
      "  0.00932461  0.00094824  0.04020016  0.00554812]\n",
      "oranges [-0.01422012 -0.01638101 -0.03042551  0.00591964 -0.01169596  0.04766793\n",
      " -0.02327204 -0.00236449  0.01989326  0.02445111]\n",
      "She [-0.00154535 -0.03032771  0.00611943 -0.0191129  -0.04995989 -0.01201131\n",
      " -0.03659074  0.02053057  0.04884037 -0.02892206]\n",
      "hates [-0.00135754 -0.02456735  0.04362607  0.00887834 -0.02004464 -0.04159974\n",
      " -0.01977385 -0.01681113 -0.00950124  0.04681111]\n",
      "pears [ 0.0256034   0.03112303 -0.00288578 -0.00851453  0.04429037 -0.0397085\n",
      "  0.04844022 -0.01593858  0.0112748  -0.02193377]\n",
      "He [ 0.02828482 -0.02189854  0.00758314  0.04874269 -0.04294729 -0.00764506\n",
      "  0.00889945 -0.00125474 -0.0445448   0.04981058]\n",
      "dislikes [ 0.00343751  0.00157746  0.03054855  0.02326491 -0.01804731  0.02913922\n",
      "  0.03489823  0.03713491  0.04956107  0.02662079]\n",
      "bananas [-0.04617471 -0.01782793  0.00274237 -0.00503201  0.01173111 -0.04105016\n",
      " -0.02255033  0.01316187 -0.01209651  0.04900146]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# Create a corpus of text data\n",
    "corpus = [\"I like apples\", \"I love oranges\", \"She hates pears\", \"He dislikes bananas\"]\n",
    "\n",
    "# Create a dictionary of words and their indices\n",
    "word_dict = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "print(word_dict)\n",
    "\n",
    "# Convert the corpus into a matrix of word indices\n",
    "corpus_matrix = np.zeros((len(corpus), len(word_dict)))\n",
    "for i, sentence in enumerate(corpus):\n",
    "    for word in sentence.split():\n",
    "        corpus_matrix[i, word_dict[word]] = 1\n",
    "\n",
    "print(corpus_matrix)\n",
    "\n",
    "# Create a neural network model with an embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_dict), 10, input_length=len(word_dict)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(corpus_matrix, np.array([1, 1, 0, 0]), epochs=100, verbose=0)\n",
    "\n",
    "# Get the learned word embeddings\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "# Print the learned embeddings for each word\n",
    "for word, index in word_dict.items():\n",
    "    print(word, embeddings[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('fake_news_dataset.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 1000\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Create the word embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
