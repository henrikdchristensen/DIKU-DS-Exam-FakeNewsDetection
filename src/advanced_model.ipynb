{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "\n",
    "import importlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 805975.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 200000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset_bin.csv\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                   out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'like': 1, 'apples': 2, 'love': 3, 'oranges': 4, 'She': 5, 'hates': 6, 'pears': 7, 'He': 8, 'dislikes': 9, 'bananas': 10}\n",
      "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "I [ 0.06881227 -0.13503245  0.13067168  0.16277544  0.1175928   0.14362365\n",
      " -0.05564805  0.05248439  0.08392346 -0.15577602]\n",
      "like [-0.14159463  0.15149534 -0.14801809 -0.1548039  -0.07514571 -0.1058769\n",
      "  0.05275943 -0.07659105 -0.15096149  0.1543007 ]\n",
      "apples [-0.0305975  -0.04509356  0.03028085  0.01999826 -0.00269739 -0.00310541\n",
      "  0.02410186 -0.04725834  0.02400244  0.04329363]\n",
      "love [-0.04896925 -0.00796487 -0.04935968 -0.01651973 -0.0330081  -0.00728096\n",
      "  0.00932461  0.00094824  0.04020016  0.00554812]\n",
      "oranges [-0.01422012 -0.01638101 -0.03042551  0.00591964 -0.01169596  0.04766793\n",
      " -0.02327204 -0.00236449  0.01989326  0.02445111]\n",
      "She [-0.00154535 -0.03032771  0.00611943 -0.0191129  -0.04995989 -0.01201131\n",
      " -0.03659074  0.02053057  0.04884037 -0.02892206]\n",
      "hates [-0.00135754 -0.02456735  0.04362607  0.00887834 -0.02004464 -0.04159974\n",
      " -0.01977385 -0.01681113 -0.00950124  0.04681111]\n",
      "pears [ 0.0256034   0.03112303 -0.00288578 -0.00851453  0.04429037 -0.0397085\n",
      "  0.04844022 -0.01593858  0.0112748  -0.02193377]\n",
      "He [ 0.02828482 -0.02189854  0.00758314  0.04874269 -0.04294729 -0.00764506\n",
      "  0.00889945 -0.00125474 -0.0445448   0.04981058]\n",
      "dislikes [ 0.00343751  0.00157746  0.03054855  0.02326491 -0.01804731  0.02913922\n",
      "  0.03489823  0.03713491  0.04956107  0.02662079]\n",
      "bananas [-0.04617471 -0.01782793  0.00274237 -0.00503201  0.01173111 -0.04105016\n",
      " -0.02255033  0.01316187 -0.01209651  0.04900146]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# Create a corpus of text data\n",
    "corpus = [\"I like apples\", \"I love oranges\", \"She hates pears\", \"He dislikes bananas\"]\n",
    "\n",
    "# Create a dictionary of words and their indices\n",
    "word_dict = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "print(word_dict)\n",
    "\n",
    "# Convert the corpus into a matrix of word indices\n",
    "corpus_matrix = np.zeros((len(corpus), len(word_dict)))\n",
    "for i, sentence in enumerate(corpus):\n",
    "    for word in sentence.split():\n",
    "        corpus_matrix[i, word_dict[word]] = 1\n",
    "\n",
    "print(corpus_matrix)\n",
    "\n",
    "# Create a neural network model with an embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_dict), 10, input_length=len(word_dict)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(corpus_matrix, np.array([1, 1, 0, 0]), epochs=100, verbose=0)\n",
    "\n",
    "# Get the learned word embeddings\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "# Print the learned embeddings for each word\n",
    "for word, index in word_dict.items():\n",
    "    print(word, embeddings[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('fake_news_dataset.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 1000\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Create the word embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
