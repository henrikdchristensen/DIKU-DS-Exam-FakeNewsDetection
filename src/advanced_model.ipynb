{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.27.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from time import time\n",
    "import importlib\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import preprocessing\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential, load_model\n",
    "\n",
    "from keras.models import Model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "\n",
    "from keras_preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout, GlobalAveragePooling1D\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "# from transformers import __version__; print(__version__)\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel, BertForMaskedLM\n",
    "\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 805975.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 200000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset_bin.csv\"\n",
    "BATCH_SIZE = 5000\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                   out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:09<00:00, 502.99it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 25805.73it/s]\n",
      "100%|██████████| 5000/5000 [00:04<00:00, 1195.88it/s]\n",
      "100%|██████████| 5000/5000 [00:32<00:00, 152.93it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 89278.12it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 156239.21it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 16890.04it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 69438.21it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 56812.60it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 7021.72it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 624747.38it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 312550.60it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 41663.81it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 36760.35it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 41662.65it/s]\n",
      "100%|██████████| 5000/5000 [00:00<00:00, 24996.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 50.90818643569946\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msplit_csr_data\u001b[39m(data: pd\u001b[39m.\u001b[39mDataFrame, feature: \u001b[39mstr\u001b[39m, y, stack_func, \u001b[39mset\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mset\u001b[39m\u001b[39m\"\u001b[39m, get_val\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     train \u001b[39m=\u001b[39m data[data[\u001b[39mset\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m     val \u001b[39m=\u001b[39m data[data[\u001b[39mset\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def split_csr_data(data: pd.DataFrame, feature: str, y, stack_func, set=\"set\", get_val=True):\n",
    "    train = data[data[set] == 0]\n",
    "    val = data[data[set] == 1]\n",
    "    test = data[data[set] == 2]\n",
    "    X_train, y_train = stack_func(train[feature]), train[y].astype(int)\n",
    "    X_val, y_val = stack_func(val[feature]), val[y].astype(int)\n",
    "    X_test, y_test = stack_func(test[feature]), test[y].astype(int)\n",
    "    if not get_val:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../datasets/sample/dataset_unbalanced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['content'].values\n",
    "y = df['type_binary'].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "125/125 [==============================] - 35s 257ms/step - loss: 0.5595 - accuracy: 0.7103 - val_loss: 0.4562 - val_accuracy: 0.7710\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.3328 - accuracy: 0.8580 - val_loss: 0.4303 - val_accuracy: 0.8040\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 32s 260ms/step - loss: 0.1814 - accuracy: 0.9283 - val_loss: 0.5382 - val_accuracy: 0.7870\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 34s 271ms/step - loss: 0.0855 - accuracy: 0.9728 - val_loss: 0.6164 - val_accuracy: 0.7870\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 36s 290ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 0.8345 - val_accuracy: 0.7890\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 38s 308ms/step - loss: 0.0619 - accuracy: 0.9790 - val_loss: 0.6924 - val_accuracy: 0.7640\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 36s 291ms/step - loss: 0.0588 - accuracy: 0.9785 - val_loss: 0.8416 - val_accuracy: 0.7700\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 36s 288ms/step - loss: 0.0105 - accuracy: 0.9983 - val_loss: 0.9763 - val_accuracy: 0.7700\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 35s 282ms/step - loss: 0.0044 - accuracy: 0.9995 - val_loss: 1.0699 - val_accuracy: 0.7670\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 36s 286ms/step - loss: 0.0030 - accuracy: 0.9995 - val_loss: 1.1170 - val_accuracy: 0.7830\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1fd46c0a8f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model B - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "df = pd.read_csv(\"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "\n",
    "# content cleaned\n",
    "tokenizer.fit_on_texts(df[df[\"set\"] == 0][\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "df[\"padded_sequences\"] = pad_sequences(sequences, maxlen=1000, truncating=\"post\").tolist()\n",
    "\n",
    "# all cleaned\n",
    "tokenizer.fit_on_texts(df[df[\"set\"] == 0][\"content_domain_authors_title\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content_domain_authors_title\"])\n",
    "df[\"padded_sequences_all\"] = pad_sequences(sequences, maxlen=1000, truncating=\"post\").tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model with feature: padded_sequences\n",
      "Training Model-A\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 2s 8ms/step - loss: 0.6889 - accuracy: 0.5792 - val_loss: 0.6874 - val_accuracy: 0.5420\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.6645 - accuracy: 0.6300 - val_loss: 0.6511 - val_accuracy: 0.6300\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.6163 - accuracy: 0.6710 - val_loss: 0.6043 - val_accuracy: 0.6880\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.5545 - accuracy: 0.7360 - val_loss: 0.5525 - val_accuracy: 0.7420\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.4922 - accuracy: 0.7803 - val_loss: 0.5175 - val_accuracy: 0.7600\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.4436 - accuracy: 0.8062 - val_loss: 0.4885 - val_accuracy: 0.7760\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.4092 - accuracy: 0.8198 - val_loss: 0.4719 - val_accuracy: 0.7920\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.3712 - accuracy: 0.8522 - val_loss: 0.4713 - val_accuracy: 0.7660\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.3390 - accuracy: 0.8650 - val_loss: 0.4539 - val_accuracy: 0.7980\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.3127 - accuracy: 0.8798 - val_loss: 0.4571 - val_accuracy: 0.7800\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4473 - accuracy: 0.7780\n",
      "Results: [0.4473037123680115, 0.777999997138977]\n",
      "Training Model-B\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 51s 381ms/step - loss: 0.6172 - accuracy: 0.6590 - val_loss: 0.5145 - val_accuracy: 0.7740\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 48s 381ms/step - loss: 0.4078 - accuracy: 0.8285 - val_loss: 0.4473 - val_accuracy: 0.7820\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 46s 370ms/step - loss: 0.2631 - accuracy: 0.9082 - val_loss: 0.5262 - val_accuracy: 0.7740\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 50s 399ms/step - loss: 0.1692 - accuracy: 0.9435 - val_loss: 0.4706 - val_accuracy: 0.7900\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 50s 402ms/step - loss: 0.1177 - accuracy: 0.9620 - val_loss: 0.5509 - val_accuracy: 0.7660\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 45s 362ms/step - loss: 0.0762 - accuracy: 0.9785 - val_loss: 0.6364 - val_accuracy: 0.7840\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 49s 392ms/step - loss: 0.0798 - accuracy: 0.9755 - val_loss: 0.6304 - val_accuracy: 0.7880\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 45s 360ms/step - loss: 0.0404 - accuracy: 0.9912 - val_loss: 0.7054 - val_accuracy: 0.7840\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 42s 339ms/step - loss: 0.0484 - accuracy: 0.9870 - val_loss: 0.9544 - val_accuracy: 0.7540\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 44s 354ms/step - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.7869 - val_accuracy: 0.7880\n",
      "16/16 [==============================] - 1s 73ms/step - loss: 0.6941 - accuracy: 0.7940\n",
      "Results: [0.6940779685974121, 0.7940000295639038]\n",
      "Training model with feature: padded_sequences_all\n",
      "Training Model-A\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.6488 - accuracy: 0.6538 - val_loss: 0.5634 - val_accuracy: 0.7360\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.5110 - accuracy: 0.7500 - val_loss: 0.5315 - val_accuracy: 0.7380\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.4505 - accuracy: 0.8025 - val_loss: 0.4976 - val_accuracy: 0.7900\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.4090 - accuracy: 0.8267 - val_loss: 0.5086 - val_accuracy: 0.7340\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.3734 - accuracy: 0.8518 - val_loss: 0.4948 - val_accuracy: 0.7440\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.3416 - accuracy: 0.8677 - val_loss: 0.4728 - val_accuracy: 0.7760\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.3118 - accuracy: 0.8845 - val_loss: 0.4506 - val_accuracy: 0.8080\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.2873 - accuracy: 0.8940 - val_loss: 0.4641 - val_accuracy: 0.7940\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.2636 - accuracy: 0.9060 - val_loss: 0.4301 - val_accuracy: 0.8340\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.2382 - accuracy: 0.9202 - val_loss: 0.4947 - val_accuracy: 0.7700\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 0.4246 - accuracy: 0.7940\n",
      "Results: [0.4245592951774597, 0.7940000295639038]\n",
      "Training Model-B\n",
      "Epoch 1/10\n",
      "125/125 [==============================] - 45s 362ms/step - loss: 0.5036 - accuracy: 0.7805 - val_loss: 0.3378 - val_accuracy: 0.8520\n",
      "Epoch 2/10\n",
      "125/125 [==============================] - 47s 375ms/step - loss: 0.2108 - accuracy: 0.9180 - val_loss: 0.2712 - val_accuracy: 0.8980\n",
      "Epoch 3/10\n",
      "125/125 [==============================] - 49s 390ms/step - loss: 0.1193 - accuracy: 0.9613 - val_loss: 0.2617 - val_accuracy: 0.9080\n",
      "Epoch 4/10\n",
      "125/125 [==============================] - 49s 394ms/step - loss: 0.0670 - accuracy: 0.9805 - val_loss: 0.2922 - val_accuracy: 0.9020\n",
      "Epoch 5/10\n",
      "125/125 [==============================] - 46s 366ms/step - loss: 0.0376 - accuracy: 0.9915 - val_loss: 0.3106 - val_accuracy: 0.9060\n",
      "Epoch 6/10\n",
      "125/125 [==============================] - 47s 374ms/step - loss: 0.0190 - accuracy: 0.9967 - val_loss: 0.3425 - val_accuracy: 0.9040\n",
      "Epoch 7/10\n",
      "125/125 [==============================] - 47s 373ms/step - loss: 0.0108 - accuracy: 0.9990 - val_loss: 0.3837 - val_accuracy: 0.8980\n",
      "Epoch 8/10\n",
      "125/125 [==============================] - 44s 352ms/step - loss: 0.0098 - accuracy: 0.9985 - val_loss: 0.3977 - val_accuracy: 0.8940\n",
      "Epoch 9/10\n",
      "125/125 [==============================] - 44s 350ms/step - loss: 0.0054 - accuracy: 0.9998 - val_loss: 0.4098 - val_accuracy: 0.8940\n",
      "Epoch 10/10\n",
      "125/125 [==============================] - 44s 349ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.4170 - val_accuracy: 0.8940\n",
      "16/16 [==============================] - 1s 63ms/step - loss: 0.4302 - accuracy: 0.8720\n",
      "Results: [0.43015122413635254, 0.871999979019165]\n"
     ]
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "models = [\n",
    "    (tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000),\n",
    "        tf.keras.layers.GlobalAveragePooling1D(),\n",
    "        tf.keras.layers.Dense(16, activation=\"relu\"),\n",
    "        tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    ]), \"Model-A\"),\n",
    "    # Results: [0.4473037123680115, 0.777999997138977]\n",
    "    # Results: [0.4245592951774597, 0.7940000295639038]\n",
    "    (tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000), # 10000 words, 64 dimensions\n",
    "        tf.keras.layers.LSTM(units=16, dropout=0.2, recurrent_dropout=0.2),\n",
    "        tf.keras.layers.Dense(units=1, activation='sigmoid')\n",
    "    ]), \"Model-B\")\n",
    "    # Results: [0.6940779685974121, 0.7940000295639038]\n",
    "    # Results: [0.43015122413635254, 0.871999979019165]\n",
    "]\n",
    "# Compile the model\n",
    "for model, name in models:\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "for feature in [\"padded_sequences\", \"padded_sequences_all\"]:\n",
    "    print(\"Training model with feature: {}\".format(feature))\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test = split_csr_data(df, feature, \"type_binary\", lambda x: np.array(x.tolist()), get_val=True)\n",
    "\n",
    "    for model, name in models:\n",
    "        print(f\"Training {name}\")\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))\n",
    "\n",
    "        # Evaluate the model\n",
    "        print(f\"Results:\", model.evaluate(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "125/125 [==============================] - 2s 8ms/step - loss: 0.6741 - accuracy: 0.5870 - val_loss: 0.6326 - val_accuracy: 0.6420\n",
      "Epoch 2/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.5415 - accuracy: 0.7070 - val_loss: 0.5130 - val_accuracy: 0.7020\n",
      "Epoch 3/30\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.3882 - accuracy: 0.8250 - val_loss: 0.4208 - val_accuracy: 0.8220\n",
      "Epoch 4/30\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.2817 - accuracy: 0.8867 - val_loss: 0.4082 - val_accuracy: 0.8060\n",
      "Epoch 5/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.2077 - accuracy: 0.9160 - val_loss: 0.4322 - val_accuracy: 0.8240\n",
      "Epoch 6/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.1561 - accuracy: 0.9400 - val_loss: 0.4569 - val_accuracy: 0.8160\n",
      "Epoch 7/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.1199 - accuracy: 0.9517 - val_loss: 0.4958 - val_accuracy: 0.8280\n",
      "Epoch 8/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.1000 - accuracy: 0.9605 - val_loss: 0.5432 - val_accuracy: 0.8080\n",
      "Epoch 9/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0755 - accuracy: 0.9730 - val_loss: 0.5898 - val_accuracy: 0.8280\n",
      "Epoch 10/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0624 - accuracy: 0.9783 - val_loss: 0.6328 - val_accuracy: 0.8080\n",
      "Epoch 11/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0555 - accuracy: 0.9790 - val_loss: 0.7137 - val_accuracy: 0.8000\n",
      "Epoch 12/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0460 - accuracy: 0.9860 - val_loss: 0.7318 - val_accuracy: 0.8140\n",
      "Epoch 13/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0344 - accuracy: 0.9908 - val_loss: 0.7833 - val_accuracy: 0.8040\n",
      "Epoch 14/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0432 - accuracy: 0.9803 - val_loss: 1.1225 - val_accuracy: 0.7580\n",
      "Epoch 15/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0348 - accuracy: 0.9877 - val_loss: 0.8967 - val_accuracy: 0.8020\n",
      "Epoch 16/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0226 - accuracy: 0.9927 - val_loss: 1.0704 - val_accuracy: 0.7660\n",
      "Epoch 17/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0272 - accuracy: 0.9898 - val_loss: 0.9709 - val_accuracy: 0.8160\n",
      "Epoch 18/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0178 - accuracy: 0.9950 - val_loss: 0.9836 - val_accuracy: 0.8060\n",
      "Epoch 19/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0112 - accuracy: 0.9980 - val_loss: 1.0280 - val_accuracy: 0.8100\n",
      "Epoch 20/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0136 - accuracy: 0.9965 - val_loss: 1.3232 - val_accuracy: 0.7660\n",
      "Epoch 21/30\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0289 - accuracy: 0.9900 - val_loss: 1.1056 - val_accuracy: 0.8080\n",
      "Epoch 22/30\n",
      "125/125 [==============================] - 1s 8ms/step - loss: 0.0087 - accuracy: 0.9980 - val_loss: 1.1519 - val_accuracy: 0.8120\n",
      "Epoch 23/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0064 - accuracy: 0.9983 - val_loss: 1.1716 - val_accuracy: 0.8160\n",
      "Epoch 24/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0098 - accuracy: 0.9965 - val_loss: 1.1818 - val_accuracy: 0.8180\n",
      "Epoch 25/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0060 - accuracy: 0.9990 - val_loss: 1.2363 - val_accuracy: 0.8140\n",
      "Epoch 26/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 1.3129 - val_accuracy: 0.7940\n",
      "Epoch 27/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0038 - accuracy: 0.9992 - val_loss: 1.2794 - val_accuracy: 0.8080\n",
      "Epoch 28/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0060 - accuracy: 0.9987 - val_loss: 1.3080 - val_accuracy: 0.8140\n",
      "Epoch 29/30\n",
      "125/125 [==============================] - 1s 7ms/step - loss: 0.0052 - accuracy: 0.9985 - val_loss: 1.3475 - val_accuracy: 0.8200\n",
      "Epoch 30/30\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.0032 - accuracy: 0.9995 - val_loss: 1.4193 - val_accuracy: 0.7980\n",
      "16/16 [==============================] - 0s 3ms/step - loss: 1.2360 - accuracy: 0.8020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.2360297441482544, 0.8019999861717224]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=64, input_length=max_len),\n",
    "    LSTM(units=64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(units=1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "\n",
    "\n",
    "# [1.0781421661376953, 0.800000011920929]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 15719.50it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf1 = pd.read_csv(\"../datasets/sample/dataset_unbalanced.csv\")\n",
    "pdf2 = pp.apply_pipeline_pd_tqdm(pdf1, [(pp.Tokenizer(), \"content\", \"tokenized\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "_ = pdf2[\"tokenized\"].apply(lambda x: list.extend(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['To',\n",
       " 'the',\n",
       " 'Editor:',\n",
       " 'Re',\n",
       " \"''Drop\",\n",
       " 'Out',\n",
       " 'of',\n",
       " 'the',\n",
       " \"College''\",\n",
       " '(editorial,',\n",
       " 'March',\n",
       " '14):',\n",
       " 'Having',\n",
       " 'written',\n",
       " 'in',\n",
       " 'favor',\n",
       " 'of',\n",
       " 'Electoral',\n",
       " 'College',\n",
       " 'reform',\n",
       " 'since',\n",
       " '2000,',\n",
       " 'I,',\n",
       " 'too,',\n",
       " 'would',\n",
       " 'prefer',\n",
       " 'the',\n",
       " 'popular',\n",
       " 'election',\n",
       " 'of',\n",
       " 'the',\n",
       " 'president.',\n",
       " 'But',\n",
       " 'the',\n",
       " 'gimmick',\n",
       " 'you',\n",
       " 'endorse,',\n",
       " 'of',\n",
       " 'having',\n",
       " 'individual',\n",
       " 'states',\n",
       " 'bind',\n",
       " 'their',\n",
       " 'electors',\n",
       " 'to',\n",
       " 'vote',\n",
       " 'for',\n",
       " 'the',\n",
       " 'national',\n",
       " 'popular-vote',\n",
       " 'winner,',\n",
       " 'seems',\n",
       " 'problematic',\n",
       " 'for',\n",
       " 'one',\n",
       " 'basic',\n",
       " 'reason.',\n",
       " 'What',\n",
       " 'is',\n",
       " 'to',\n",
       " 'stop',\n",
       " 'state',\n",
       " 'legislatures',\n",
       " 'with',\n",
       " 'strong',\n",
       " 'partisan',\n",
       " 'loyalties',\n",
       " 'of',\n",
       " 'their',\n",
       " 'own',\n",
       " 'from',\n",
       " 'abandoning',\n",
       " 'such',\n",
       " 'an',\n",
       " 'agreement',\n",
       " 'when',\n",
       " 'urgent',\n",
       " 'calculations',\n",
       " 'of',\n",
       " 'party',\n",
       " 'advantage',\n",
       " 'come',\n",
       " 'to',\n",
       " 'the',\n",
       " 'fore?',\n",
       " 'What',\n",
       " 'one',\n",
       " 'legislature',\n",
       " 'can',\n",
       " 'do,',\n",
       " 'another',\n",
       " 'can',\n",
       " 'undo.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'end,',\n",
       " 'difficult',\n",
       " 'as',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be,',\n",
       " 'presidential-election',\n",
       " 'reform',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'taking',\n",
       " 'the',\n",
       " 'amendment',\n",
       " 'process',\n",
       " 'seriously.',\n",
       " 'And',\n",
       " \"that's\",\n",
       " 'what',\n",
       " 'the',\n",
       " 'examples',\n",
       " 'your',\n",
       " 'editorial',\n",
       " 'cites',\n",
       " 'in',\n",
       " 'conclusion',\n",
       " 'demonstrate.',\n",
       " 'Although',\n",
       " 'individual',\n",
       " 'states',\n",
       " 'did',\n",
       " 'set',\n",
       " 'legislative',\n",
       " 'precedents',\n",
       " 'for',\n",
       " 'granting',\n",
       " 'the',\n",
       " 'suffrage',\n",
       " 'to',\n",
       " 'African-Americans',\n",
       " 'and',\n",
       " 'to',\n",
       " 'women,',\n",
       " 'and',\n",
       " 'also',\n",
       " 'for',\n",
       " 'the',\n",
       " 'popular',\n",
       " 'election',\n",
       " 'of',\n",
       " 'senators,',\n",
       " 'in',\n",
       " 'the',\n",
       " 'end',\n",
       " 'these',\n",
       " 'rights',\n",
       " 'were',\n",
       " 'entrenched',\n",
       " 'through',\n",
       " 'constitutional',\n",
       " 'amendments,',\n",
       " 'not',\n",
       " 'left',\n",
       " 'to',\n",
       " 'unstable',\n",
       " 'legislative',\n",
       " 'gimmickry.',\n",
       " 'Jack',\n",
       " 'Rakove',\n",
       " 'Stanford,',\n",
       " 'Calif.,',\n",
       " 'March',\n",
       " '14,',\n",
       " '2006',\n",
       " 'The',\n",
       " 'writer',\n",
       " 'is',\n",
       " 'a',\n",
       " 'professor',\n",
       " 'of',\n",
       " 'history,',\n",
       " 'American',\n",
       " 'studies',\n",
       " 'and',\n",
       " 'political',\n",
       " 'science',\n",
       " 'at',\n",
       " 'Stanford',\n",
       " 'University.',\n",
       " 'Drawing',\n",
       " '(Drawing',\n",
       " 'by',\n",
       " 'Thomas',\n",
       " 'Fuchs)',\n",
       " 'So',\n",
       " 'I’ve',\n",
       " 'written',\n",
       " 'for',\n",
       " 'another',\n",
       " 'channel',\n",
       " 'my',\n",
       " 'advice',\n",
       " 'for',\n",
       " 'how',\n",
       " 'Republicans',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Senate',\n",
       " 'should',\n",
       " 'scrupulously',\n",
       " 'adhere',\n",
       " 'to',\n",
       " 'constitutional',\n",
       " 'forms',\n",
       " 'and',\n",
       " 'Scalia’s',\n",
       " 'constitutionalism',\n",
       " 'in',\n",
       " 'entering',\n",
       " 'into',\n",
       " 'the',\n",
       " 'battle',\n",
       " 'to',\n",
       " 'replace',\n",
       " 'the',\n",
       " 'irreplaceable',\n",
       " 'justice.',\n",
       " 'I',\n",
       " 'agree',\n",
       " 'with',\n",
       " 'Pete',\n",
       " 'below',\n",
       " 'that',\n",
       " 'the',\n",
       " 'Republicans',\n",
       " 'have',\n",
       " 'become',\n",
       " 'an',\n",
       " 'ideological',\n",
       " 'disaster.',\n",
       " 'Tyler',\n",
       " 'Cowen,',\n",
       " 'a',\n",
       " 'very',\n",
       " 'thoughtful',\n",
       " 'and',\n",
       " 'erudite,',\n",
       " 'fairly',\n",
       " 'libertarian',\n",
       " 'economist,',\n",
       " 'reflects',\n",
       " 'on',\n",
       " 'what',\n",
       " 'libertarians',\n",
       " 'should',\n",
       " 'learn',\n",
       " 'from',\n",
       " 'the',\n",
       " 'unexpected',\n",
       " 'rise',\n",
       " 'of',\n",
       " 'Trump.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'his',\n",
       " 'takeaways:',\n",
       " 'True',\n",
       " 'cosmopolitans',\n",
       " 'are',\n",
       " 'hard',\n",
       " 'to',\n",
       " 'find.',\n",
       " 'I',\n",
       " 'guess',\n",
       " 'that’s',\n",
       " 'true,',\n",
       " 'although',\n",
       " 'it',\n",
       " 'depends',\n",
       " 'on',\n",
       " 'what',\n",
       " 'you',\n",
       " 'mean',\n",
       " 'by',\n",
       " 'cosmopolitan.',\n",
       " 'From',\n",
       " 'a',\n",
       " 'libertarian',\n",
       " 'economist’s',\n",
       " 'view',\n",
       " '(as',\n",
       " 'I’ve',\n",
       " 'learned',\n",
       " 'from',\n",
       " 'Cowen’s',\n",
       " 'books),',\n",
       " 'a',\n",
       " 'cosmopolitan',\n",
       " 'is',\n",
       " 'a',\n",
       " 'free',\n",
       " 'individual',\n",
       " 'who’s',\n",
       " 'been',\n",
       " 'productive',\n",
       " 'enough',\n",
       " 'to',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'enjoy,',\n",
       " 'from',\n",
       " 'a',\n",
       " 'detached',\n",
       " 'or',\n",
       " 'abstracted',\n",
       " 'and',\n",
       " 'displaced',\n",
       " 'or',\n",
       " 'multicultural',\n",
       " 'point',\n",
       " 'of',\n",
       " 'view,',\n",
       " 'all',\n",
       " 'that',\n",
       " 'the',\n",
       " 'various',\n",
       " 'cultures',\n",
       " 'and',\n",
       " 'peoples',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world',\n",
       " 'have',\n",
       " 'to',\n",
       " 'offer.',\n",
       " 'The',\n",
       " 'foodie',\n",
       " 'Cowen,',\n",
       " 'for',\n",
       " 'example,',\n",
       " 'can',\n",
       " 'enjoy',\n",
       " 'French',\n",
       " 'or',\n",
       " 'Ethiopian',\n",
       " 'food',\n",
       " 'without',\n",
       " 'all',\n",
       " 'the',\n",
       " 'repressive',\n",
       " 'baggage',\n",
       " 'of',\n",
       " 'actually',\n",
       " 'being',\n",
       " 'French',\n",
       " 'or',\n",
       " 'Ethiopian.',\n",
       " 'And',\n",
       " 'it’s',\n",
       " 'true',\n",
       " 'enough,',\n",
       " 'after',\n",
       " 'all,',\n",
       " 'that',\n",
       " 'those',\n",
       " 'people',\n",
       " 'free',\n",
       " 'enough',\n",
       " 'through',\n",
       " 'habits',\n",
       " 'of',\n",
       " 'abstract',\n",
       " 'thinking',\n",
       " 'to',\n",
       " 'be',\n",
       " 'facile',\n",
       " 'role',\n",
       " 'players',\n",
       " 'deploying',\n",
       " 'their',\n",
       " 'technical',\n",
       " 'skills',\n",
       " 'in',\n",
       " 'a',\n",
       " 'wide',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'settings',\n",
       " 'are',\n",
       " 'those',\n",
       " 'most',\n",
       " 'prepared',\n",
       " 'to',\n",
       " 'flourish',\n",
       " 'in',\n",
       " 'the',\n",
       " '21st-century',\n",
       " 'global',\n",
       " 'competitive',\n",
       " 'marketplace.',\n",
       " 'Still,',\n",
       " 'someone',\n",
       " 'might',\n",
       " 'say,',\n",
       " 'that',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'detachment',\n",
       " 'isn’t',\n",
       " 'true',\n",
       " 'cosmopolitanism.',\n",
       " 'To',\n",
       " 'be',\n",
       " 'a',\n",
       " 'genuine',\n",
       " 'citizen',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world,',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'community',\n",
       " 'of',\n",
       " 'philosophers',\n",
       " 'dedicated',\n",
       " 'to',\n",
       " 'pursuing',\n",
       " 'the',\n",
       " 'truth',\n",
       " 'we',\n",
       " 'all',\n",
       " 'share.',\n",
       " 'Or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'necessary',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'God,',\n",
       " 'that',\n",
       " 'loving',\n",
       " 'and',\n",
       " 'relational',\n",
       " 'personal',\n",
       " 'community',\n",
       " 'that',\n",
       " 'includes',\n",
       " 'us',\n",
       " 'all.',\n",
       " 'Or',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'to',\n",
       " 'be',\n",
       " 'a',\n",
       " 'member',\n",
       " 'of',\n",
       " 'a',\n",
       " 'Stoic',\n",
       " 'community',\n",
       " 'of',\n",
       " 'rational',\n",
       " 'and',\n",
       " 'virtuous',\n",
       " 'men',\n",
       " 'and',\n",
       " 'women',\n",
       " 'across',\n",
       " 'time',\n",
       " 'and',\n",
       " 'space',\n",
       " 'who',\n",
       " 'know',\n",
       " 'one',\n",
       " 'another',\n",
       " '—',\n",
       " 'Pericles,',\n",
       " 'Marcus',\n",
       " 'Aurelius,',\n",
       " 'and',\n",
       " 'Robert',\n",
       " 'E.',\n",
       " 'Lee',\n",
       " '—',\n",
       " 'as',\n",
       " 'of',\n",
       " 'the',\n",
       " 'same',\n",
       " 'kind.',\n",
       " 'I',\n",
       " 'could',\n",
       " 'go',\n",
       " 'on,',\n",
       " 'but',\n",
       " 'a',\n",
       " 'true',\n",
       " 'cosmopolitan',\n",
       " 'isn’t',\n",
       " 'merely',\n",
       " 'free',\n",
       " 'from',\n",
       " 'relational',\n",
       " 'prejudices',\n",
       " 'to',\n",
       " 'maximize',\n",
       " 'his',\n",
       " 'personal',\n",
       " 'productivity',\n",
       " 'in',\n",
       " 'the',\n",
       " 'service',\n",
       " 'of',\n",
       " 'random',\n",
       " 'personal',\n",
       " 'preferences.',\n",
       " 'It',\n",
       " 'really',\n",
       " 'is',\n",
       " 'true,',\n",
       " 'I',\n",
       " 'agree,',\n",
       " 'that',\n",
       " 'Trumpism',\n",
       " 'and',\n",
       " 'so',\n",
       " 'forth',\n",
       " 'are,',\n",
       " 'at',\n",
       " 'some',\n",
       " 'level',\n",
       " 'and',\n",
       " 'to',\n",
       " 'some',\n",
       " 'extent,',\n",
       " 'rebellions',\n",
       " 'against',\n",
       " 'the',\n",
       " 'emptiness',\n",
       " 'or',\n",
       " 'irresponsibility',\n",
       " 'of',\n",
       " 'the',\n",
       " 'cosmopolitanism',\n",
       " 'of',\n",
       " 'a',\n",
       " 'merely',\n",
       " 'cognitive',\n",
       " 'elite,',\n",
       " 'one',\n",
       " 'distinguished',\n",
       " 'only',\n",
       " 'by',\n",
       " 'its',\n",
       " 'marvelously',\n",
       " 'unprecedented',\n",
       " 'capacity',\n",
       " 'for',\n",
       " 'almost',\n",
       " 'infinitely',\n",
       " 'productive',\n",
       " 'mental',\n",
       " 'labor.',\n",
       " 'Their',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'faux',\n",
       " 'cosmopolitanism',\n",
       " 'is',\n",
       " 'an',\n",
       " 'experience',\n",
       " 'for',\n",
       " 'a',\n",
       " 'privileged',\n",
       " 'few,',\n",
       " 'one',\n",
       " 'not',\n",
       " 'as',\n",
       " 'accompanied',\n",
       " 'as',\n",
       " 'it',\n",
       " 'should',\n",
       " 'be',\n",
       " 'by',\n",
       " 'corresponding',\n",
       " 'responsibilities.',\n",
       " 'So',\n",
       " 'one',\n",
       " 'problem',\n",
       " 'with',\n",
       " 'some',\n",
       " 'libertarians',\n",
       " 'is',\n",
       " 'not',\n",
       " 'thinking',\n",
       " 'of',\n",
       " 'citizenship',\n",
       " 'as',\n",
       " 'a',\n",
       " 'real',\n",
       " 'and',\n",
       " 'ennobling',\n",
       " 'human',\n",
       " 'experience.',\n",
       " 'It’s',\n",
       " 'not',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'shared',\n",
       " 'all',\n",
       " 'that',\n",
       " 'much',\n",
       " 'by',\n",
       " 'even',\n",
       " 'Augustinian',\n",
       " 'Christians',\n",
       " 'who',\n",
       " 'don’t',\n",
       " 'think',\n",
       " 'that',\n",
       " 'the',\n",
       " 'political',\n",
       " 'diversity',\n",
       " 'of',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'Man',\n",
       " 'is',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'be',\n",
       " 'displaced',\n",
       " 'by',\n",
       " 'the',\n",
       " 'City',\n",
       " 'of',\n",
       " 'God.',\n",
       " 'Most',\n",
       " 'people',\n",
       " 'find',\n",
       " 'personal',\n",
       " 'fulfillment',\n",
       " 'through',\n",
       " 'love',\n",
       " 'and',\n",
       " 'work',\n",
       " 'in',\n",
       " 'institutions',\n",
       " 'that',\n",
       " 'correspond',\n",
       " 'to',\n",
       " 'the',\n",
       " 'limitations',\n",
       " 'of',\n",
       " 'our',\n",
       " 'existence',\n",
       " 'as',\n",
       " 'embodied',\n",
       " 'animals',\n",
       " 'with',\n",
       " 'relational',\n",
       " 'longings.',\n",
       " 'And',\n",
       " 'those',\n",
       " 'limitations',\n",
       " 'aren’t',\n",
       " 'even',\n",
       " 'limitations',\n",
       " 'exactly;',\n",
       " 'they',\n",
       " 'shape',\n",
       " 'our',\n",
       " 'experiences',\n",
       " 'of',\n",
       " 'who',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'is',\n",
       " 'and',\n",
       " 'what',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'is',\n",
       " 'supposed',\n",
       " 'to',\n",
       " 'do.',\n",
       " 'Before',\n",
       " 'we',\n",
       " 'can',\n",
       " 'be',\n",
       " 'citizens',\n",
       " 'of',\n",
       " 'the',\n",
       " 'world,',\n",
       " 'someone',\n",
       " 'might',\n",
       " 'say,',\n",
       " 'we',\n",
       " 'must',\n",
       " 'be',\n",
       " 'citizens',\n",
       " 'of',\n",
       " 'a',\n",
       " 'particular',\n",
       " 'country.',\n",
       " 'And',\n",
       " 'in',\n",
       " 'the',\n",
       " 'most',\n",
       " 'truthfully',\n",
       " 'self-conscious',\n",
       " 'cases,',\n",
       " 'each',\n",
       " 'of',\n",
       " 'us',\n",
       " 'will',\n",
       " 'be',\n",
       " 'both.',\n",
       " 'It’s',\n",
       " 'through',\n",
       " 'our',\n",
       " 'particular',\n",
       " 'experiences',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'limited',\n",
       " 'number',\n",
       " 'of',\n",
       " 'places',\n",
       " 'that',\n",
       " 'we',\n",
       " 'get',\n",
       " 'what',\n",
       " 'access',\n",
       " 'we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'the',\n",
       " 'universal',\n",
       " 'truth',\n",
       " 'and',\n",
       " 'the',\n",
       " 'loving',\n",
       " 'personal',\n",
       " 'God.',\n",
       " 'David',\n",
       " 'Gutierrez',\n",
       " 'Natural',\n",
       " 'News',\n",
       " 'November',\n",
       " '3,',\n",
       " '2009',\n",
       " 'Awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'risks',\n",
       " 'over',\n",
       " 'the',\n",
       " 'smallpox',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'prevented',\n",
       " 'the',\n",
       " 'government',\n",
       " 'from',\n",
       " 'requiring',\n",
       " 'vaccination',\n",
       " 'of',\n",
       " 'civilians.',\n",
       " 'Approximately',\n",
       " '200',\n",
       " 'soldiers',\n",
       " 'have',\n",
       " 'suffered',\n",
       " 'from',\n",
       " 'serious',\n",
       " 'and',\n",
       " 'even',\n",
       " 'life-threatening',\n",
       " 'complications',\n",
       " 'from',\n",
       " 'the',\n",
       " 'government-mandated',\n",
       " 'smallpox',\n",
       " 'vaccine,',\n",
       " 'and',\n",
       " 'one',\n",
       " 'has',\n",
       " 'even',\n",
       " 'died.',\n",
       " 'Starting',\n",
       " 'in',\n",
       " '2002,',\n",
       " 'fears',\n",
       " 'over',\n",
       " 'a',\n",
       " 'bioterrorist',\n",
       " 'attack',\n",
       " 'have',\n",
       " 'led',\n",
       " 'the',\n",
       " 'U.S.',\n",
       " 'government',\n",
       " 'to',\n",
       " 'require',\n",
       " 'that',\n",
       " 'all',\n",
       " 'of',\n",
       " 'its',\n",
       " 'military',\n",
       " 'servicepeople',\n",
       " 'receive',\n",
       " 'vaccination',\n",
       " 'against',\n",
       " 'a',\n",
       " 'variety',\n",
       " 'of',\n",
       " 'diseases',\n",
       " 'before',\n",
       " 'deployment,',\n",
       " 'including',\n",
       " 'anthrax',\n",
       " 'and',\n",
       " 'smallpox.',\n",
       " 'An',\n",
       " 'estimated',\n",
       " '1.7',\n",
       " 'million',\n",
       " 'have',\n",
       " 'been',\n",
       " 'vaccinated',\n",
       " 'against',\n",
       " 'smallpox',\n",
       " 'since',\n",
       " 'then.',\n",
       " 'Yet',\n",
       " 'in',\n",
       " 'a',\n",
       " 'number',\n",
       " 'of',\n",
       " 'cases,',\n",
       " 'the',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'led',\n",
       " 'to',\n",
       " 'severe',\n",
       " 'complications',\n",
       " 'such',\n",
       " 'as',\n",
       " 'inflammations',\n",
       " 'of',\n",
       " 'the',\n",
       " 'brain',\n",
       " 'or',\n",
       " 'heart.',\n",
       " 'In',\n",
       " '2003,',\n",
       " 'two',\n",
       " 'expert',\n",
       " 'panels',\n",
       " 'concluded',\n",
       " 'that',\n",
       " 'Army',\n",
       " 'Specialist',\n",
       " 'Rachel',\n",
       " 'Ray',\n",
       " 'died',\n",
       " 'in',\n",
       " 'part',\n",
       " 'due',\n",
       " 'to',\n",
       " 'complications',\n",
       " 'from',\n",
       " 'the',\n",
       " 'deployment',\n",
       " 'vaccines',\n",
       " 'that',\n",
       " 'she',\n",
       " 'had',\n",
       " 'been',\n",
       " 'given.',\n",
       " '“The',\n",
       " 'reality',\n",
       " 'is,',\n",
       " 'we’re',\n",
       " 'never',\n",
       " 'going',\n",
       " 'to',\n",
       " 'have',\n",
       " 'zero',\n",
       " 'risk',\n",
       " 'on',\n",
       " 'a',\n",
       " 'vaccine,”',\n",
       " 'said',\n",
       " 'Dr.',\n",
       " 'Michael',\n",
       " 'Kilpatrick',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Military',\n",
       " 'Health',\n",
       " 'System.',\n",
       " '“There’s',\n",
       " 'always',\n",
       " 'going',\n",
       " 'to',\n",
       " 'be',\n",
       " 'that',\n",
       " 'individual',\n",
       " 'that',\n",
       " 'has',\n",
       " 'some',\n",
       " 'untoward',\n",
       " 'event',\n",
       " 'that',\n",
       " 'would',\n",
       " 'occur.”',\n",
       " 'Awareness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'risks',\n",
       " 'over',\n",
       " 'the',\n",
       " 'smallpox',\n",
       " 'vaccine',\n",
       " 'has',\n",
       " 'prevented',\n",
       " 'the',\n",
       " 'government',\n",
       " 'from',\n",
       " 'requiring',\n",
       " 'vaccination',\n",
       " 'of',\n",
       " 'civilians.',\n",
       " 'One',\n",
       " 'potential',\n",
       " 'side',\n",
       " 'effect',\n",
       " 'is',\n",
       " 'infection',\n",
       " 'with',\n",
       " 'the',\n",
       " 'virus',\n",
       " 'used',\n",
       " 'in',\n",
       " 'the',\n",
       " 'vaccine,',\n",
       " 'a',\n",
       " 'condition',\n",
       " 'known',\n",
       " 'as',\n",
       " 'progressive',\n",
       " 'vaccinia.',\n",
       " 'Back',\n",
       " 'when',\n",
       " 'smallpox',\n",
       " 'vaccination',\n",
       " 'was',\n",
       " 'widespread,',\n",
       " 'the',\n",
       " 'infection',\n",
       " 'had',\n",
       " 'a',\n",
       " '15',\n",
       " 'percent',\n",
       " 'fatality',\n",
       " 'rate.',\n",
       " 'Read',\n",
       " 'entire',\n",
       " 'article',\n",
       " 'Colorado',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'too',\n",
       " 'close',\n",
       " 'to',\n",
       " 'call',\n",
       " 'right',\n",
       " 'now.',\n",
       " 'But',\n",
       " 'it',\n",
       " 'is.',\n",
       " 'Unions',\n",
       " ...]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hej', 'med', 'dig', 'hej', 'med', 'dig']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"hej\", \"med\", \"dig\"] + [\"hej\", \"med\", \"dig\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model C - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model D - tensorflow (word embedding, neural network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I': 0, 'like': 1, 'apples': 2, 'love': 3, 'oranges': 4, 'She': 5, 'hates': 6, 'pears': 7, 'He': 8, 'dislikes': 9, 'bananas': 10}\n",
      "[[1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1.]]\n",
      "I [ 0.06881227 -0.13503245  0.13067168  0.16277544  0.1175928   0.14362365\n",
      " -0.05564805  0.05248439  0.08392346 -0.15577602]\n",
      "like [-0.14159463  0.15149534 -0.14801809 -0.1548039  -0.07514571 -0.1058769\n",
      "  0.05275943 -0.07659105 -0.15096149  0.1543007 ]\n",
      "apples [-0.0305975  -0.04509356  0.03028085  0.01999826 -0.00269739 -0.00310541\n",
      "  0.02410186 -0.04725834  0.02400244  0.04329363]\n",
      "love [-0.04896925 -0.00796487 -0.04935968 -0.01651973 -0.0330081  -0.00728096\n",
      "  0.00932461  0.00094824  0.04020016  0.00554812]\n",
      "oranges [-0.01422012 -0.01638101 -0.03042551  0.00591964 -0.01169596  0.04766793\n",
      " -0.02327204 -0.00236449  0.01989326  0.02445111]\n",
      "She [-0.00154535 -0.03032771  0.00611943 -0.0191129  -0.04995989 -0.01201131\n",
      " -0.03659074  0.02053057  0.04884037 -0.02892206]\n",
      "hates [-0.00135754 -0.02456735  0.04362607  0.00887834 -0.02004464 -0.04159974\n",
      " -0.01977385 -0.01681113 -0.00950124  0.04681111]\n",
      "pears [ 0.0256034   0.03112303 -0.00288578 -0.00851453  0.04429037 -0.0397085\n",
      "  0.04844022 -0.01593858  0.0112748  -0.02193377]\n",
      "He [ 0.02828482 -0.02189854  0.00758314  0.04874269 -0.04294729 -0.00764506\n",
      "  0.00889945 -0.00125474 -0.0445448   0.04981058]\n",
      "dislikes [ 0.00343751  0.00157746  0.03054855  0.02326491 -0.01804731  0.02913922\n",
      "  0.03489823  0.03713491  0.04956107  0.02662079]\n",
      "bananas [-0.04617471 -0.01782793  0.00274237 -0.00503201  0.01173111 -0.04105016\n",
      " -0.02255033  0.01316187 -0.01209651  0.04900146]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Flatten\n",
    "\n",
    "# Create a corpus of text data\n",
    "corpus = [\"I like apples\", \"I love oranges\", \"She hates pears\", \"He dislikes bananas\"]\n",
    "\n",
    "# Create a dictionary of words and their indices\n",
    "word_dict = {}\n",
    "for sentence in corpus:\n",
    "    for word in sentence.split():\n",
    "        if word not in word_dict:\n",
    "            word_dict[word] = len(word_dict)\n",
    "\n",
    "print(word_dict)\n",
    "\n",
    "# Convert the corpus into a matrix of word indices\n",
    "corpus_matrix = np.zeros((len(corpus), len(word_dict)))\n",
    "for i, sentence in enumerate(corpus):\n",
    "    for word in sentence.split():\n",
    "        corpus_matrix[i, word_dict[word]] = 1\n",
    "\n",
    "print(corpus_matrix)\n",
    "\n",
    "# Create a neural network model with an embedding layer\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_dict), 10, input_length=len(word_dict)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(corpus_matrix, np.array([1, 1, 0, 0]), epochs=100, verbose=0)\n",
    "\n",
    "# Get the learned word embeddings\n",
    "embeddings = model.get_weights()[0]\n",
    "\n",
    "# Print the learned embeddings for each word\n",
    "for word, index in word_dict.items():\n",
    "    print(word, embeddings[index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('fake_news_dataset.csv')\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the text data to sequences of word indices\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "max_len = 1000\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Create the word embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word_embeddings.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Create the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index)+1, embedding_dim, input_length=max_len, weights=[embedding_matrix], trainable=False))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train, batch_size=32, epochs=10, validation_data=(X_test_pad, y_test))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "score = model.evaluate(X_test_pad, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
