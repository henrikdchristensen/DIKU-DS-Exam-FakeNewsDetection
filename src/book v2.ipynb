{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "\n",
    "import importlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 955534.42it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 891987.58it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 882902.63it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 624993.15it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 817216.81it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 856141.15it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 867721.21it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 829481.85it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 839216.63it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 909037.59it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 803656.62it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 862199.68it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 833539.48it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 803916.17it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 876998.85it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 881661.03it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 877914.84it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 831296.83it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 917729.56it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 909083.89it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 833146.25it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 827329.48it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 863326.61it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 873995.42it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 806330.21it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 837342.70it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 834215.89it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 795974.49it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 909478.14it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 919411.35it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 849228.18it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 848863.81it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 913320.14it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 908998.19it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 850387.81it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 910785.53it/s]\n",
      "100%|██████████| 73069/73069 [00:00<00:00, 775856.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 807.0935735702515\n"
     ]
    }
   ],
   "source": [
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/dataset.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 925826.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 200000\n",
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 672157.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 200000\n",
      "running\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 879091.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 200000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset_bin.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                   out_file=\"../datasets/big/dataset_unbalanced.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, True, False], \n",
    "                                   out_file=\"../datasets/big/dataset_balanced_types.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, True, False],\n",
    "                                   out_file=\"../datasets/big/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/big/dataset_unbalanced.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.121625, conspiracy: 0.123125, junksci: 0.0155, hate: 0.01, unreliable: 0.04575, bias: 0.15475, satire: 0.016125, reliable: 0.259125, clickbait: 0.032, political: 0.222\n",
      "True: 4105, Fake: 3895\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.129, conspiracy: 0.124, junksci: 0.012, hate: 0.014, unreliable: 0.045, bias: 0.147, satire: 0.022, reliable: 0.262, clickbait: 0.021, political: 0.224\n",
      "True: 507, Fake: 493\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.106, conspiracy: 0.125, junksci: 0.017, hate: 0.008, unreliable: 0.05, bias: 0.172, satire: 0.014, reliable: 0.255, clickbait: 0.032, political: 0.221\n",
      "True: 508, Fake: 492\n",
      "File: ../datasets/big/dataset_balanced_types.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 2400, Fake: 5600\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 300, Fake: 700\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.12, conspiracy: 0.142, junksci: 0.015, hate: 0.013, unreliable: 0.04, bias: 0.144, satire: 0.02, reliable: 0.248, clickbait: 0.034, political: 0.224\n",
      "True: 506, Fake: 494\n",
      "File: ../datasets/big/dataset_balanced_bin.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.124625, conspiracy: 0.127, junksci: 0.01575, hate: 0.01025, unreliable: 0.047125, bias: 0.158625, satire: 0.016625, reliable: 0.252625, clickbait: 0.031375, political: 0.216\n",
      "True: 4000, Fake: 4000\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.125, conspiracy: 0.121, junksci: 0.013, hate: 0.017, unreliable: 0.048, bias: 0.154, satire: 0.022, reliable: 0.256, clickbait: 0.026, political: 0.218\n",
      "True: 500, Fake: 500\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.121, conspiracy: 0.115, junksci: 0.022, hate: 0.008, unreliable: 0.044, bias: 0.162, satire: 0.012, reliable: 0.259, clickbait: 0.029, political: 0.228\n",
      "True: 516, Fake: 484\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "for file in [\"../datasets/big/dataset_unbalanced.csv\", \"../datasets/big/dataset_balanced_types.csv\", \"../datasets/big/dataset_balanced_bin.csv\"]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "        nrows=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "Clean_data(\"../datasets/big/dataset_unbalanced.csv\", \"../datasets/big/dataset_unbalanced_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 494.45it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 22968.95it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1209.29it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 159.28it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 92568.44it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 153820.63it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 16752.46it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 263645.13it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 54978.78it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6692.81it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 526386.97it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 291151.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 46858.34it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 44990.61it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 34735.38it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 27057.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 98.6530590057373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:20<00:00, 493.56it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 22404.27it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1207.30it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 160.39it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 83516.44it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 153761.42it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 16998.18it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 182701.03it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 54056.79it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 7029.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 490298.09it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 269567.21it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 46010.25it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 45974.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 44545.52it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 27625.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 98.27062797546387\n"
     ]
    }
   ],
   "source": [
    "Clean_data(\"../datasets/big/dataset_balanced_types.csv\", \"../datasets/big/dataset_balanced_types_cleaned.csv\")\n",
    "Clean_data(\"../datasets/big/dataset_balanced_bin.csv\", \"../datasets/big/dataset_balanced_bin_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_content(data, col=\"content\", new_col=\"count_vectorized\"):\n",
    "    # Prepare the tf-idf (term frequency-inverse document frequency) TODO: read up on this for report\n",
    "    start_time = time() \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    tf_idf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "    # fit and transform train data to count vectorizer\n",
    "    count_vectorizer.fit(data[col].values)\n",
    "    count_vect_train = count_vectorizer.transform(data[col].values)\n",
    "    # fit the counts vector to tfidf transformer\n",
    "    tf_idf_transformer.fit(count_vect_train)\n",
    "    count_vect_train = tf_idf_transformer.transform(count_vect_train)\n",
    "    data[new_col] = [x for x in count_vect_train]\n",
    "    #cleaned_data_combined['count_vectorized'] = cleaned_data_combined['count_vectorized'].apply(lambda x: tf_idf_transformer.transform([x]))\n",
    "\n",
    "    end_time = time()\n",
    "    print(f\"Time elapsed of TF IDF transform for {col}:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csr_data(data, features=\"content\", y=\"type\", set=\"set\", get_val=True):\n",
    "    train = data[data[set] == 0]\n",
    "    val = data[data[set] == 1]\n",
    "    test = data[data[set] == 2]\n",
    "    X_train, y_train = vstack(train[features]), train[y].astype(int)\n",
    "    X_val, y_val = vstack(val[features]), val[y].astype(int)\n",
    "    X_test, y_test = vstack(test[features]), test[y].astype(int)\n",
    "    if not get_val:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.DataFrame()\n",
    "def try_models(models, X_train, X_test, y_train, y_test, name=None):\n",
    "    global backup\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        start_time = time() \n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time() - start_time\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
    "            \"test_acc\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred), \n",
    "            \"time\": \"{:.2f}\".format(train_time)\n",
    "        })\n",
    "        backup = pd.DataFrame(metrics)\n",
    "        print(f\"{name} finished in {(time() - start_time):.2f} seconds\")\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_baseline():\n",
    "    def __init__(self):\n",
    "        self.metrics = pd.DataFrame()\n",
    "\n",
    "    def test_baseline(self, X_train, X_test, y_train, y_test, name=None, model=None):\n",
    "        if model == None:\n",
    "            model = LogisticRegression()\n",
    "        metric = try_models([model], X_train, X_test, y_train, y_test, name=name)\n",
    "        self.metrics = pd.concat([self.metrics, metric])\n",
    "\n",
    "    def test_cols(self, data, cols_to_test):\n",
    "        for col, name in cols_to_test:\n",
    "            self.test_baseline(*split_csr_data(data, features=col, y=\"type_binary\", get_val=False), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = pd.read_csv(\"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "for col in [\"content_combined\", \"content_authors\", \"content_title\", \"content_domain\", \"content_domain_authors_title\"]:\n",
    "    vectorize_content(vectorized_data, col=col, new_col=f\"{col}_vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = Test_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_test = [\n",
    "    (\"content_combined_vectorized\", \"content_combined_unbalanced\"),\n",
    "    (\"content_authors_vectorized\", \"content_authors_unbalanced\"), \n",
    "    (\"content_title_vectorized\", \"content_title_unbalanced\"),\n",
    "    (\"content_domain_vectorized\", \"content_domain_unbalanced\"),\n",
    "    (\"content_domain_authors_title_vectorized\", \"content_domain_authors_title_unbalanced\")]\n",
    "\n",
    "tests.test_cols(vectorized_data, cols_to_test)\n",
    "tests.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = pd.read_csv(\"../datasets/big/dataset_balanced_types_cleaned.csv\")\n",
    "for col in [\"content_combined\", \"content_authors\", \"content_title\", \"content_domain\", \"content_domain_authors_title\"]:\n",
    "    vectorize_content(vectorized_data, col=col, new_col=f\"{col}_vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_test = [\n",
    "    (\"content_combined_vectorized\", \"content_combined_balanced_types\"),\n",
    "    (\"content_authors_vectorized\", \"content_authors_balanced_types\"), \n",
    "    (\"content_title_vectorized\", \"content_title_balanced_types\"),\n",
    "    (\"content_domain_vectorized\", \"content_domain_balanced_types\"),\n",
    "    (\"content_domain_authors_title_vectorized\", \"content_domain_authors_title_balanced_types\")]\n",
    "\n",
    "tests.test_cols(vectorized_data, cols_to_test)\n",
    "tests.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed of TF IDF transform for content_title: , 20.984931230545044\n",
      "Time elapsed of TF IDF transform for content_domain: , 18.75245451927185\n",
      "Time elapsed of TF IDF transform for content_domain_authors_title: , 18.719897985458374\n"
     ]
    }
   ],
   "source": [
    "vectorized_data = pd.read_csv(\"../datasets/big/dataset_balanced_bin_cleaned.csv\")\n",
    "for col in [\"content_combined\", \"content_authors\", \"content_title\", \"content_domain\", \"content_domain_authors_title\"]:\n",
    "    vectorize_content(vectorized_data, col=col, new_col=f\"{col}_vectorized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_combined_balanced_bin finished in 13.79 seconds\n",
      "content_authors_balanced_bin finished in 13.79 seconds\n",
      "content_title_balanced_bin finished in 8.47 seconds\n",
      "content_domain_balanced_bin finished in 15.09 seconds\n",
      "content_domain_authors_title_balanced_bin finished in 11.56 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_unbalanced</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>0.884</td>\n",
       "      <td>0.898374</td>\n",
       "      <td>0.870079</td>\n",
       "      <td>0.884000</td>\n",
       "      <td>14.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_bin</td>\n",
       "      <td>0.978500</td>\n",
       "      <td>0.877</td>\n",
       "      <td>0.896970</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.878338</td>\n",
       "      <td>11.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_unbalanced</td>\n",
       "      <td>0.975875</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.874751</td>\n",
       "      <td>0.866142</td>\n",
       "      <td>0.870425</td>\n",
       "      <td>16.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_bin</td>\n",
       "      <td>0.976250</td>\n",
       "      <td>0.866</td>\n",
       "      <td>0.877470</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.868885</td>\n",
       "      <td>15.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_unbalanced</td>\n",
       "      <td>0.974625</td>\n",
       "      <td>0.869</td>\n",
       "      <td>0.887064</td>\n",
       "      <td>0.850394</td>\n",
       "      <td>0.868342</td>\n",
       "      <td>16.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_bin</td>\n",
       "      <td>0.973625</td>\n",
       "      <td>0.861</td>\n",
       "      <td>0.888660</td>\n",
       "      <td>0.835271</td>\n",
       "      <td>0.861139</td>\n",
       "      <td>13.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_unbalanced</td>\n",
       "      <td>0.967500</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.846307</td>\n",
       "      <td>0.834646</td>\n",
       "      <td>0.840436</td>\n",
       "      <td>10.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_balanced_bin</td>\n",
       "      <td>0.969500</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.856855</td>\n",
       "      <td>0.823643</td>\n",
       "      <td>0.839921</td>\n",
       "      <td>13.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_bin</td>\n",
       "      <td>0.968875</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.859756</td>\n",
       "      <td>0.819767</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>8.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_unbalanced</td>\n",
       "      <td>0.968250</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.842315</td>\n",
       "      <td>0.830709</td>\n",
       "      <td>0.836472</td>\n",
       "      <td>15.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_types</td>\n",
       "      <td>0.916250</td>\n",
       "      <td>0.746</td>\n",
       "      <td>0.980916</td>\n",
       "      <td>0.507905</td>\n",
       "      <td>0.669271</td>\n",
       "      <td>14.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_types</td>\n",
       "      <td>0.894000</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.979675</td>\n",
       "      <td>0.476285</td>\n",
       "      <td>0.640957</td>\n",
       "      <td>12.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_types</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.982143</td>\n",
       "      <td>0.434783</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_types</td>\n",
       "      <td>0.878375</td>\n",
       "      <td>0.698</td>\n",
       "      <td>0.972222</td>\n",
       "      <td>0.415020</td>\n",
       "      <td>0.581717</td>\n",
       "      <td>17.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_balanced_types</td>\n",
       "      <td>0.877250</td>\n",
       "      <td>0.694</td>\n",
       "      <td>0.971698</td>\n",
       "      <td>0.407115</td>\n",
       "      <td>0.573816</td>\n",
       "      <td>13.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name  train_acc  test_acc  \\\n",
       "0      content_domain_authors_title_unbalanced   0.978500     0.884   \n",
       "0    content_domain_authors_title_balanced_bin   0.978500     0.877   \n",
       "0                    content_domain_unbalanced   0.975875     0.869   \n",
       "0                  content_domain_balanced_bin   0.976250     0.866   \n",
       "0                   content_authors_unbalanced   0.974625     0.869   \n",
       "0                 content_authors_balanced_bin   0.973625     0.861   \n",
       "0                     content_title_unbalanced   0.967500     0.839   \n",
       "0                content_combined_balanced_bin   0.969500     0.838   \n",
       "0                   content_title_balanced_bin   0.968875     0.838   \n",
       "0                  content_combined_unbalanced   0.968250     0.835   \n",
       "0  content_domain_authors_title_balanced_types   0.916250     0.746   \n",
       "0               content_authors_balanced_types   0.894000     0.730   \n",
       "0                content_domain_balanced_types   0.905000     0.710   \n",
       "0                 content_title_balanced_types   0.878375     0.698   \n",
       "0              content_combined_balanced_types   0.877250     0.694   \n",
       "\n",
       "   precision    recall        f1   time  \n",
       "0   0.898374  0.870079  0.884000  14.94  \n",
       "0   0.896970  0.860465  0.878338  11.52  \n",
       "0   0.874751  0.866142  0.870425  16.33  \n",
       "0   0.877470  0.860465  0.868885  15.04  \n",
       "0   0.887064  0.850394  0.868342  16.88  \n",
       "0   0.888660  0.835271  0.861139  13.74  \n",
       "0   0.846307  0.834646  0.840436  10.92  \n",
       "0   0.856855  0.823643  0.839921  13.75  \n",
       "0   0.859756  0.819767  0.839286   8.43  \n",
       "0   0.842315  0.830709  0.836472  15.13  \n",
       "0   0.980916  0.507905  0.669271  14.86  \n",
       "0   0.979675  0.476285  0.640957  12.68  \n",
       "0   0.982143  0.434783  0.602740  13.83  \n",
       "0   0.972222  0.415020  0.581717  17.93  \n",
       "0   0.971698  0.407115  0.573816  13.83  "
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_test = [\n",
    "    (\"content_combined_vectorized\", \"content_combined_balanced_bin\"),\n",
    "    (\"content_authors_vectorized\", \"content_authors_balanced_bin\"), \n",
    "    (\"content_title_vectorized\", \"content_title_balanced_bin\"),\n",
    "    (\"content_domain_vectorized\", \"content_domain_balanced_bin\"),\n",
    "    (\"content_domain_authors_title_vectorized\", \"content_domain_authors_title_balanced_bin\")]\n",
    "\n",
    "tests.test_cols(vectorized_data, cols_to_test)\n",
    "tests.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
