{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "\n",
    "import importlib\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../datasets/big/cleaned_input_cols.csv\", nrows=BATCH_SIZE, dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 598946.71it/s]\n",
      "100%|██████████| 10000/10000 [00:23<00:00, 433.40it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 23047.08it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1136.04it/s]\n",
      "100%|██████████| 10000/10000 [01:05<00:00, 152.99it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 88913.05it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 156229.31it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 17453.70it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 244076.26it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 54293.23it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6477.39it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 175660.73it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 156271.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish time: 102.85886311531067\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            (pp.Clean_domain(), 'domain')\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "        nrows=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "cleaned_data = Clean_data(\"../datasets/big/dataset.csv\", \"../datasets/big/cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "def vectorize_content(data, col=\"content\", new_col=\"count_vectorized\"):\n",
    "    # Prepare the tf-idf (term frequency-inverse document frequency) TODO: read up on this for report\n",
    "    start_time = time() \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    tf_idf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "    # fit and transform train data to count vectorizer\n",
    "    count_vectorizer.fit(data[col].values)\n",
    "    count_vect_train = count_vectorizer.transform(data[col].values)\n",
    "    # fit the counts vector to tfidf transformer\n",
    "    tf_idf_transformer.fit(count_vect_train)\n",
    "    count_vect_train = tf_idf_transformer.transform(count_vect_train)\n",
    "    data[new_col] = [x for x in count_vect_train]\n",
    "    #cleaned_data_combined['count_vectorized'] = cleaned_data_combined['count_vectorized'].apply(lambda x: tf_idf_transformer.transform([x]))\n",
    "\n",
    "    end_time = time()\n",
    "    print(\"Time elapsed for TF IDF transform: ,\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 28094.52it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 28761.42it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 27845.35it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 14129.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed for TF IDF transform: , 20.43290376663208\n",
      "Time elapsed for TF IDF transform: , 18.50907826423645\n",
      "Time elapsed for TF IDF transform: , 19.508578538894653\n",
      "Time elapsed for TF IDF transform: , 18.717485904693604\n",
      "Time elapsed for TF IDF transform: , 19.075685739517212\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "def Create_input_cols(file, new_file):\n",
    "    cleaned_data_combined = pp.apply_pipeline_pd_tqdm(pd.read_csv(file, dtype=str), [\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ])\n",
    "    vectorize_content(cleaned_data_combined, col=\"content_combined\", new_col=\"content_combined_vectorized\")\n",
    "    vectorize_content(cleaned_data_combined, col=\"content_authors\", new_col=\"content_authors_vectorized\")\n",
    "    vectorize_content(cleaned_data_combined, col=\"content_title\", new_col=\"content_title_vectorized\")\n",
    "    vectorize_content(cleaned_data_combined, col=\"content_domain\", new_col=\"content_domain_vectorized\")\n",
    "    vectorize_content(cleaned_data_combined, col=\"content_domain_authors_title\", new_col=\"content_domain_authors_title_vectorized\")\n",
    "    cleaned_data_combined.to_csv(new_file, index=False)\n",
    "\n",
    "cleaned_data = Create_input_cols(\"../datasets/big/cleaned.csv\", \"../datasets/big/cleaned_input_cols.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csr_data(data, col=\"content\", get_val=True):\n",
    "    train = data[data[\"set\"] == 0]\n",
    "    val = data[data[\"set\"] == 1]\n",
    "    test = data[data[\"set\"] == 2]\n",
    "    X_train, y_train = vstack(train[col]), train[\"type\"].astype(int)\n",
    "    X_val, y_val = vstack(val[col]), val[\"type\"].astype(int)\n",
    "    X_test, y_test = vstack(test[col]), test[\"type\"].astype(int)\n",
    "    if not get_val:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "backup = pd.DataFrame()\n",
    "def try_models(models, X_train, X_test, y_train, y_test, name=None):\n",
    "    global backup\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        start_time = time() \n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time() - start_time\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
    "            \"test_acc\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred), \n",
    "            \"time\": \"{:.2f}\".format(train_time)\n",
    "        })\n",
    "        backup = pd.DataFrame(metrics)\n",
    "        print(f\"{name} finished in {(time() - start_time):.2f} seconds\")\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_baseline():\n",
    "    def __init__(self):\n",
    "        self.metrics = pd.DataFrame()\n",
    "\n",
    "    def test_baseline(self, X_train, X_test, y_train, y_test, name=None, model=None):\n",
    "        if model == None:\n",
    "            model = LogisticRegression()\n",
    "        metric = try_models([model], X_train, X_test, y_train, y_test, name=name)\n",
    "        self.metrics = pd.concat([self.metrics, metric])\n",
    "        \n",
    "tests = Test_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests.test_baseline(*split_csr_data(cleaned_data_combined, col=\"count_vectorized\", get_val=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
