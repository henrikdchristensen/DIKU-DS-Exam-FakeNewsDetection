{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "\n",
    "import importlib\n",
    "import math"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/dataset.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:01<00:00, 152296.33it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 176616.40it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 164883.53it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 185039.16it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 186903.95it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 187540.29it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 190906.82it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 191636.46it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 193830.14it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 193353.57it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 566782.14it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 558838.17it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 581432.65it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 555758.89it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 577280.55it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 498551.82it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 566136.45it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 586492.49it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 575857.08it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 573166.96it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 532029.36it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 555639.99it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 550953.82it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 574494.51it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 559509.84it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 569519.60it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 578898.38it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 548579.08it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 558352.38it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 550073.02it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 484773.51it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 548506.27it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 557201.18it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 637836.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 6800000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 498589.16it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 488578.70it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 516741.00it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 483314.14it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 587605.42it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 756896.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 1200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 668309.54it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 622432.36it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 670391.97it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 642936.75it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 674643.20it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 659918.07it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 553486.85it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 637649.29it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 691680.24it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 705313.26it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 700668.04it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 631871.65it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 674796.24it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 680684.11it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 709069.22it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 680064.95it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 672913.58it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 635058.95it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 751438.45it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 809465.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 4000000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset_bin.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                   out_file=\"../datasets/big/dataset_unbalanced.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.6,0.1,0.1], [True, True, False], \n",
    "                                   out_file=\"../datasets/big/dataset_balanced_types.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, True, False],\n",
    "                                   out_file=\"../datasets/big/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, True, False], \n",
    "                                   out_file=\"../datasets/big/dataset_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/big/dataset_unbalanced.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.121625, conspiracy: 0.123125, junksci: 0.0155, hate: 0.01, unreliable: 0.04575, bias: 0.15475, satire: 0.016125, reliable: 0.259125, clickbait: 0.032, political: 0.222\n",
      "True: 4105, Fake: 3895\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.129, conspiracy: 0.124, junksci: 0.012, hate: 0.014, unreliable: 0.045, bias: 0.147, satire: 0.022, reliable: 0.262, clickbait: 0.021, political: 0.224\n",
      "True: 507, Fake: 493\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.106, conspiracy: 0.125, junksci: 0.017, hate: 0.008, unreliable: 0.05, bias: 0.172, satire: 0.014, reliable: 0.255, clickbait: 0.032, political: 0.221\n",
      "True: 508, Fake: 492\n",
      "File: ../datasets/big/dataset_balanced_types.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 2400, Fake: 5600\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 300, Fake: 700\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.12, conspiracy: 0.142, junksci: 0.015, hate: 0.013, unreliable: 0.04, bias: 0.144, satire: 0.02, reliable: 0.248, clickbait: 0.034, political: 0.224\n",
      "True: 506, Fake: 494\n",
      "File: ../datasets/big/dataset_balanced_bin.csv ----------------------------------\n",
      "Distribution of train with size 8000:\n",
      "fake: 0.124625, conspiracy: 0.127, junksci: 0.01575, hate: 0.01025, unreliable: 0.047125, bias: 0.158625, satire: 0.016625, reliable: 0.252625, clickbait: 0.031375, political: 0.216\n",
      "True: 4000, Fake: 4000\n",
      "Distribution of val with size 1000:\n",
      "fake: 0.125, conspiracy: 0.121, junksci: 0.013, hate: 0.017, unreliable: 0.048, bias: 0.154, satire: 0.022, reliable: 0.256, clickbait: 0.026, political: 0.218\n",
      "True: 500, Fake: 500\n",
      "Distribution of test with size 1000:\n",
      "fake: 0.121, conspiracy: 0.115, junksci: 0.022, hate: 0.008, unreliable: 0.044, bias: 0.162, satire: 0.012, reliable: 0.259, clickbait: 0.029, political: 0.228\n",
      "True: 516, Fake: 484\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "for file in [\"../datasets/big/dataset_unbalanced.csv\", \"../datasets/big/dataset_balanced_types.csv\", \"../datasets/big/dataset_balanced_bin.csv\"]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [06:07<00:00, 543.87it/s]\n",
      "100%|██████████| 200000/200000 [00:08<00:00, 22498.70it/s]\n",
      "100%|██████████| 200000/200000 [02:44<00:00, 1218.72it/s]\n",
      "100%|██████████| 200000/200000 [2:37:40<00:00, 21.14it/s]     \n",
      "100%|██████████| 200000/200000 [00:05<00:00, 37714.99it/s]\n",
      "100%|██████████| 200000/200000 [00:01<00:00, 174595.96it/s]\n",
      "100%|██████████| 200000/200000 [00:10<00:00, 18988.77it/s]\n",
      "100%|██████████| 200000/200000 [00:04<00:00, 40743.86it/s] \n",
      "100%|██████████| 200000/200000 [00:03<00:00, 60886.85it/s]\n",
      "100%|██████████| 200000/200000 [00:29<00:00, 6803.84it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 673690.16it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 331466.09it/s]\n",
      "100%|██████████| 200000/200000 [00:04<00:00, 44450.21it/s]\n",
      "100%|██████████| 200000/200000 [00:04<00:00, 43561.39it/s]\n",
      "100%|██████████| 200000/200000 [00:05<00:00, 39662.88it/s]\n",
      "100%|██████████| 200000/200000 [00:07<00:00, 25032.76it/s]\n",
      " 65%|██████▍   | 129952/200000 [04:10<02:09, 539.74it/s]"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/big/dataset_unbalanced.csv\", \"../datasets/big/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/big/dataset_balanced_types.csv\", \"../datasets/big/dataset_balanced_types_cleaned.csv\")\n",
    "Clean_data(\"../datasets/big/dataset_balanced_bin.csv\", \"../datasets/big/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/big/dataset_reliable_fake.csv\", \"../datasets/big/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_content(data, col=\"content\", new_col=\"count_vectorized\"):\n",
    "    # Prepare the tf-idf (term frequency-inverse document frequency) TODO: read up on this for report\n",
    "    start_time = time() \n",
    "    count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "    tf_idf_transformer = TfidfTransformer(smooth_idf=False)\n",
    "\n",
    "    # fit and transform train data to count vectorizer\n",
    "    count_vectorizer.fit(data[col].values)\n",
    "    count_vect_train = count_vectorizer.transform(data[col].values)\n",
    "    # fit the counts vector to tfidf transformer\n",
    "    tf_idf_transformer.fit(count_vect_train)\n",
    "    count_vect_train = tf_idf_transformer.transform(count_vect_train)\n",
    "    data[new_col] = [x for x in count_vect_train]\n",
    "    #cleaned_data_combined['count_vectorized'] = cleaned_data_combined['count_vectorized'].apply(lambda x: tf_idf_transformer.transform([x]))\n",
    "\n",
    "    end_time = time()\n",
    "    print(f\"Time elapsed of TF IDF transform for {col}:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_csr_data(data, features=\"content\", y=\"type\", set=\"set\", get_val=True):\n",
    "    train = data[data[set] == 0]\n",
    "    val = data[data[set] == 1]\n",
    "    test = data[data[set] == 2]\n",
    "    X_train, y_train = vstack(train[features]), train[y].astype(int)\n",
    "    X_val, y_val = vstack(val[features]), val[y].astype(int)\n",
    "    X_test, y_test = vstack(test[features]), test[y].astype(int)\n",
    "    if not get_val:\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_models(models, X_train, X_test, y_train, y_test, name=None):\n",
    "    metrics = []\n",
    "    for model in models:\n",
    "        start_time = time() \n",
    "        model.fit(X_train, y_train)\n",
    "        train_time = time() - start_time\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if name == None:\n",
    "            name = type(model).__name__\n",
    "        metrics.append({\n",
    "            \"name\": name,\n",
    "            \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
    "            \"test_acc\": accuracy_score(y_test, y_pred),\n",
    "            \"precision\": precision_score(y_test, y_pred),\n",
    "            \"recall\": recall_score(y_test, y_pred),\n",
    "            \"f1\": f1_score(y_test, y_pred), \n",
    "            \"time\": \"{:.2f}\".format(train_time)\n",
    "        })\n",
    "        print(f\"{name} finished in {(time() - start_time):.2f} seconds\")\n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test_baseline():\n",
    "    def __init__(self):\n",
    "        self.metrics = pd.DataFrame()\n",
    "\n",
    "    def test_baseline(self, X_train, X_test, y_train, y_test, name=None, model=None):\n",
    "        if model == None:\n",
    "            model = LogisticRegression()\n",
    "        metric = try_models([model], X_train, X_test, y_train, y_test, name=name)\n",
    "        self.metrics = pd.concat([self.metrics, metric])\n",
    "\n",
    "    def test_col(self, data, col, name, model=None):\n",
    "        self.test_baseline(*split_csr_data(data, features=col, y=\"type_binary\", get_val=False), name=name, model=model)\n",
    "\n",
    "    def test_cols(self, data, cols_to_test, model=None):\n",
    "        for col, name in cols_to_test:\n",
    "            self.test_baseline(*split_csr_data(data, features=col, y=\"type_binary\", get_val=False), name=name, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = Test_baseline()\n",
    "# contains the files to test and the name of the test-group (for the dataframe)\n",
    "files = [\n",
    "    (\"../datasets/big/dataset_unbalanced_cleaned.csv\", \"unbalanced\"), \n",
    "    (\"../datasets/big/dataset_balanced_types_cleaned.csv\", \"balanced_types\"), \n",
    "    (\"../datasets/big/dataset_balanced_bin_cleaned.csv\", \"balanced_bin\"),\n",
    "    (\"../datasets/big/dataset_reliable_fake_cleaned.csv\", \"reliable_fake\")\n",
    "]\n",
    "# contains the columns to test and the name of the specific test (for the dataframe)\n",
    "cols_to_test = [\n",
    "    (\"content_combined\", \"content\"),\n",
    "    (\"content_authors\", \"content_authors\"), \n",
    "    (\"content_title\", \"content_title\"),\n",
    "    (\"content_domain\", \"content_domain\"),\n",
    "    (\"content_domain_authors_title\", \"content_domain_authors_title\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proccessing: unbalanced...\n",
      "Time elapsed of TF IDF transform for content_combined: 9.632106304168701\n",
      "content_unbalanced finished in 5.60 seconds\n",
      "Time elapsed of TF IDF transform for content_authors: 7.3649742603302\n",
      "content_authors_unbalanced finished in 4.12 seconds\n",
      "Time elapsed of TF IDF transform for content_title: 7.952449083328247\n",
      "content_title_unbalanced finished in 5.75 seconds\n",
      "Time elapsed of TF IDF transform for content_domain: 8.076876640319824\n",
      "content_domain_unbalanced finished in 4.44 seconds\n",
      "Time elapsed of TF IDF transform for content_domain_authors_title: 7.574965000152588\n",
      "content_domain_authors_title_unbalanced finished in 5.15 seconds\n",
      "Proccessing: balanced_types...\n",
      "Time elapsed of TF IDF transform for content_combined: 7.003679037094116\n",
      "content_balanced_types finished in 3.78 seconds\n",
      "Time elapsed of TF IDF transform for content_authors: 6.9939961433410645\n",
      "content_authors_balanced_types finished in 3.42 seconds\n",
      "Time elapsed of TF IDF transform for content_title: 7.603183031082153\n",
      "content_title_balanced_types finished in 4.43 seconds\n",
      "Time elapsed of TF IDF transform for content_domain: 9.105665922164917\n",
      "content_domain_balanced_types finished in 4.30 seconds\n",
      "Time elapsed of TF IDF transform for content_domain_authors_title: 7.848972320556641\n",
      "content_domain_authors_title_balanced_types finished in 3.91 seconds\n",
      "Proccessing: balanced_bin...\n",
      "Time elapsed of TF IDF transform for content_combined: 7.680804252624512\n",
      "content_balanced_bin finished in 4.25 seconds\n",
      "Time elapsed of TF IDF transform for content_authors: 7.945714235305786\n",
      "content_authors_balanced_bin finished in 4.08 seconds\n",
      "Time elapsed of TF IDF transform for content_title: 7.8183088302612305\n",
      "content_title_balanced_bin finished in 3.78 seconds\n",
      "Time elapsed of TF IDF transform for content_domain: 7.76019549369812\n",
      "content_domain_balanced_bin finished in 4.30 seconds\n",
      "Time elapsed of TF IDF transform for content_domain_authors_title: 8.660028457641602\n",
      "content_domain_authors_title_balanced_bin finished in 4.36 seconds\n",
      "Proccessing: reliable_fake...\n",
      "Time elapsed of TF IDF transform for content_combined: 8.602804183959961\n",
      "content_reliable_fake finished in 3.23 seconds\n",
      "Time elapsed of TF IDF transform for content_authors: 7.54083251953125\n",
      "content_authors_reliable_fake finished in 3.98 seconds\n",
      "Time elapsed of TF IDF transform for content_title: 7.547281742095947\n",
      "content_title_reliable_fake finished in 4.13 seconds\n",
      "Time elapsed of TF IDF transform for content_domain: 7.5217366218566895\n",
      "content_domain_reliable_fake finished in 4.31 seconds\n",
      "Time elapsed of TF IDF transform for content_domain_authors_title: 7.789882659912109\n",
      "content_domain_authors_title_reliable_fake finished in 4.52 seconds\n"
     ]
    }
   ],
   "source": [
    "for file, name in files:\n",
    "    print(f\"Proccessing: {name}\")\n",
    "    cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "    vectorized_data = pd.read_csv(file, usecols=cols_to_read)\n",
    "    print(\"Read data into dataframe\")\n",
    "    for col, entry_name in cols_to_test:\n",
    "        #vectorize_content(vectorized_data, col=col, new_col=f\"{col}_vectorized\")\n",
    "        #tests.test_col(vectorized_data, f\"{col}_vectorized\", f\"{entry_name}_{name}\")\n",
    "        vectorize_content(vectorized_data, col=col, new_col=col)\n",
    "        tests.test_col(vectorized_data, col, f\"{entry_name}_{name}\")\n",
    "        del vectorized_data[col] # free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_bin</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.854127</td>\n",
       "      <td>0.842803</td>\n",
       "      <td>0.848427</td>\n",
       "      <td>4.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_unbalanced</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.838290</td>\n",
       "      <td>0.857414</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>4.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_bin</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.836466</td>\n",
       "      <td>0.842803</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>4.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_unbalanced</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.812834</td>\n",
       "      <td>0.866920</td>\n",
       "      <td>0.839006</td>\n",
       "      <td>4.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_bin</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.824663</td>\n",
       "      <td>4.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_unbalanced</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.820416</td>\n",
       "      <td>0.825095</td>\n",
       "      <td>0.822749</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_bin</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.820116</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.811483</td>\n",
       "      <td>3.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_unbalanced</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.823194</td>\n",
       "      <td>0.807083</td>\n",
       "      <td>5.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_balanced_bin</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>4.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_unbalanced</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.780180</td>\n",
       "      <td>0.823194</td>\n",
       "      <td>0.801110</td>\n",
       "      <td>5.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_types</td>\n",
       "      <td>0.8370</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>4.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_types</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.292220</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_balanced_types</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288425</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>3.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_types</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288425</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>3.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_types</td>\n",
       "      <td>0.8215</td>\n",
       "      <td>0.615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269450</td>\n",
       "      <td>0.424514</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name  train_acc  test_acc  \\\n",
       "0    content_domain_authors_title_balanced_bin     0.9960     0.841   \n",
       "0      content_domain_authors_title_unbalanced     0.9960     0.838   \n",
       "0                  content_domain_balanced_bin     0.9950     0.830   \n",
       "0                    content_domain_unbalanced     0.9950     0.825   \n",
       "0                 content_authors_balanced_bin     0.9955     0.818   \n",
       "0                   content_authors_unbalanced     0.9945     0.813   \n",
       "0                   content_title_balanced_bin     0.9940     0.803   \n",
       "0                     content_title_unbalanced     0.9940     0.793   \n",
       "0                         content_balanced_bin     0.9945     0.794   \n",
       "0                           content_unbalanced     0.9945     0.785   \n",
       "0                content_domain_balanced_types     0.8370     0.628   \n",
       "0               content_authors_balanced_types     0.8385     0.626   \n",
       "0                       content_balanced_types     0.8225     0.625   \n",
       "0  content_domain_authors_title_balanced_types     0.8495     0.625   \n",
       "0                 content_title_balanced_types     0.8215     0.615   \n",
       "\n",
       "   precision    recall        f1  time  \n",
       "0   0.854127  0.842803  0.848427  4.71  \n",
       "0   0.838290  0.857414  0.847744  4.64  \n",
       "0   0.836466  0.842803  0.839623  4.40  \n",
       "0   0.812834  0.866920  0.839006  4.57  \n",
       "0   0.839216  0.810606  0.824663  4.15  \n",
       "0   0.820416  0.825095  0.822749  4.03  \n",
       "0   0.820116  0.803030  0.811483  3.96  \n",
       "0   0.791590  0.823194  0.807083  5.17  \n",
       "0   0.808429  0.799242  0.803810  4.01  \n",
       "0   0.780180  0.823194  0.801110  5.35  \n",
       "0   1.000000  0.294118  0.454545  4.36  \n",
       "0   0.993548  0.292220  0.451613  3.93  \n",
       "0   1.000000  0.288425  0.447717  3.63  \n",
       "0   1.000000  0.288425  0.447717  3.22  \n",
       "0   1.000000  0.269450  0.424514  4.13  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best file and features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_file = \"../datasets/big/dataset_unbalanced_cleaned.csv\"\n",
    "best_col = \"content_domain_authors_title\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed of TF IDF transform for content_domain_authors_title: 9.814547061920166\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(best_file)\n",
    "vectorize_content(data, col=best_col, new_col=best_col)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "[CV] END ................................C=300, max_iter=100; total time=  10.0s\n",
      "[CV] END ................................C=300, max_iter=100; total time=  10.8s\n",
      "[CV] END ................................C=300, max_iter=100; total time=   7.1s\n",
      "[CV] END ................................C=300, max_iter=150; total time=   8.8s\n",
      "[CV] END ................................C=300, max_iter=150; total time=  11.2s\n",
      "[CV] END ................................C=300, max_iter=150; total time=   6.7s\n",
      "[CV] END ................................C=300, max_iter=200; total time=   8.6s\n",
      "[CV] END ................................C=300, max_iter=200; total time=  10.6s\n",
      "[CV] END ................................C=300, max_iter=200; total time=   7.2s\n",
      "hyper_1 finished in 88.89 seconds\n",
      "LogisticRegression(C=300)\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression()\n",
    "param_grid = {\"C\": [250, 300, 350], \"max_iter\": [150, 200, 250]} #200 won - det samme\n",
    "#param_grid = {'penalty': ['l1', 'l2'],'C': [350], \"maxiter\": [200], 'solver': ['liblinear', 'saga']}\n",
    "\n",
    "grid = GridSearchCV(estimator=model,\n",
    "                    param_grid=param_grid,\n",
    "                    cv=3,\n",
    "                    scoring=['f1'],\n",
    "                    refit='f1',\n",
    "                    verbose=2) #'accuracy'\n",
    "\n",
    "\n",
    "tests.test_col(data, best_col, \"hyper_1\", model=grid)\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyper_all finished in 8.66 seconds\n",
      "Time elapsed of TF IDF transform for content_combined: 7.822778701782227\n",
      "hyper_content finished in 9.59 seconds\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(best_file)\n",
    "vectorize_content(data, col=best_col, new_col=best_col)\n",
    "tests.test_col(data, best_col, \"hyper_all\", model=LogisticRegression(C=300, max_iter=200))\n",
    "# test the best parameters on the other featues\n",
    "vectorize_content(data, col=\"content_combined\", new_col=\"content_combined\")\n",
    "tests.test_col(data, \"content_combined\", \"hyper_content\", model=LogisticRegression(C=300, max_iter=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_reliable_fake</td>\n",
       "      <td>0.9965</td>\n",
       "      <td>0.934</td>\n",
       "      <td>0.979332</td>\n",
       "      <td>0.920777</td>\n",
       "      <td>0.949153</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_reliable_fake</td>\n",
       "      <td>0.9970</td>\n",
       "      <td>0.929</td>\n",
       "      <td>0.985390</td>\n",
       "      <td>0.907324</td>\n",
       "      <td>0.944747</td>\n",
       "      <td>4.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_reliable_fake</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.903</td>\n",
       "      <td>0.976667</td>\n",
       "      <td>0.875934</td>\n",
       "      <td>0.923562</td>\n",
       "      <td>3.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_reliable_fake</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.970149</td>\n",
       "      <td>0.874439</td>\n",
       "      <td>0.919811</td>\n",
       "      <td>3.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_reliable_fake</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.976391</td>\n",
       "      <td>0.865471</td>\n",
       "      <td>0.917591</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyper_all</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.881553</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.872238</td>\n",
       "      <td>8.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyper_all</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.867</td>\n",
       "      <td>0.881553</td>\n",
       "      <td>0.863118</td>\n",
       "      <td>0.872238</td>\n",
       "      <td>8.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_bin</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.841</td>\n",
       "      <td>0.854127</td>\n",
       "      <td>0.842803</td>\n",
       "      <td>0.848427</td>\n",
       "      <td>4.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_unbalanced</td>\n",
       "      <td>0.9960</td>\n",
       "      <td>0.838</td>\n",
       "      <td>0.838290</td>\n",
       "      <td>0.857414</td>\n",
       "      <td>0.847744</td>\n",
       "      <td>5.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_bin</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.836466</td>\n",
       "      <td>0.842803</td>\n",
       "      <td>0.839623</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_unbalanced</td>\n",
       "      <td>0.9950</td>\n",
       "      <td>0.825</td>\n",
       "      <td>0.812834</td>\n",
       "      <td>0.866920</td>\n",
       "      <td>0.839006</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_bin</td>\n",
       "      <td>0.9955</td>\n",
       "      <td>0.818</td>\n",
       "      <td>0.839216</td>\n",
       "      <td>0.810606</td>\n",
       "      <td>0.824663</td>\n",
       "      <td>4.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_unbalanced</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.820416</td>\n",
       "      <td>0.825095</td>\n",
       "      <td>0.822749</td>\n",
       "      <td>4.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_bin</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.820116</td>\n",
       "      <td>0.803030</td>\n",
       "      <td>0.811483</td>\n",
       "      <td>3.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_unbalanced</td>\n",
       "      <td>0.9940</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.791590</td>\n",
       "      <td>0.823194</td>\n",
       "      <td>0.807083</td>\n",
       "      <td>5.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_balanced_bin</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.808429</td>\n",
       "      <td>0.799242</td>\n",
       "      <td>0.803810</td>\n",
       "      <td>4.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_unbalanced</td>\n",
       "      <td>0.9945</td>\n",
       "      <td>0.785</td>\n",
       "      <td>0.780180</td>\n",
       "      <td>0.823194</td>\n",
       "      <td>0.801110</td>\n",
       "      <td>5.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hyper_content</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>0.793</td>\n",
       "      <td>0.834382</td>\n",
       "      <td>0.756654</td>\n",
       "      <td>0.793619</td>\n",
       "      <td>9.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_balanced_types</td>\n",
       "      <td>0.8370</td>\n",
       "      <td>0.628</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.294118</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>4.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_balanced_types</td>\n",
       "      <td>0.8385</td>\n",
       "      <td>0.626</td>\n",
       "      <td>0.993548</td>\n",
       "      <td>0.292220</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>3.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_balanced_types</td>\n",
       "      <td>0.8495</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288425</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>3.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_balanced_types</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>0.625</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.288425</td>\n",
       "      <td>0.447717</td>\n",
       "      <td>3.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_balanced_types</td>\n",
       "      <td>0.8215</td>\n",
       "      <td>0.615</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.269450</td>\n",
       "      <td>0.424514</td>\n",
       "      <td>4.41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          name  train_acc  test_acc  \\\n",
       "0                 content_domain_reliable_fake     0.9965     0.934   \n",
       "0   content_domain_authors_title_reliable_fake     0.9970     0.929   \n",
       "0                content_authors_reliable_fake     0.9955     0.903   \n",
       "0                        content_reliable_fake     0.9955     0.898   \n",
       "0                  content_title_reliable_fake     0.9955     0.896   \n",
       "0                                    hyper_all     1.0000     0.867   \n",
       "0                                    hyper_all     1.0000     0.867   \n",
       "0    content_domain_authors_title_balanced_bin     0.9960     0.841   \n",
       "0      content_domain_authors_title_unbalanced     0.9960     0.838   \n",
       "0                  content_domain_balanced_bin     0.9950     0.830   \n",
       "0                    content_domain_unbalanced     0.9950     0.825   \n",
       "0                 content_authors_balanced_bin     0.9955     0.818   \n",
       "0                   content_authors_unbalanced     0.9945     0.813   \n",
       "0                   content_title_balanced_bin     0.9940     0.803   \n",
       "0                     content_title_unbalanced     0.9940     0.793   \n",
       "0                         content_balanced_bin     0.9945     0.794   \n",
       "0                           content_unbalanced     0.9945     0.785   \n",
       "0                                hyper_content     1.0000     0.793   \n",
       "0                content_domain_balanced_types     0.8370     0.628   \n",
       "0               content_authors_balanced_types     0.8385     0.626   \n",
       "0  content_domain_authors_title_balanced_types     0.8495     0.625   \n",
       "0                       content_balanced_types     0.8225     0.625   \n",
       "0                 content_title_balanced_types     0.8215     0.615   \n",
       "\n",
       "   precision    recall        f1  time  \n",
       "0   0.979332  0.920777  0.949153  4.29  \n",
       "0   0.985390  0.907324  0.944747  4.51  \n",
       "0   0.976667  0.875934  0.923562  3.97  \n",
       "0   0.970149  0.874439  0.919811  3.21  \n",
       "0   0.976391  0.865471  0.917591  4.10  \n",
       "0   0.881553  0.863118  0.872238  8.65  \n",
       "0   0.881553  0.863118  0.872238  8.21  \n",
       "0   0.854127  0.842803  0.848427  4.34  \n",
       "0   0.838290  0.857414  0.847744  5.14  \n",
       "0   0.836466  0.842803  0.839623  4.29  \n",
       "0   0.812834  0.866920  0.839006  4.42  \n",
       "0   0.839216  0.810606  0.824663  4.06  \n",
       "0   0.820416  0.825095  0.822749  4.10  \n",
       "0   0.820116  0.803030  0.811483  3.76  \n",
       "0   0.791590  0.823194  0.807083  5.74  \n",
       "0   0.808429  0.799242  0.803810  4.24  \n",
       "0   0.780180  0.823194  0.801110  5.57  \n",
       "0   0.834382  0.756654  0.793619  9.57  \n",
       "0   1.000000  0.294118  0.454545  4.29  \n",
       "0   0.993548  0.292220  0.451613  3.39  \n",
       "0   1.000000  0.288425  0.447717  3.89  \n",
       "0   1.000000  0.288425  0.447717  3.77  \n",
       "0   1.000000  0.269450  0.424514  4.41  "
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
