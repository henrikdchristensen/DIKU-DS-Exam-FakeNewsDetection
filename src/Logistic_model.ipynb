{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'type'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m importlib\u001b[39m.\u001b[39mreload(pp)\n\u001b[1;32m----> 3\u001b[0m pp\u001b[39m.\u001b[39;49mapply_pipeline(\n\u001b[0;32m      4\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39m../datasets/big/combined_cleaned.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      5\u001b[0m     [(pp\u001b[39m.\u001b[39;49mBinary_labels_LIAR(), \u001b[39m'\u001b[39;49m\u001b[39mtype\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mtype_binary\u001b[39;49m\u001b[39m'\u001b[39;49m)], \n\u001b[0;32m      6\u001b[0m     new_file\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../datasets/big/combined_cleaned_bin.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, \n\u001b[0;32m      7\u001b[0m     progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\pipeline.py:590\u001b[0m, in \u001b[0;36mapply_pipeline\u001b[1;34m(old_file, function_cols, new_file, batch_size, get_batch, progress_bar, nrows)\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[39mreturn\u001b[39;00m chunk\n\u001b[0;32m    589\u001b[0m \u001b[39m# Apply the specified functions to each row in the batch\u001b[39;00m\n\u001b[1;32m--> 590\u001b[0m chunk \u001b[39m=\u001b[39m applier(function_cols, chunk, progress_bar\u001b[39m=\u001b[39;49mprogress_bar)\n\u001b[0;32m    592\u001b[0m \u001b[39m# If an output file is specified, append the processed data to it\u001b[39;00m\n\u001b[0;32m    593\u001b[0m \u001b[39mif\u001b[39;00m new_file \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\pipeline.py:564\u001b[0m, in \u001b[0;36mapplier\u001b[1;34m(function_cols, chunk, progress_bar)\u001b[0m\n\u001b[0;32m    562\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    563\u001b[0m     \u001b[39mif\u001b[39;00m progress_bar:\n\u001b[1;32m--> 564\u001b[0m         chunk[to_col] \u001b[39m=\u001b[39m chunk[from_col]\u001b[39m.\u001b[39mprogress_apply(function\u001b[39m.\u001b[39mfunction_to_apply)\n\u001b[0;32m    565\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    566\u001b[0m         chunk[to_col] \u001b[39m=\u001b[39m chunk[from_col]\u001b[39m.\u001b[39mapply(function\u001b[39m.\u001b[39mfunction_to_apply)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\pandas\\core\\frame.py:3805\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3803\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3804\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3805\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3806\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3807\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3810\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'type'"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/combined_cleaned.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/combined_cleaned_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/dataset.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nans\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/sample/dataset_unbalanced_1M.csv\",\n",
    "    [(pp.Delete_nan(), 'content_title'),\n",
    "     (pp.Delete_nan(), 'content_domain'),\n",
    "     (pp.Delete_nan(), 'content_authors'),\n",
    "     (pp.Delete_nan(), 'content_domain_authors_title')],\n",
    "     new_file=\"../datasets/sample/dataset_unbalanced_1M_.csv\",\n",
    "     progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset_bin.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_unbalanced_1M.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 768849.38it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 539.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 18019.53it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1205.96it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 161.14it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 82612.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 169198.84it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 15205.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 124929.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 51255.62it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6579.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 497940.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 167259.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 38457.73it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37036.88it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37947.78it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 26879.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 97.20679092407227\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting liar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data = pd.read_csv(\"../datasets/big/combined_cleaned_bin.csv\")\n",
    "X_liar =  liar_data[\"statement_combined\"].values\n",
    "y_liar = liar_data[\"type_binary\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models (other than logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_bayes finished in 3.49 seconds\n",
      "random_forest finished in 129.19 seconds\n",
      "decision_tree finished in 56.97 seconds\n",
      "ada_boost finished in 64.29 seconds\n",
      "passive_aggressive finished in 105.30 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>val</td>\n",
       "      <td>0.897184</td>\n",
       "      <td>0.849520</td>\n",
       "      <td>0.831041</td>\n",
       "      <td>0.892127</td>\n",
       "      <td>0.860501</td>\n",
       "      <td>104.06</td>\n",
       "      <td>[[38540, 9436], [5612, 46412]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>val</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.805740</td>\n",
       "      <td>0.816596</td>\n",
       "      <td>0.808089</td>\n",
       "      <td>0.812320</td>\n",
       "      <td>1.28</td>\n",
       "      <td>[[38534, 9442], [9984, 42040]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>val</td>\n",
       "      <td>0.686635</td>\n",
       "      <td>0.686450</td>\n",
       "      <td>0.662730</td>\n",
       "      <td>0.809011</td>\n",
       "      <td>0.728601</td>\n",
       "      <td>36.01</td>\n",
       "      <td>[[26557, 21419], [9936, 42088]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>val</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.653150</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.428456</td>\n",
       "      <td>0.562417</td>\n",
       "      <td>55.09</td>\n",
       "      <td>[[43025, 4951], [29734, 22290]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>val</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.653150</td>\n",
       "      <td>0.818252</td>\n",
       "      <td>0.428456</td>\n",
       "      <td>0.562417</td>\n",
       "      <td>60.94</td>\n",
       "      <td>[[43025, 4951], [29734, 22290]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>test</td>\n",
       "      <td>0.897184</td>\n",
       "      <td>0.848720</td>\n",
       "      <td>0.831731</td>\n",
       "      <td>0.892052</td>\n",
       "      <td>0.860836</td>\n",
       "      <td>104.06</td>\n",
       "      <td>[[38083, 9466], [5662, 46789]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>test</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.809479</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>1.28</td>\n",
       "      <td>[[38334, 9215], [9993, 42458]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>test</td>\n",
       "      <td>0.686635</td>\n",
       "      <td>0.688660</td>\n",
       "      <td>0.667022</td>\n",
       "      <td>0.811538</td>\n",
       "      <td>0.732218</td>\n",
       "      <td>36.01</td>\n",
       "      <td>[[26300, 21249], [9885, 42566]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>test</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.652750</td>\n",
       "      <td>0.821077</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.566236</td>\n",
       "      <td>55.09</td>\n",
       "      <td>[[42610, 4939], [29786, 22665]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>test</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.652750</td>\n",
       "      <td>0.821077</td>\n",
       "      <td>0.432118</td>\n",
       "      <td>0.566236</td>\n",
       "      <td>60.94</td>\n",
       "      <td>[[42610, 4939], [29786, 22665]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.529435</td>\n",
       "      <td>0.561704</td>\n",
       "      <td>0.711382</td>\n",
       "      <td>0.627744</td>\n",
       "      <td>1.28</td>\n",
       "      <td>[[1697, 3960], [2059, 5075]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.897184</td>\n",
       "      <td>0.490032</td>\n",
       "      <td>0.577010</td>\n",
       "      <td>0.320858</td>\n",
       "      <td>0.412395</td>\n",
       "      <td>104.06</td>\n",
       "      <td>[[3979, 1678], [4845, 2289]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.444062</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>55.09</td>\n",
       "      <td>[[5616, 41], [7070, 64]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.653153</td>\n",
       "      <td>0.444062</td>\n",
       "      <td>0.609524</td>\n",
       "      <td>0.008971</td>\n",
       "      <td>0.017682</td>\n",
       "      <td>60.94</td>\n",
       "      <td>[[5616, 41], [7070, 64]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.686635</td>\n",
       "      <td>0.443984</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.013319</td>\n",
       "      <td>36.01</td>\n",
       "      <td>[[5631, 26], [7086, 48]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name split  train_acc       acc  precision    recall  \\\n",
       "0  passive_aggressive   val   0.897184  0.849520   0.831041  0.892127   \n",
       "0         naive_bayes   val   0.817462  0.805740   0.816596  0.808089   \n",
       "0       random_forest   val   0.686635  0.686450   0.662730  0.809011   \n",
       "0       decision_tree   val   0.653153  0.653150   0.818252  0.428456   \n",
       "0           ada_boost   val   0.653153  0.653150   0.818252  0.428456   \n",
       "1  passive_aggressive  test   0.897184  0.848720   0.831731  0.892052   \n",
       "1         naive_bayes  test   0.817462  0.807920   0.821667  0.809479   \n",
       "1       random_forest  test   0.686635  0.688660   0.667022  0.811538   \n",
       "1       decision_tree  test   0.653153  0.652750   0.821077  0.432118   \n",
       "1           ada_boost  test   0.653153  0.652750   0.821077  0.432118   \n",
       "2         naive_bayes  liar   0.817462  0.529435   0.561704  0.711382   \n",
       "2  passive_aggressive  liar   0.897184  0.490032   0.577010  0.320858   \n",
       "2       decision_tree  liar   0.653153  0.444062   0.609524  0.008971   \n",
       "2           ada_boost  liar   0.653153  0.444062   0.609524  0.008971   \n",
       "2       random_forest  liar   0.686635  0.443984   0.648649  0.006728   \n",
       "\n",
       "         f1    time                 confusion_matrix  \\\n",
       "0  0.860501  104.06   [[38540, 9436], [5612, 46412]]   \n",
       "0  0.812320    1.28   [[38534, 9442], [9984, 42040]]   \n",
       "0  0.728601   36.01  [[26557, 21419], [9936, 42088]]   \n",
       "0  0.562417   55.09  [[43025, 4951], [29734, 22290]]   \n",
       "0  0.562417   60.94  [[43025, 4951], [29734, 22290]]   \n",
       "1  0.860836  104.06   [[38083, 9466], [5662, 46789]]   \n",
       "1  0.815528    1.28   [[38334, 9215], [9993, 42458]]   \n",
       "1  0.732218   36.01  [[26300, 21249], [9885, 42566]]   \n",
       "1  0.566236   55.09  [[42610, 4939], [29786, 22665]]   \n",
       "1  0.566236   60.94  [[42610, 4939], [29786, 22665]]   \n",
       "2  0.627744    1.28     [[1697, 3960], [2059, 5075]]   \n",
       "2  0.412395  104.06     [[3979, 1678], [4845, 2289]]   \n",
       "2  0.017682   55.09         [[5616, 41], [7070, 64]]   \n",
       "2  0.017682   60.94         [[5616, 41], [7070, 64]]   \n",
       "2  0.013319   36.01         [[5631, 26], [7086, 48]]   \n",
       "\n",
       "                                               model  \n",
       "0                      PassiveAggressiveClassifier()  \n",
       "0                                    MultinomialNB()  \n",
       "0  (DecisionTreeClassifier(max_depth=5, max_featu...  \n",
       "0                DecisionTreeClassifier(max_depth=2)  \n",
       "0  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "1                      PassiveAggressiveClassifier()  \n",
       "1                                    MultinomialNB()  \n",
       "1  (DecisionTreeClassifier(max_depth=5, max_featu...  \n",
       "1                DecisionTreeClassifier(max_depth=2)  \n",
       "1  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "2                                    MultinomialNB()  \n",
       "2                      PassiveAggressiveClassifier()  \n",
       "2                DecisionTreeClassifier(max_depth=2)  \n",
       "2  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "2  (DecisionTreeClassifier(max_depth=5, max_featu...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info_list = [(\n",
    "    \"../datasets/sample/dataset_unbalanced_1M.csv\", \"content_combined\", mt.create_count_vector, [\n",
    "        (MultinomialNB(), \"naive_bayes\"),\n",
    "        (RandomForestClassifier(max_depth=5), \"random_forest\"), #25\n",
    "        (DecisionTreeClassifier(max_depth=2), \"decision_tree\"),\n",
    "        (AdaBoostClassifier(n_estimators=2), \"ada_boost\"), #2\n",
    "        #(SVC(kernel='linear', max_iter=10), \"svm\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=2, algorithm='kd_tree'), \"knn\"), #15\n",
    "        (PassiveAggressiveClassifier(), \"passive_aggressive\")\n",
    "        ])\n",
    "]\n",
    "\n",
    "test_stats_base = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_100K.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_base)\n",
    "test_stats_base.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.873580</td>\n",
       "      <td>0.892032</td>\n",
       "      <td>0.861237</td>\n",
       "      <td>0.876364</td>\n",
       "      <td>357.04</td>\n",
       "      <td>[[42553, 5423], [7219, 44805]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.872150</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.858969</td>\n",
       "      <td>0.874852</td>\n",
       "      <td>370.91</td>\n",
       "      <td>[[42528, 5448], [7337, 44687]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.899829</td>\n",
       "      <td>0.870950</td>\n",
       "      <td>0.889639</td>\n",
       "      <td>0.858431</td>\n",
       "      <td>0.873756</td>\n",
       "      <td>361.62</td>\n",
       "      <td>[[42436, 5540], [7365, 44659]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.871240</td>\n",
       "      <td>0.890248</td>\n",
       "      <td>0.860613</td>\n",
       "      <td>0.875179</td>\n",
       "      <td>357.04</td>\n",
       "      <td>[[41984, 5565], [7311, 45140]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.899829</td>\n",
       "      <td>0.870400</td>\n",
       "      <td>0.888821</td>\n",
       "      <td>0.860556</td>\n",
       "      <td>0.874460</td>\n",
       "      <td>361.62</td>\n",
       "      <td>[[41903, 5646], [7314, 45137]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.870110</td>\n",
       "      <td>0.889033</td>\n",
       "      <td>0.859659</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>370.91</td>\n",
       "      <td>[[41921, 5628], [7361, 45090]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.899829</td>\n",
       "      <td>0.477132</td>\n",
       "      <td>0.586167</td>\n",
       "      <td>0.212644</td>\n",
       "      <td>0.312076</td>\n",
       "      <td>361.62</td>\n",
       "      <td>[[4586, 1071], [5617, 1517]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.212363</td>\n",
       "      <td>0.311280</td>\n",
       "      <td>370.91</td>\n",
       "      <td>[[4572, 1085], [5619, 1515]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.473849</td>\n",
       "      <td>0.583540</td>\n",
       "      <td>0.197785</td>\n",
       "      <td>0.295436</td>\n",
       "      <td>357.04</td>\n",
       "      <td>[[4650, 1007], [5723, 1411]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name split  train_acc       acc  precision    recall  \\\n",
       "0  content_count_hyper   val   0.897783  0.873580   0.892032  0.861237   \n",
       "0        content_count   val   0.900499  0.872150   0.891333  0.858969   \n",
       "0  content_count_hyper   val   0.899829  0.870950   0.889639  0.858431   \n",
       "1  content_count_hyper  test   0.897783  0.871240   0.890248  0.860613   \n",
       "1  content_count_hyper  test   0.899829  0.870400   0.888821  0.860556   \n",
       "1        content_count  test   0.900499  0.870110   0.889033  0.859659   \n",
       "2  content_count_hyper  liar   0.899829  0.477132   0.586167  0.212644   \n",
       "2        content_count  liar   0.900499  0.475881   0.582692  0.212363   \n",
       "2  content_count_hyper  liar   0.897783  0.473849   0.583540  0.197785   \n",
       "\n",
       "         f1    time                confusion_matrix  \\\n",
       "0  0.876364  357.04  [[42553, 5423], [7219, 44805]]   \n",
       "0  0.874852  370.91  [[42528, 5448], [7337, 44687]]   \n",
       "0  0.873756  361.62  [[42436, 5540], [7365, 44659]]   \n",
       "1  0.875179  357.04  [[41984, 5565], [7311, 45140]]   \n",
       "1  0.874460  361.62  [[41903, 5646], [7314, 45137]]   \n",
       "1  0.874100  370.91  [[41921, 5628], [7361, 45090]]   \n",
       "2  0.312076  361.62    [[4586, 1071], [5617, 1517]]   \n",
       "2  0.311280  370.91    [[4572, 1085], [5619, 1515]]   \n",
       "2  0.295436  357.04    [[4650, 1007], [5723, 1411]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINISHED\n",
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count\"), \n",
    "                                                              (LogisticRegression(max_iter=300, C=0.1), \"content_count_hyper\"),\n",
    "                                                              (LogisticRegression(max_iter=300, C=250), \"content_count_hyper\")]),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_types\")]),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_bin\")]),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_reliable_fake\")]),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 1.3838915824890137 seconds)\n",
      "Saved vector 0 in 4.555252313613892 seconds\n",
      "Creating vector 1 (data read in 1.1119475364685059 seconds)\n",
      "Saved vector 1 in 15.999372720718384 seconds\n",
      "Creating vector 2 (data read in 1.0411128997802734 seconds)\n",
      "Saved vector 2 in 117.24633693695068 seconds\n",
      "content_tfidf_bi finished in 0.62 seconds\n",
      "content_tfidf_bi_hyper_1 finished in 3.26 seconds\n",
      "content_tfidf_bi_hyper_2 finished in 0.34 seconds\n",
      "content_tfidf_bi finished in 7.79 seconds\n",
      "content_tfidf_bi_hyper_1 finished in 34.92 seconds\n",
      "content_tfidf_bi_hyper_2 finished in 7.24 seconds\n",
      "content_tfidf_tri finished in 15.85 seconds\n",
      "content_tfidf_tri_hyper_1 finished in 35.05 seconds\n",
      "content_tfidf_hyper_2 finished in 12.46 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.847000</td>\n",
       "      <td>0.838475</td>\n",
       "      <td>0.878327</td>\n",
       "      <td>0.857939</td>\n",
       "      <td>34.86</td>\n",
       "      <td>[[385, 89], [64, 462]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_tri_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.842000</td>\n",
       "      <td>0.818339</td>\n",
       "      <td>0.899240</td>\n",
       "      <td>0.856884</td>\n",
       "      <td>34.98</td>\n",
       "      <td>[[369, 105], [53, 473]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>val</td>\n",
       "      <td>0.965250</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.815603</td>\n",
       "      <td>0.874525</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>7.72</td>\n",
       "      <td>[[370, 104], [66, 460]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>val</td>\n",
       "      <td>0.978250</td>\n",
       "      <td>0.825000</td>\n",
       "      <td>0.805217</td>\n",
       "      <td>0.880228</td>\n",
       "      <td>0.841054</td>\n",
       "      <td>15.78</td>\n",
       "      <td>[[362, 112], [63, 463]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>val</td>\n",
       "      <td>0.919375</td>\n",
       "      <td>0.823000</td>\n",
       "      <td>0.822551</td>\n",
       "      <td>0.846008</td>\n",
       "      <td>0.834114</td>\n",
       "      <td>0.58</td>\n",
       "      <td>[[378, 96], [81, 445]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.797000</td>\n",
       "      <td>0.753532</td>\n",
       "      <td>0.912548</td>\n",
       "      <td>0.825451</td>\n",
       "      <td>7.16</td>\n",
       "      <td>[[317, 157], [46, 480]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.784000</td>\n",
       "      <td>0.722063</td>\n",
       "      <td>0.958175</td>\n",
       "      <td>0.823529</td>\n",
       "      <td>12.39</td>\n",
       "      <td>[[280, 194], [22, 504]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.810000</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.836502</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>3.23</td>\n",
       "      <td>[[370, 104], [86, 440]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.782000</td>\n",
       "      <td>0.767361</td>\n",
       "      <td>0.840304</td>\n",
       "      <td>0.802178</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[340, 134], [84, 442]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_tri_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.833000</td>\n",
       "      <td>0.803309</td>\n",
       "      <td>0.879276</td>\n",
       "      <td>0.839577</td>\n",
       "      <td>34.98</td>\n",
       "      <td>[[396, 107], [60, 437]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.820809</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.838583</td>\n",
       "      <td>34.86</td>\n",
       "      <td>[[410, 93], [71, 426]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>test</td>\n",
       "      <td>0.978250</td>\n",
       "      <td>0.820000</td>\n",
       "      <td>0.781528</td>\n",
       "      <td>0.885312</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>15.78</td>\n",
       "      <td>[[380, 123], [57, 440]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>test</td>\n",
       "      <td>0.919375</td>\n",
       "      <td>0.828000</td>\n",
       "      <td>0.816764</td>\n",
       "      <td>0.843058</td>\n",
       "      <td>0.829703</td>\n",
       "      <td>0.58</td>\n",
       "      <td>[[409, 94], [78, 419]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>test</td>\n",
       "      <td>0.965250</td>\n",
       "      <td>0.822000</td>\n",
       "      <td>0.794824</td>\n",
       "      <td>0.865191</td>\n",
       "      <td>0.828516</td>\n",
       "      <td>7.72</td>\n",
       "      <td>[[392, 111], [67, 430]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.808000</td>\n",
       "      <td>0.804391</td>\n",
       "      <td>0.810865</td>\n",
       "      <td>0.807615</td>\n",
       "      <td>3.23</td>\n",
       "      <td>[[405, 98], [94, 403]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.790000</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.845070</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[370, 133], [77, 420]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.777000</td>\n",
       "      <td>0.722403</td>\n",
       "      <td>0.895372</td>\n",
       "      <td>0.799641</td>\n",
       "      <td>7.16</td>\n",
       "      <td>[[332, 171], [52, 445]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.762000</td>\n",
       "      <td>0.692996</td>\n",
       "      <td>0.935614</td>\n",
       "      <td>0.796233</td>\n",
       "      <td>12.39</td>\n",
       "      <td>[[297, 206], [32, 465]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.886500</td>\n",
       "      <td>0.556954</td>\n",
       "      <td>0.571317</td>\n",
       "      <td>0.823661</td>\n",
       "      <td>0.674666</td>\n",
       "      <td>12.39</td>\n",
       "      <td>[[1248, 4409], [1258, 5876]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.978250</td>\n",
       "      <td>0.549292</td>\n",
       "      <td>0.583020</td>\n",
       "      <td>0.673816</td>\n",
       "      <td>0.625138</td>\n",
       "      <td>15.78</td>\n",
       "      <td>[[2219, 3438], [2327, 4807]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.874250</td>\n",
       "      <td>0.547182</td>\n",
       "      <td>0.583624</td>\n",
       "      <td>0.656434</td>\n",
       "      <td>0.617892</td>\n",
       "      <td>7.16</td>\n",
       "      <td>[[2316, 3341], [2451, 4683]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.965250</td>\n",
       "      <td>0.542647</td>\n",
       "      <td>0.588699</td>\n",
       "      <td>0.597281</td>\n",
       "      <td>0.592959</td>\n",
       "      <td>7.72</td>\n",
       "      <td>[[2680, 2977], [2873, 4261]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.838500</td>\n",
       "      <td>0.541005</td>\n",
       "      <td>0.592991</td>\n",
       "      <td>0.564480</td>\n",
       "      <td>0.578384</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[[2893, 2764], [3107, 4027]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.919375</td>\n",
       "      <td>0.539833</td>\n",
       "      <td>0.592747</td>\n",
       "      <td>0.559013</td>\n",
       "      <td>0.575386</td>\n",
       "      <td>0.58</td>\n",
       "      <td>[[2917, 2740], [3146, 3988]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_tri_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.528028</td>\n",
       "      <td>0.592982</td>\n",
       "      <td>0.490328</td>\n",
       "      <td>0.536791</td>\n",
       "      <td>34.98</td>\n",
       "      <td>[[3256, 2401], [3636, 3498]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520522</td>\n",
       "      <td>0.588101</td>\n",
       "      <td>0.468321</td>\n",
       "      <td>0.521420</td>\n",
       "      <td>3.23</td>\n",
       "      <td>[[3317, 2340], [3793, 3341]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.523728</td>\n",
       "      <td>0.602317</td>\n",
       "      <td>0.429913</td>\n",
       "      <td>0.501718</td>\n",
       "      <td>34.86</td>\n",
       "      <td>[[3632, 2025], [4067, 3067]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        name split  train_acc       acc  precision    recall  \\\n",
       "0   content_tfidf_bi_hyper_1   val   1.000000  0.847000   0.838475  0.878327   \n",
       "0  content_tfidf_tri_hyper_1   val   1.000000  0.842000   0.818339  0.899240   \n",
       "0           content_tfidf_bi   val   0.965250  0.830000   0.815603  0.874525   \n",
       "0          content_tfidf_tri   val   0.978250  0.825000   0.805217  0.880228   \n",
       "0           content_tfidf_bi   val   0.919375  0.823000   0.822551  0.846008   \n",
       "0   content_tfidf_bi_hyper_2   val   0.874250  0.797000   0.753532  0.912548   \n",
       "0      content_tfidf_hyper_2   val   0.886500  0.784000   0.722063  0.958175   \n",
       "0   content_tfidf_bi_hyper_1   val   1.000000  0.810000   0.808824  0.836502   \n",
       "0   content_tfidf_bi_hyper_2   val   0.838500  0.782000   0.767361  0.840304   \n",
       "1  content_tfidf_tri_hyper_1  test   1.000000  0.833000   0.803309  0.879276   \n",
       "1   content_tfidf_bi_hyper_1  test   1.000000  0.836000   0.820809  0.857143   \n",
       "1          content_tfidf_tri  test   0.978250  0.820000   0.781528  0.885312   \n",
       "1           content_tfidf_bi  test   0.919375  0.828000   0.816764  0.843058   \n",
       "1           content_tfidf_bi  test   0.965250  0.822000   0.794824  0.865191   \n",
       "1   content_tfidf_bi_hyper_1  test   1.000000  0.808000   0.804391  0.810865   \n",
       "1   content_tfidf_bi_hyper_2  test   0.838500  0.790000   0.759494  0.845070   \n",
       "1   content_tfidf_bi_hyper_2  test   0.874250  0.777000   0.722403  0.895372   \n",
       "1      content_tfidf_hyper_2  test   0.886500  0.762000   0.692996  0.935614   \n",
       "2      content_tfidf_hyper_2  liar   0.886500  0.556954   0.571317  0.823661   \n",
       "2          content_tfidf_tri  liar   0.978250  0.549292   0.583020  0.673816   \n",
       "2   content_tfidf_bi_hyper_2  liar   0.874250  0.547182   0.583624  0.656434   \n",
       "2           content_tfidf_bi  liar   0.965250  0.542647   0.588699  0.597281   \n",
       "2   content_tfidf_bi_hyper_2  liar   0.838500  0.541005   0.592991  0.564480   \n",
       "2           content_tfidf_bi  liar   0.919375  0.539833   0.592747  0.559013   \n",
       "2  content_tfidf_tri_hyper_1  liar   1.000000  0.528028   0.592982  0.490328   \n",
       "2   content_tfidf_bi_hyper_1  liar   1.000000  0.520522   0.588101  0.468321   \n",
       "2   content_tfidf_bi_hyper_1  liar   1.000000  0.523728   0.602317  0.429913   \n",
       "\n",
       "         f1   time              confusion_matrix  \\\n",
       "0  0.857939  34.86        [[385, 89], [64, 462]]   \n",
       "0  0.856884  34.98       [[369, 105], [53, 473]]   \n",
       "0  0.844037   7.72       [[370, 104], [66, 460]]   \n",
       "0  0.841054  15.78       [[362, 112], [63, 463]]   \n",
       "0  0.834114   0.58        [[378, 96], [81, 445]]   \n",
       "0  0.825451   7.16       [[317, 157], [46, 480]]   \n",
       "0  0.823529  12.39       [[280, 194], [22, 504]]   \n",
       "0  0.822430   3.23       [[370, 104], [86, 440]]   \n",
       "0  0.802178   0.30       [[340, 134], [84, 442]]   \n",
       "1  0.839577  34.98       [[396, 107], [60, 437]]   \n",
       "1  0.838583  34.86        [[410, 93], [71, 426]]   \n",
       "1  0.830189  15.78       [[380, 123], [57, 440]]   \n",
       "1  0.829703   0.58        [[409, 94], [78, 419]]   \n",
       "1  0.828516   7.72       [[392, 111], [67, 430]]   \n",
       "1  0.807615   3.23        [[405, 98], [94, 403]]   \n",
       "1  0.800000   0.30       [[370, 133], [77, 420]]   \n",
       "1  0.799641   7.16       [[332, 171], [52, 445]]   \n",
       "1  0.796233  12.39       [[297, 206], [32, 465]]   \n",
       "2  0.674666  12.39  [[1248, 4409], [1258, 5876]]   \n",
       "2  0.625138  15.78  [[2219, 3438], [2327, 4807]]   \n",
       "2  0.617892   7.16  [[2316, 3341], [2451, 4683]]   \n",
       "2  0.592959   7.72  [[2680, 2977], [2873, 4261]]   \n",
       "2  0.578384   0.30  [[2893, 2764], [3107, 4027]]   \n",
       "2  0.575386   0.58  [[2917, 2740], [3146, 3988]]   \n",
       "2  0.536791  34.98  [[3256, 2401], [3636, 3498]]   \n",
       "2  0.521420   3.23  [[3317, 2340], [3793, 3341]]   \n",
       "2  0.501718  34.86  [[3632, 2025], [4067, 3067]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "info_list = [\n",
    "      (\"../datasets/sample/dataset_unbalanced_10K.csv\", \"content_combined\", mt.create_tdfidf_vector_unigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_bi\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_bi_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_bi_hyper_2\")]),\n",
    "     (\"../datasets/sample/dataset_unbalanced_10K.csv\", \"content_combined\", mt.create_tdfidf_vector_bigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_bi\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_bi_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_bi_hyper_2\")]),\n",
    "     (\"../datasets/sample/dataset_unbalanced_10K.csv\", \"content_combined\", mt.create_tdfidf_vector_trigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_tri\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_tri_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_tdidf_bitri = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_10K.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_10K.pickle\", info_list, tests=test_stats_tdidf_bitri)\n",
    "test_stats_tdidf_bitri.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 108.31220841407776 seconds)\n",
      "Saved vector 0 in 440.462379693985 seconds\n",
      "Creating vector 1 (data read in 106.4322395324707 seconds)\n",
      "Saved vector 1 in 427.0809762477875 seconds\n",
      "Creating vector 2 (data read in 102.18216276168823 seconds)\n",
      "Saved vector 2 in 409.81818413734436 seconds\n",
      "Creating vector 3 (data read in 105.79786205291748 seconds)\n",
      "Saved vector 3 in 438.70478320121765 seconds\n",
      "content_title_count finished in 385.34 seconds\n",
      "content_domain_count finished in 337.14 seconds\n",
      "content_authors_count finished in 335.44 seconds\n",
      "all_count finished in 343.60 seconds\n",
      "all_count_hyper_1 finished in 336.64 seconds\n",
      "all_count_hyper_2 finished in 343.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.996040</td>\n",
       "      <td>0.996939</td>\n",
       "      <td>0.995444</td>\n",
       "      <td>0.996191</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[47817, 159], [237, 51787]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.995730</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>0.995464</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[47785, 191], [236, 51788]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.994480</td>\n",
       "      <td>0.995152</td>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.994692</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[47724, 252], [300, 51724]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.993980</td>\n",
       "      <td>0.994614</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.994212</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[47696, 280], [322, 51702]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>0.938097</td>\n",
       "      <td>0.912329</td>\n",
       "      <td>0.925033</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[44844, 3132], [4561, 47463]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.882290</td>\n",
       "      <td>0.894089</td>\n",
       "      <td>0.877710</td>\n",
       "      <td>0.885824</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[42567, 5409], [6362, 45662]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.996060</td>\n",
       "      <td>0.996869</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>0.996242</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[47385, 164], [230, 52221]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.995690</td>\n",
       "      <td>0.996317</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[47356, 193], [238, 52213]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.994520</td>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.994223</td>\n",
       "      <td>0.994773</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[47304, 245], [303, 52148]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.995226</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.994419</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[47299, 250], [335, 52116]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.920630</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>0.909630</td>\n",
       "      <td>0.923209</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[44352, 3197], [4740, 47711]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.881610</td>\n",
       "      <td>0.894046</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.886145</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[42089, 5460], [6379, 46072]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.483699</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.267452</td>\n",
       "      <td>0.366219</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[4279, 1378], [5226, 1908]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.448988</td>\n",
       "      <td>0.563798</td>\n",
       "      <td>0.053266</td>\n",
       "      <td>0.097336</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[5363, 294], [6754, 380]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.449222</td>\n",
       "      <td>0.604215</td>\n",
       "      <td>0.036165</td>\n",
       "      <td>0.068245</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[5488, 169], [6876, 258]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.443280</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.009183</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[5637, 20], [7101, 33]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.442342</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[5643, 14], [7119, 15]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.442186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[5656, 1], [7134, 0]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name split  train_acc       acc  precision    recall  \\\n",
       "0      all_count_hyper_2   val   0.999620  0.996040   0.996939  0.995444   \n",
       "0              all_count   val   0.999941  0.995730   0.996325  0.995464   \n",
       "0      all_count_hyper_1   val   0.999982  0.994480   0.995152  0.994233   \n",
       "0   content_domain_count   val   0.999764  0.993980   0.994614  0.993811   \n",
       "0  content_authors_count   val   0.946640  0.923070   0.938097  0.912329   \n",
       "0    content_title_count   val   0.911011  0.882290   0.894089  0.877710   \n",
       "1      all_count_hyper_2  test   0.999620  0.996060   0.996869  0.995615   \n",
       "1              all_count  test   0.999941  0.995690   0.996317  0.995462   \n",
       "1      all_count_hyper_1  test   0.999982  0.994520   0.995324  0.994223   \n",
       "1   content_domain_count  test   0.999764  0.994150   0.995226  0.993613   \n",
       "1  content_authors_count  test   0.946640  0.920630   0.937200  0.909630   \n",
       "1    content_title_count  test   0.911011  0.881610   0.894046  0.878382   \n",
       "2    content_title_count  liar   0.911011  0.483699   0.580645  0.267452   \n",
       "2  content_authors_count  liar   0.946640  0.448988   0.563798  0.053266   \n",
       "2      all_count_hyper_1  liar   0.999982  0.449222   0.604215  0.036165   \n",
       "2   content_domain_count  liar   0.999764  0.443280   0.622642  0.004626   \n",
       "2              all_count  liar   0.999941  0.442342   0.517241  0.002103   \n",
       "2      all_count_hyper_2  liar   0.999620  0.442186   0.000000  0.000000   \n",
       "\n",
       "         f1    time                confusion_matrix  \\\n",
       "0  0.996191  342.35    [[47817, 159], [237, 51787]]   \n",
       "0  0.995894  342.31    [[47785, 191], [236, 51788]]   \n",
       "0  0.994692  335.35    [[47724, 252], [300, 51724]]   \n",
       "0  0.994212  335.69    [[47696, 280], [322, 51702]]   \n",
       "0  0.925033  334.12  [[44844, 3132], [4561, 47463]]   \n",
       "0  0.885824  383.68  [[42567, 5409], [6362, 45662]]   \n",
       "1  0.996242  342.35    [[47385, 164], [230, 52221]]   \n",
       "1  0.995890  342.31    [[47356, 193], [238, 52213]]   \n",
       "1  0.994773  335.35    [[47304, 245], [303, 52148]]   \n",
       "1  0.994419  335.69    [[47299, 250], [335, 52116]]   \n",
       "1  0.923209  334.12  [[44352, 3197], [4740, 47711]]   \n",
       "1  0.886145  383.68  [[42089, 5460], [6379, 46072]]   \n",
       "2  0.366219  383.68    [[4279, 1378], [5226, 1908]]   \n",
       "2  0.097336  334.12      [[5363, 294], [6754, 380]]   \n",
       "2  0.068245  335.35      [[5488, 169], [6876, 258]]   \n",
       "2  0.009183  335.69        [[5637, 20], [7101, 33]]   \n",
       "2  0.004188  342.31        [[5643, 14], [7119, 15]]   \n",
       "2  0.000000  342.35          [[5656, 1], [7134, 0]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_title_count\")]),\n",
    "    (unbalanced, \"content_domain\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_domain_count\")]),\n",
    "    (unbalanced, \"content_authors\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_authors_count\")]),\n",
    "    (unbalanced, \"content_domain_authors_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"all_count\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=250), \"all_count_hyper_1\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=0.1), \"all_count_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_meta = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, tests=test_stats_meta)\n",
    "test_stats_meta.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"content_count_hyper\"\n",
    "\n",
    "metrics = test_stats_simple.metrics\n",
    "metrics_test = metrics[metrics[\"split\"] == \"test\"]\n",
    "cm = metrics_test[metrics_test[\"name\"] == model_name][\"confusion_matrix\"].values[0]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sns.heatmap([[tp, fn],[fp, tn]], annot=True, cmap=\"Blues\", xticklabels=[\"Fake\",\"True\"], yticklabels=[\"Fake\",\"True\"], fmt=\"d\", annot_kws={\"size\": 12})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrlll}\n",
      "\\toprule\n",
      "                        name & split &  train\\_acc &      acc &  precision &   recall &       f1 &  time &             confusion\\_matrix &                                   model \\\\\n",
      "\\midrule\n",
      "                   all\\_count &   val &   1.000000 & 0.915000 &   0.918406 & 0.920152 & 0.919278 &  4.07 &       [[431, 43], [42, 484]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &   val &   1.000000 & 0.907000 &   0.912381 & 0.910646 & 0.911513 &  4.97 &       [[428, 46], [47, 479]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &   val &   0.997500 & 0.903000 &   0.911708 & 0.903042 & 0.907354 &  2.61 &       [[428, 46], [51, 475]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &   val &   1.000000 & 0.901000 &   0.905123 & 0.906844 & 0.905983 &  3.18 &       [[424, 50], [49, 477]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &   val &   0.999375 & 0.851000 &   0.845872 & 0.876426 & 0.860878 &  4.05 &       [[390, 84], [65, 461]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &   val &   0.965250 & 0.830000 &   0.815603 & 0.874525 & 0.844037 &  8.01 &      [[370, 104], [66, 460]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &   val &   0.978250 & 0.825000 &   0.805217 & 0.880228 & 0.841054 & 16.49 &      [[362, 112], [63, 463]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &   val &   0.919375 & 0.823000 &   0.822551 & 0.846008 & 0.834114 &  0.56 &       [[378, 96], [81, 445]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &   val &   1.000000 & 0.810000 &   0.808824 & 0.836502 & 0.822430 &  3.55 &      [[370, 104], [86, 440]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &   val &   0.999375 & 0.809000 &   0.820268 & 0.815589 & 0.817922 &  4.24 &       [[380, 94], [97, 429]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &   val &   0.980500 & 0.807000 &   0.817143 & 0.815589 & 0.816365 &  2.56 &       [[378, 96], [97, 429]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &   val &   0.998125 & 0.803000 &   0.802239 & 0.825336 & 0.813623 &  3.83 &      [[373, 106], [91, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_count &   val &   0.998500 & 0.802000 &   0.805970 & 0.821293 & 0.813559 &  5.31 &      [[370, 104], [94, 432]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &   val &   0.838500 & 0.782000 &   0.767361 & 0.840304 & 0.802178 &  0.35 &      [[340, 134], [84, 442]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &   val &   0.998200 & 0.764000 &   0.862319 & 0.666045 & 0.751579 &  2.31 &      [[407, 57], [179, 357]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &   val &   0.999625 & 0.684000 &   0.753589 & 0.596591 & 0.665962 &  3.46 &     [[369, 103], [213, 315]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  test &   1.000000 & 0.914000 &   0.911824 & 0.915493 & 0.913655 &  4.97 &       [[459, 44], [42, 455]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  test &   1.000000 & 0.907000 &   0.908907 & 0.903421 & 0.906155 &  4.07 &       [[458, 45], [48, 449]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  test &   1.000000 & 0.901000 &   0.899598 & 0.901408 & 0.900503 &  3.18 &       [[453, 50], [49, 448]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  test &   0.997500 & 0.901000 &   0.904472 & 0.895372 & 0.899899 &  2.61 &       [[456, 47], [52, 445]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  test &   0.978250 & 0.820000 &   0.781528 & 0.885312 & 0.830189 & 16.49 &      [[380, 123], [57, 440]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  test &   0.919375 & 0.828000 &   0.816764 & 0.843058 & 0.829703 &  0.56 &       [[409, 94], [78, 419]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  test &   0.965250 & 0.822000 &   0.794824 & 0.865191 & 0.828516 &  8.01 &      [[392, 111], [67, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  test &   0.999375 & 0.819000 &   0.829167 & 0.800805 & 0.814739 &  4.05 &       [[421, 82], [99, 398]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  test &   1.000000 & 0.808000 &   0.804391 & 0.810865 & 0.807615 &  3.55 &       [[405, 98], [94, 403]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  test &   0.998500 & 0.806000 &   0.808554 & 0.798793 & 0.803644 &  5.31 &      [[409, 94], [100, 397]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  test &   0.980500 & 0.807000 &   0.817992 & 0.786720 & 0.802051 &  2.56 &      [[416, 87], [106, 391]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  test &   0.838500 & 0.790000 &   0.759494 & 0.845070 & 0.800000 &  0.35 &      [[370, 133], [77, 420]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  test &   0.999375 & 0.802000 &   0.804481 & 0.794769 & 0.799595 &  4.24 &      [[407, 96], [102, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  test &   0.998125 & 0.798000 &   0.804481 & 0.788423 & 0.796371 &  3.83 &      [[403, 96], [106, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  test &   0.998200 & 0.775000 &   0.878173 & 0.661568 & 0.754635 &  2.31 &      [[429, 48], [177, 346]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  test &   0.999625 & 0.695000 &   0.750594 & 0.612403 & 0.674493 &  3.46 &     [[379, 105], [200, 316]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  liar &   0.978250 & 0.549292 &   0.583020 & 0.673816 & 0.625138 & 16.49 & [[2219, 3438], [2327, 4807]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  liar &   0.965250 & 0.542647 &   0.588699 & 0.597281 & 0.592959 &  8.01 & [[2680, 2977], [2873, 4261]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  liar &   0.838500 & 0.541005 &   0.592991 & 0.564480 & 0.578384 &  0.35 & [[2893, 2764], [3107, 4027]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  liar &   0.919375 & 0.539833 &   0.592747 & 0.559013 & 0.575386 &  0.56 & [[2917, 2740], [3146, 3988]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  liar &   1.000000 & 0.520522 &   0.588101 & 0.468321 & 0.521420 &  3.55 & [[3317, 2340], [3793, 3341]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  liar &   0.998500 & 0.480807 &   0.609410 & 0.192459 & 0.292532 &  5.31 &  [[4777, 880], [5761, 1373]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  liar &   0.998125 & 0.477132 &   0.613544 & 0.168909 & 0.264893 &  3.83 &  [[4898, 759], [5929, 1205]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  liar &   0.980500 & 0.475803 &   0.624927 & 0.150407 & 0.242458 &  2.56 &  [[5013, 644], [6061, 1073]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  liar &   0.999375 & 0.474318 &   0.618360 & 0.150126 & 0.241597 &  4.24 &  [[4996, 661], [6063, 1071]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  liar &   0.999625 & 0.470174 &   0.603358 & 0.146061 & 0.235188 &  3.46 &  [[4972, 685], [6092, 1042]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  liar &   0.999375 & 0.465093 &   0.627622 & 0.100645 & 0.173472 &  4.05 &   [[5231, 426], [6416, 718]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  liar &   0.998200 & 0.462044 &   0.625372 & 0.088450 & 0.154980 &  2.31 &   [[5279, 378], [6503, 631]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  liar &   1.000000 & 0.448440 &   0.665272 & 0.022288 & 0.043130 &  3.18 &    [[5577, 80], [6975, 159]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  liar &   1.000000 & 0.446642 &   0.637255 & 0.018223 & 0.035432 &  4.97 &    [[5583, 74], [7004, 130]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  liar &   1.000000 & 0.445939 &   0.659864 & 0.013597 & 0.026645 &  4.07 &     [[5607, 50], [7037, 97]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  liar &   0.997500 & 0.444140 &   0.666667 & 0.006728 & 0.013322 &  2.61 &     [[5633, 24], [7086, 48]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False).to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 2.222195863723755 seconds)\n",
      "Saved vector 0 in 5.955408573150635 seconds\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.0s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.3s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.0s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.1s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   6.5s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   6.6s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.9s\n",
      "content_count finished in 77.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.796545</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.792741</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.780287</td>\n",
       "      <td>0.764588</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466891</td>\n",
       "      <td>0.604930</td>\n",
       "      <td>0.127278</td>\n",
       "      <td>0.210307</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name split  train_acc       acc  precision    recall        f1  \\\n",
       "0  content_count   val        1.0  0.783000   0.796545  0.788973  0.792741   \n",
       "1  content_count  test        1.0  0.776000   0.780287  0.764588  0.772358   \n",
       "2  content_count  liar        1.0  0.466891   0.604930  0.127278  0.210307   \n",
       "\n",
       "    time                                              model  \n",
       "0  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "1  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "2  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500]},#[500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(grid, \"content_count\")]),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, X_liar, y_liar) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)\n",
    "# best params\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
    "    if tests == None:\n",
    "        tests = Test_statistic()\n",
    "    for file, name in files:\n",
    "        print(f\"Proccessing: {name}\")\n",
    "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "        data = pd.read_csv(file, usecols=cols_to_read)\n",
    "        print(\"Read data into dataframe\")\n",
    "\n",
    "        for col, entry_name in cols_to_test:\n",
    "            for func, model, func_name in vec_funcs:\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
    "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
    "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
    "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "importlib.reload(pp)\n",
    "\n",
    "def test_on_liar(test, file):\n",
    "    liar_data = pp.apply_pipeline_pd_tqdm(pd.read_csv(file), [(pp.Binary_labels_LIAR(), 'label', 'type_binary')])\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for row in info_list:\n",
    "        model_name = row[-1]\n",
    "        model = test.metrics[test.metrics[\"name\"] == model_name][\"model\"].values[0]\n",
    "        vectorizer = test.metrics[test.metrics[\"name\"] == model_name][\"vectorizer\"].values[0]\n",
    "        X = vectorizer.transform(liar_data[\"statement_combined\"].values)\n",
    "        #print(liar_data[\"type_binary\"].astype(int).value_counts())\n",
    "        metrics = pd.concat([mt.get_predict_metrics(model, X, liar_data[\"type_binary\"].astype(int), name=model_name), metrics])\n",
    "\n",
    "        \n",
    "    return metrics.sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/sample/dataset_unbalanced_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.12475, conspiracy: 0.112, junksci: 0.0165, hate: 0.01025, unreliable: 0.03825, bias: 0.15625, satire: 0.01725, reliable: 0.257, clickbait: 0.03075, political: 0.237\n",
      "True: 2099, Fake: 1901\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.098, junksci: 0.01, hate: 0.008, unreliable: 0.044, bias: 0.174, satire: 0.01, reliable: 0.25, clickbait: 0.044, political: 0.254\n",
      "True: 274, Fake: 226\n",
      "Distribution of test with size 500:\n",
      "fake: 0.134, conspiracy: 0.128, junksci: 0.026, hate: 0.01, unreliable: 0.056, bias: 0.134, satire: 0.012, reliable: 0.244, clickbait: 0.034, political: 0.222\n",
      "True: 250, Fake: 250\n",
      "File: ../datasets/sample/dataset_balanced_types_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 1200, Fake: 2800\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.112, junksci: 0.006, hate: 0.008, unreliable: 0.05, bias: 0.166, satire: 0.014, reliable: 0.27, clickbait: 0.026, political: 0.24\n",
      "True: 268, Fake: 232\n",
      "Distribution of test with size 500:\n",
      "fake: 0.132, conspiracy: 0.096, junksci: 0.014, hate: 0.01, unreliable: 0.044, bias: 0.158, satire: 0.01, reliable: 0.244, clickbait: 0.022, political: 0.27\n",
      "True: 268, Fake: 232\n",
      "File: ../datasets/sample/dataset_balanced_bin_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.13175, conspiracy: 0.11775, junksci: 0.017, hate: 0.0105, unreliable: 0.0405, bias: 0.165, satire: 0.0175, reliable: 0.18275, clickbait: 0.022, political: 0.16425\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.088, conspiracy: 0.088, junksci: 0.016, hate: 0.008, unreliable: 0.034, bias: 0.138, satire: 0.014, reliable: 0.22, clickbait: 0.038, political: 0.202\n",
      "True: 307, Fake: 193\n",
      "Distribution of test with size 500:\n",
      "fake: 0.122, conspiracy: 0.096, junksci: 0.016, hate: 0.008, unreliable: 0.05, bias: 0.116, satire: 0.006, reliable: 0.214, clickbait: 0.026, political: 0.202\n",
      "True: 293, Fake: 207\n",
      "File: ../datasets/sample/dataset_reliable_fake_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.5, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.5, clickbait: 0.0, political: 0.0\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.306, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.694, clickbait: 0.0, political: 0.0\n",
      "True: 347, Fake: 153\n",
      "Distribution of test with size 500:\n",
      "fake: 0.302, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.698, clickbait: 0.0, political: 0.0\n",
      "True: 349, Fake: 151\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_cleaned.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "for file in [unbalanced, balanced_types, balanced_bin, balanced_reliable_fake]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbformat": 4,
   "nbformat_minor": 2
}