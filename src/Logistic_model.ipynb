{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/large/cleaned_file.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/large/cleaned_file_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nans\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/sample/dataset_unbalanced_1M.csv\",\n",
    "    [(pp.Delete_nan(), 'content_title'),\n",
    "     (pp.Delete_nan(), 'content_domain'),\n",
    "     (pp.Delete_nan(), 'content_authors'),\n",
    "     (pp.Delete_nan(), 'content_domain_authors_title')],\n",
    "     new_file=\"../datasets/sample/dataset_unbalanced_1M_.csv\",\n",
    "     progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [00:00<00:00, 763187.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 500000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/large/cleaned_file.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.1,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/large/dataset_unbalanced_100k.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    " #                                   out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting liar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data = pd.read_csv(\"../datasets/liar_dataset/cleaned/combined_cleaned.csv\")\n",
    "X_liar =  liar_data[\"statement_combined\"].values\n",
    "y_liar = liar_data[\"label_binary\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models (other than logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 249.15490126609802 seconds)\n",
      "Saved vector 0 in 305.8764753341675 seconds\n",
      "naive_bayes finished in 2.04 seconds\n",
      "random_forest finished in 82.84 seconds\n",
      "decision_tree finished in 24.52 seconds\n",
      "ada_boost finished in 34.10 seconds\n",
      "passive_aggressive finished in 50.92 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>val</td>\n",
       "      <td>0.891746</td>\n",
       "      <td>0.845360</td>\n",
       "      <td>0.829780</td>\n",
       "      <td>0.851294</td>\n",
       "      <td>0.840400</td>\n",
       "      <td>49.88</td>\n",
       "      <td>[[43822, 8352], [7112, 40714]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>val</td>\n",
       "      <td>0.817662</td>\n",
       "      <td>0.805500</td>\n",
       "      <td>0.791838</td>\n",
       "      <td>0.804918</td>\n",
       "      <td>0.798324</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[[42054, 10120], [9330, 38496]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>val</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.653320</td>\n",
       "      <td>0.590572</td>\n",
       "      <td>0.896960</td>\n",
       "      <td>0.712213</td>\n",
       "      <td>23.05</td>\n",
       "      <td>[[22434, 29740], [4928, 42898]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>val</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.653320</td>\n",
       "      <td>0.590572</td>\n",
       "      <td>0.896960</td>\n",
       "      <td>0.712213</td>\n",
       "      <td>31.49</td>\n",
       "      <td>[[22434, 29740], [4928, 42898]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>val</td>\n",
       "      <td>0.685954</td>\n",
       "      <td>0.684550</td>\n",
       "      <td>0.756936</td>\n",
       "      <td>0.501443</td>\n",
       "      <td>0.603252</td>\n",
       "      <td>20.79</td>\n",
       "      <td>[[44473, 7701], [23844, 23982]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>test</td>\n",
       "      <td>0.891746</td>\n",
       "      <td>0.848850</td>\n",
       "      <td>0.832606</td>\n",
       "      <td>0.856451</td>\n",
       "      <td>0.844360</td>\n",
       "      <td>49.88</td>\n",
       "      <td>[[43885, 8243], [6872, 41000]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>test</td>\n",
       "      <td>0.817662</td>\n",
       "      <td>0.807900</td>\n",
       "      <td>0.793355</td>\n",
       "      <td>0.809596</td>\n",
       "      <td>0.801394</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[[42033, 10095], [9115, 38757]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>test</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.651480</td>\n",
       "      <td>0.589364</td>\n",
       "      <td>0.896850</td>\n",
       "      <td>0.711299</td>\n",
       "      <td>23.05</td>\n",
       "      <td>[[22214, 29914], [4938, 42934]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>test</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.651480</td>\n",
       "      <td>0.589364</td>\n",
       "      <td>0.896850</td>\n",
       "      <td>0.711299</td>\n",
       "      <td>31.49</td>\n",
       "      <td>[[22214, 29914], [4938, 42934]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>test</td>\n",
       "      <td>0.685954</td>\n",
       "      <td>0.688940</td>\n",
       "      <td>0.763434</td>\n",
       "      <td>0.507478</td>\n",
       "      <td>0.609682</td>\n",
       "      <td>20.79</td>\n",
       "      <td>[[44600, 7528], [23578, 24294]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.444062</td>\n",
       "      <td>0.442693</td>\n",
       "      <td>0.992752</td>\n",
       "      <td>0.612332</td>\n",
       "      <td>23.05</td>\n",
       "      <td>[[64, 7070], [41, 5616]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.652070</td>\n",
       "      <td>0.444062</td>\n",
       "      <td>0.442693</td>\n",
       "      <td>0.992752</td>\n",
       "      <td>0.612332</td>\n",
       "      <td>31.49</td>\n",
       "      <td>[[64, 7070], [41, 5616]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.685954</td>\n",
       "      <td>0.445391</td>\n",
       "      <td>0.443062</td>\n",
       "      <td>0.988333</td>\n",
       "      <td>0.611841</td>\n",
       "      <td>20.79</td>\n",
       "      <td>[[106, 7028], [66, 5591]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=5, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.891746</td>\n",
       "      <td>0.484169</td>\n",
       "      <td>0.451669</td>\n",
       "      <td>0.777267</td>\n",
       "      <td>0.571336</td>\n",
       "      <td>49.88</td>\n",
       "      <td>[[1796, 5338], [1260, 4397]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.817662</td>\n",
       "      <td>0.536002</td>\n",
       "      <td>0.462208</td>\n",
       "      <td>0.300513</td>\n",
       "      <td>0.364221</td>\n",
       "      <td>0.70</td>\n",
       "      <td>[[5156, 1978], [3957, 1700]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name split  train_acc       acc  precision    recall  \\\n",
       "0  passive_aggressive   val   0.891746  0.845360   0.829780  0.851294   \n",
       "0         naive_bayes   val   0.817662  0.805500   0.791838  0.804918   \n",
       "0       decision_tree   val   0.652070  0.653320   0.590572  0.896960   \n",
       "0           ada_boost   val   0.652070  0.653320   0.590572  0.896960   \n",
       "0       random_forest   val   0.685954  0.684550   0.756936  0.501443   \n",
       "1  passive_aggressive  test   0.891746  0.848850   0.832606  0.856451   \n",
       "1         naive_bayes  test   0.817662  0.807900   0.793355  0.809596   \n",
       "1       decision_tree  test   0.652070  0.651480   0.589364  0.896850   \n",
       "1           ada_boost  test   0.652070  0.651480   0.589364  0.896850   \n",
       "1       random_forest  test   0.685954  0.688940   0.763434  0.507478   \n",
       "2       decision_tree  liar   0.652070  0.444062   0.442693  0.992752   \n",
       "2           ada_boost  liar   0.652070  0.444062   0.442693  0.992752   \n",
       "2       random_forest  liar   0.685954  0.445391   0.443062  0.988333   \n",
       "2  passive_aggressive  liar   0.891746  0.484169   0.451669  0.777267   \n",
       "2         naive_bayes  liar   0.817662  0.536002   0.462208  0.300513   \n",
       "\n",
       "         f1   time                 confusion_matrix  \\\n",
       "0  0.840400  49.88   [[43822, 8352], [7112, 40714]]   \n",
       "0  0.798324   0.70  [[42054, 10120], [9330, 38496]]   \n",
       "0  0.712213  23.05  [[22434, 29740], [4928, 42898]]   \n",
       "0  0.712213  31.49  [[22434, 29740], [4928, 42898]]   \n",
       "0  0.603252  20.79  [[44473, 7701], [23844, 23982]]   \n",
       "1  0.844360  49.88   [[43885, 8243], [6872, 41000]]   \n",
       "1  0.801394   0.70  [[42033, 10095], [9115, 38757]]   \n",
       "1  0.711299  23.05  [[22214, 29914], [4938, 42934]]   \n",
       "1  0.711299  31.49  [[22214, 29914], [4938, 42934]]   \n",
       "1  0.609682  20.79  [[44600, 7528], [23578, 24294]]   \n",
       "2  0.612332  23.05         [[64, 7070], [41, 5616]]   \n",
       "2  0.612332  31.49         [[64, 7070], [41, 5616]]   \n",
       "2  0.611841  20.79        [[106, 7028], [66, 5591]]   \n",
       "2  0.571336  49.88     [[1796, 5338], [1260, 4397]]   \n",
       "2  0.364221   0.70     [[5156, 1978], [3957, 1700]]   \n",
       "\n",
       "                                               model  \n",
       "0                      PassiveAggressiveClassifier()  \n",
       "0                                    MultinomialNB()  \n",
       "0                DecisionTreeClassifier(max_depth=2)  \n",
       "0  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "0  (DecisionTreeClassifier(max_depth=5, max_featu...  \n",
       "1                      PassiveAggressiveClassifier()  \n",
       "1                                    MultinomialNB()  \n",
       "1                DecisionTreeClassifier(max_depth=2)  \n",
       "1  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "1  (DecisionTreeClassifier(max_depth=5, max_featu...  \n",
       "2                DecisionTreeClassifier(max_depth=2)  \n",
       "2  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "2  (DecisionTreeClassifier(max_depth=5, max_featu...  \n",
       "2                      PassiveAggressiveClassifier()  \n",
       "2                                    MultinomialNB()  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info_list = [(\n",
    "    \"../datasets/large/dataset_unbalanced_1M.csv\", \"content_no_swords_combined\", mt.create_count_vector, [\n",
    "        (MultinomialNB(), \"naive_bayes\"),\n",
    "        (RandomForestClassifier(max_depth=5), \"random_forest\"), #25\n",
    "        (DecisionTreeClassifier(max_depth=2), \"decision_tree\"),\n",
    "        (AdaBoostClassifier(n_estimators=2), \"ada_boost\"), #2\n",
    "        #(SVC(kernel='linear', max_iter=10), \"svm\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=2, algorithm='kd_tree'), \"knn\"), #15\n",
    "        (PassiveAggressiveClassifier(), \"passive_aggressive\")\n",
    "        ])\n",
    "]\n",
    "\n",
    "test_stats_base = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, tests=test_stats_base)\n",
    "test_stats_base.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svm finished in 527.08 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm</td>\n",
       "      <td>val</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.478810</td>\n",
       "      <td>0.478504</td>\n",
       "      <td>0.999080</td>\n",
       "      <td>0.647089</td>\n",
       "      <td>251.51</td>\n",
       "      <td>[[99, 52075], [44, 47782]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svm</td>\n",
       "      <td>test</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.479440</td>\n",
       "      <td>0.479048</td>\n",
       "      <td>0.999164</td>\n",
       "      <td>0.647604</td>\n",
       "      <td>251.51</td>\n",
       "      <td>[[112, 52016], [40, 47832]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.477913</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>0.442264</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.613291</td>\n",
       "      <td>251.51</td>\n",
       "      <td>[[0, 7134], [0, 5657]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=100)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name split  train_acc       acc  precision    recall        f1    time  \\\n",
       "0  svm   val   0.477913  0.478810   0.478504  0.999080  0.647089  251.51   \n",
       "1  svm  test   0.477913  0.479440   0.479048  0.999164  0.647604  251.51   \n",
       "2  svm  liar   0.477913  0.442264   0.442264  1.000000  0.613291  251.51   \n",
       "\n",
       "              confusion_matrix                               model  \n",
       "0   [[99, 52075], [44, 47782]]  SVC(kernel='linear', max_iter=100)  \n",
       "1  [[112, 52016], [40, 47832]]  SVC(kernel='linear', max_iter=100)  \n",
       "2       [[0, 7134], [0, 5657]]  SVC(kernel='linear', max_iter=100)  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info_list = [(\n",
    "    \"../datasets/large/dataset_unbalanced_1M.csv\", \"content_no_swords_combined\", mt.create_count_vector, [\n",
    "        #(MultinomialNB(), \"naive_bayes\"),\n",
    "        #(RandomForestClassifier(max_depth=5), \"random_forest\"),  # 25\n",
    "        #(DecisionTreeClassifier(max_depth=2), \"decision_tree\"),\n",
    "        #(AdaBoostClassifier(n_estimators=2), \"ada_boost\"),  # 2\n",
    "        (SVC(kernel='linear', max_iter=100), \"svm\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=2, algorithm='kd_tree'), \"knn\"), #15\n",
    "        #(PassiveAggressiveClassifier(), \"passive_aggressive\")\n",
    "    ])\n",
    "]\n",
    "\n",
    "test_stats_base_svm = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, tests=test_stats_base_svm)\n",
    "test_stats_base_svm.metrics.sort_values(by=[\"split\", \"f1\"], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 157.5944595336914 seconds)\n",
      "Saved vector 0 in 321.01699686050415 seconds\n",
      "content_count finished in 198.35 seconds\n",
      "content_count_hyper finished in 191.77 seconds\n",
      "content_count_hyper finished in 198.20 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.897497</td>\n",
       "      <td>0.873170</td>\n",
       "      <td>0.892116</td>\n",
       "      <td>0.860238</td>\n",
       "      <td>0.875887</td>\n",
       "      <td>190.70</td>\n",
       "      <td>[[42564, 5412], [7271, 44753]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.900826</td>\n",
       "      <td>0.871920</td>\n",
       "      <td>0.890256</td>\n",
       "      <td>0.859795</td>\n",
       "      <td>0.874760</td>\n",
       "      <td>197.26</td>\n",
       "      <td>[[42462, 5514], [7294, 44730]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.900057</td>\n",
       "      <td>0.870970</td>\n",
       "      <td>0.889256</td>\n",
       "      <td>0.858950</td>\n",
       "      <td>0.873840</td>\n",
       "      <td>196.89</td>\n",
       "      <td>[[42411, 5565], [7338, 44686]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.897497</td>\n",
       "      <td>0.871370</td>\n",
       "      <td>0.890553</td>\n",
       "      <td>0.860517</td>\n",
       "      <td>0.875278</td>\n",
       "      <td>190.70</td>\n",
       "      <td>[[42002, 5547], [7316, 45135]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.900826</td>\n",
       "      <td>0.870100</td>\n",
       "      <td>0.887915</td>\n",
       "      <td>0.861032</td>\n",
       "      <td>0.874267</td>\n",
       "      <td>197.26</td>\n",
       "      <td>[[41848, 5701], [7289, 45162]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.900057</td>\n",
       "      <td>0.869520</td>\n",
       "      <td>0.887634</td>\n",
       "      <td>0.860117</td>\n",
       "      <td>0.873659</td>\n",
       "      <td>196.89</td>\n",
       "      <td>[[41838, 5711], [7337, 45114]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.900057</td>\n",
       "      <td>0.520053</td>\n",
       "      <td>0.413805</td>\n",
       "      <td>0.204525</td>\n",
       "      <td>0.273749</td>\n",
       "      <td>196.89</td>\n",
       "      <td>[[5495, 1639], [4500, 1157]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.900826</td>\n",
       "      <td>0.523024</td>\n",
       "      <td>0.411058</td>\n",
       "      <td>0.181368</td>\n",
       "      <td>0.251686</td>\n",
       "      <td>197.26</td>\n",
       "      <td>[[5664, 1470], [4631, 1026]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.897497</td>\n",
       "      <td>0.527168</td>\n",
       "      <td>0.415331</td>\n",
       "      <td>0.169524</td>\n",
       "      <td>0.240773</td>\n",
       "      <td>190.70</td>\n",
       "      <td>[[5784, 1350], [4698, 959]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  name split  train_acc       acc  precision    recall  \\\n",
       "0  content_count_hyper   val   0.897497  0.873170   0.892116  0.860238   \n",
       "0        content_count   val   0.900826  0.871920   0.890256  0.859795   \n",
       "0  content_count_hyper   val   0.900057  0.870970   0.889256  0.858950   \n",
       "1  content_count_hyper  test   0.897497  0.871370   0.890553  0.860517   \n",
       "1        content_count  test   0.900826  0.870100   0.887915  0.861032   \n",
       "1  content_count_hyper  test   0.900057  0.869520   0.887634  0.860117   \n",
       "2  content_count_hyper  liar   0.900057  0.520053   0.413805  0.204525   \n",
       "2        content_count  liar   0.900826  0.523024   0.411058  0.181368   \n",
       "2  content_count_hyper  liar   0.897497  0.527168   0.415331  0.169524   \n",
       "\n",
       "         f1    time                confusion_matrix  \\\n",
       "0  0.875887  190.70  [[42564, 5412], [7271, 44753]]   \n",
       "0  0.874760  197.26  [[42462, 5514], [7294, 44730]]   \n",
       "0  0.873840  196.89  [[42411, 5565], [7338, 44686]]   \n",
       "1  0.875278  190.70  [[42002, 5547], [7316, 45135]]   \n",
       "1  0.874267  197.26  [[41848, 5701], [7289, 45162]]   \n",
       "1  0.873659  196.89  [[41838, 5711], [7337, 45114]]   \n",
       "2  0.273749  196.89    [[5495, 1639], [4500, 1157]]   \n",
       "2  0.251686  197.26    [[5664, 1470], [4631, 1026]]   \n",
       "2  0.240773  190.70     [[5784, 1350], [4698, 959]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FINISHED\n",
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/large/dataset_unbalanced_1M.csv\"\n",
    "#balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "#balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "#balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count\"), \n",
    "                                                              (LogisticRegression(max_iter=300, C=0.1), \"content_count_hyper\"),\n",
    "                                                              (LogisticRegression(max_iter=300, C=250), \"content_count_hyper\")]),\n",
    "    #(balanced_types, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_types\")]),\n",
    "    #(balanced_bin, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_bin\")]),\n",
    "    #(balanced_reliable_fake, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_reliable_fake\")]),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 155.73694348335266 seconds)\n",
      "Saved vector 0 in 318.0987198352814 seconds\n",
      "Creating vector 1 (data read in 99.3843047618866 seconds)\n",
      "Saved vector 1 in 1135.6230821609497 seconds\n",
      "content_tfidf_bi finished in 198.51 seconds\n",
      "content_tfidf_bi_hyper_1 finished in 200.57 seconds\n",
      "content_tfidf_bi_hyper_2 finished in 59.96 seconds\n",
      "content_tfidf_bi finished in 1939.47 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m test_stats_tdidf_bitri \u001b[39m=\u001b[39m mt\u001b[39m.\u001b[39mTest_statistic()\n\u001b[0;32m     20\u001b[0m mt\u001b[39m.\u001b[39mcreate_vectors_from_infolist(\u001b[39m\"\u001b[39m\u001b[39m../datasets/sample/dataset_tdidf_vectors.pickle\u001b[39m\u001b[39m\"\u001b[39m, info_list, X_liar, y_liar)\n\u001b[1;32m---> 21\u001b[0m mt\u001b[39m.\u001b[39;49mtest_vectors_from_infolist(\u001b[39m\"\u001b[39;49m\u001b[39m../datasets/sample/dataset_tdidf_vectors.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m, info_list, tests\u001b[39m=\u001b[39;49mtest_stats_tdidf_bitri)\n\u001b[0;32m     22\u001b[0m test_stats_tdidf_bitri\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msplit\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m], ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32md:\\projects\\FakeNews\\src\\model_tests.py:281\u001b[0m, in \u001b[0;36mtest_vectors_from_infolist\u001b[1;34m(from_file, info_list, tests, use_standard)\u001b[0m\n\u001b[0;32m    278\u001b[0m         X_train, X_val, X_test, X_liar \u001b[39m=\u001b[39m (pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f))\n\u001b[0;32m    280\u001b[0m         \u001b[39mfor\u001b[39;00m model, name \u001b[39min\u001b[39;00m models:\n\u001b[1;32m--> 281\u001b[0m             tests\u001b[39m.\u001b[39;49mtest_baseline(model, X_train, y_train, [\n\u001b[0;32m    282\u001b[0m                 (X_val, y_val, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mval\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    283\u001b[0m                 (X_test, y_test, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[0;32m    284\u001b[0m                 (X_liar, y_liar, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mliar\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m    285\u001b[0m             ], name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m tests\n",
      "File \u001b[1;32md:\\projects\\FakeNews\\src\\model_tests.py:226\u001b[0m, in \u001b[0;36mTest_statistic.test_baseline\u001b[1;34m(self, model, X_train, y_train, predict_pairs, name)\u001b[0m\n\u001b[0;32m    225\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_baseline\u001b[39m(\u001b[39mself\u001b[39m, model, X_train, y_train, predict_pairs, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 226\u001b[0m     metric \u001b[39m=\u001b[39m try_models([model], X_train, y_train, predict_pairs, name)\n\u001b[0;32m    227\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics, metric])\n",
      "File \u001b[1;32md:\\projects\\FakeNews\\src\\model_tests.py:169\u001b[0m, in \u001b[0;36mtry_models\u001b[1;34m(models, X_train, y_train, predict_pairs, name)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m    168\u001b[0m     start_time \u001b[39m=\u001b[39m time() \n\u001b[1;32m--> 169\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    170\u001b[0m     train_time \u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    171\u001b[0m     y_train_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1291\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1292\u001b[0m     path_func(\n\u001b[0;32m   1293\u001b[0m         X,\n\u001b[0;32m   1294\u001b[0m         y,\n\u001b[0;32m   1295\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1296\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1297\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1298\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1299\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1300\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1301\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1302\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1303\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1304\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1305\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1306\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1307\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1308\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1309\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1310\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1311\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1312\u001b[0m     )\n\u001b[0;32m   1313\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1314\u001b[0m )\n\u001b[0;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    446\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    447\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    448\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[0;32m    449\u001b[0m ]\n\u001b[1;32m--> 450\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    451\u001b[0m     func,\n\u001b[0;32m    452\u001b[0m     w0,\n\u001b[0;32m    453\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    454\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    455\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    456\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[0;32m    457\u001b[0m )\n\u001b[0;32m    458\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    459\u001b[0m     solver,\n\u001b[0;32m    460\u001b[0m     opt_res,\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    464\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:274\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    271\u001b[0m n_dof \u001b[39m=\u001b[39m n_features \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m raw_prediction \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     weights, intercept, raw_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_intercept_raw(coef, X)\n\u001b[0;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     weights, intercept \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_intercept(coef)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:162\u001b[0m, in \u001b[0;36mLinearModelLoss.weight_intercept_raw\u001b[1;34m(self, coef, X)\u001b[0m\n\u001b[0;32m    159\u001b[0m weights, intercept \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mis_multiclass:\n\u001b[1;32m--> 162\u001b[0m     raw_prediction \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39;49m weights \u001b[39m+\u001b[39m intercept\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# weights has shape (n_classes, n_dof)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     raw_prediction \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39m weights\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m intercept  \u001b[39m# ndarray, likely C-contiguous\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\sparse\\_base.py:528\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    526\u001b[0m     \u001b[39m# Fast path for the most common case\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N,):\n\u001b[1;32m--> 528\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_vector(other)\n\u001b[0;32m    529\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[0;32m    530\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other\u001b[39m.\u001b[39mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\henri\\miniconda3\\envs\\fake-news\\lib\\site-packages\\scipy\\sparse\\_compressed.py:489\u001b[0m, in \u001b[0;36m_cs_matrix._mul_vector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39m# csr_matvec or csc_matvec\u001b[39;00m\n\u001b[0;32m    488\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvec\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 489\u001b[0m fn(M, N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other, result)\n\u001b[0;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "info_list = [\n",
    "      (\"../datasets/large/dataset_unbalanced_1M.csv\", \"content_combined\", mt.create_tdfidf_vector_unigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_uni\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_uni_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_uni_hyper_2\")]),\n",
    "     (\"../datasets/large/dataset_unbalanced_1M.csv\", \"content_combined\", mt.create_tdfidf_vector_bigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_bi\")]),\n",
    " #    (\"../datasets/sample/dataset_unbalanced_10K.csv\", \"content_combined\", mt.create_tdfidf_vector_trigram, [\n",
    " #       (LogisticRegression(max_iter=300), \"content_tfidf_tri\"),\n",
    " #       (LogisticRegression(max_iter=300, C=250), \"content_tfidf_tri_hyper_1\"),\n",
    "  #      (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_tdidf = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, tests=test_stats_tdidf)\n",
    "test_stats_tdidf.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>val</td>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.904830</td>\n",
       "      <td>0.902536</td>\n",
       "      <td>0.915981</td>\n",
       "      <td>0.909209</td>\n",
       "      <td>1936.20</td>\n",
       "      <td>[[42830, 5146], [4371, 47653]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>0.932771</td>\n",
       "      <td>0.879950</td>\n",
       "      <td>0.884717</td>\n",
       "      <td>0.884496</td>\n",
       "      <td>0.884606</td>\n",
       "      <td>199.62</td>\n",
       "      <td>[[41980, 5996], [6009, 46015]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>val</td>\n",
       "      <td>0.894134</td>\n",
       "      <td>0.878860</td>\n",
       "      <td>0.881578</td>\n",
       "      <td>0.886187</td>\n",
       "      <td>0.883877</td>\n",
       "      <td>197.55</td>\n",
       "      <td>[[41783, 6193], [5921, 46103]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.867195</td>\n",
       "      <td>0.863610</td>\n",
       "      <td>0.867440</td>\n",
       "      <td>0.870925</td>\n",
       "      <td>0.869179</td>\n",
       "      <td>59.07</td>\n",
       "      <td>[[41052, 6924], [6715, 45309]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>test</td>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.903780</td>\n",
       "      <td>0.902702</td>\n",
       "      <td>0.915197</td>\n",
       "      <td>0.908907</td>\n",
       "      <td>1936.20</td>\n",
       "      <td>[[42375, 5174], [4448, 48003]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>test</td>\n",
       "      <td>0.894134</td>\n",
       "      <td>0.878910</td>\n",
       "      <td>0.882084</td>\n",
       "      <td>0.887819</td>\n",
       "      <td>0.884942</td>\n",
       "      <td>197.55</td>\n",
       "      <td>[[41324, 6225], [5884, 46567]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>0.932771</td>\n",
       "      <td>0.879180</td>\n",
       "      <td>0.884137</td>\n",
       "      <td>0.885722</td>\n",
       "      <td>0.884929</td>\n",
       "      <td>199.62</td>\n",
       "      <td>[[41461, 6088], [5994, 46457]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.867195</td>\n",
       "      <td>0.861890</td>\n",
       "      <td>0.866353</td>\n",
       "      <td>0.871061</td>\n",
       "      <td>0.868701</td>\n",
       "      <td>59.07</td>\n",
       "      <td>[[40501, 7048], [6763, 45688]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.945090</td>\n",
       "      <td>0.443124</td>\n",
       "      <td>0.411259</td>\n",
       "      <td>0.600495</td>\n",
       "      <td>0.488180</td>\n",
       "      <td>1936.20</td>\n",
       "      <td>[[2271, 4863], [2260, 3397]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.867195</td>\n",
       "      <td>0.459620</td>\n",
       "      <td>0.418070</td>\n",
       "      <td>0.566024</td>\n",
       "      <td>0.480925</td>\n",
       "      <td>59.07</td>\n",
       "      <td>[[2677, 4457], [2455, 3202]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.894134</td>\n",
       "      <td>0.457978</td>\n",
       "      <td>0.415787</td>\n",
       "      <td>0.556832</td>\n",
       "      <td>0.476083</td>\n",
       "      <td>197.55</td>\n",
       "      <td>[[2708, 4426], [2507, 3150]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_bi_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.932771</td>\n",
       "      <td>0.462669</td>\n",
       "      <td>0.415344</td>\n",
       "      <td>0.527311</td>\n",
       "      <td>0.464678</td>\n",
       "      <td>199.62</td>\n",
       "      <td>[[2935, 4199], [2674, 2983]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       name split  train_acc       acc  precision    recall  \\\n",
       "0          content_tfidf_bi   val   0.945090  0.904830   0.902536  0.915981   \n",
       "0  content_tfidf_bi_hyper_1   val   0.932771  0.879950   0.884717  0.884496   \n",
       "0          content_tfidf_bi   val   0.894134  0.878860   0.881578  0.886187   \n",
       "0  content_tfidf_bi_hyper_2   val   0.867195  0.863610   0.867440  0.870925   \n",
       "1          content_tfidf_bi  test   0.945090  0.903780   0.902702  0.915197   \n",
       "1          content_tfidf_bi  test   0.894134  0.878910   0.882084  0.887819   \n",
       "1  content_tfidf_bi_hyper_1  test   0.932771  0.879180   0.884137  0.885722   \n",
       "1  content_tfidf_bi_hyper_2  test   0.867195  0.861890   0.866353  0.871061   \n",
       "2          content_tfidf_bi  liar   0.945090  0.443124   0.411259  0.600495   \n",
       "2  content_tfidf_bi_hyper_2  liar   0.867195  0.459620   0.418070  0.566024   \n",
       "2          content_tfidf_bi  liar   0.894134  0.457978   0.415787  0.556832   \n",
       "2  content_tfidf_bi_hyper_1  liar   0.932771  0.462669   0.415344  0.527311   \n",
       "\n",
       "         f1     time                confusion_matrix  \\\n",
       "0  0.909209  1936.20  [[42830, 5146], [4371, 47653]]   \n",
       "0  0.884606   199.62  [[41980, 5996], [6009, 46015]]   \n",
       "0  0.883877   197.55  [[41783, 6193], [5921, 46103]]   \n",
       "0  0.869179    59.07  [[41052, 6924], [6715, 45309]]   \n",
       "1  0.908907  1936.20  [[42375, 5174], [4448, 48003]]   \n",
       "1  0.884942   197.55  [[41324, 6225], [5884, 46567]]   \n",
       "1  0.884929   199.62  [[41461, 6088], [5994, 46457]]   \n",
       "1  0.868701    59.07  [[40501, 7048], [6763, 45688]]   \n",
       "2  0.488180  1936.20    [[2271, 4863], [2260, 3397]]   \n",
       "2  0.480925    59.07    [[2677, 4457], [2455, 3202]]   \n",
       "2  0.476083   197.55    [[2708, 4426], [2507, 3150]]   \n",
       "2  0.464678   199.62    [[2935, 4199], [2674, 2983]]   \n",
       "\n",
       "                                     model  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats_tdidf_bitri.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "info_list = [\n",
    "\n",
    "    (\"../datasets/large/dataset_unbalanced_100k.csv\", \"content_no_swords_combined\", mt.create_tdfidf_vector_trigram, [\n",
    "       (LogisticRegression(max_iter=300), \"content_tfidf_tri\"),\n",
    "       (LogisticRegression(max_iter=300, C=250), \"content_tfidf_tri_hyper_1\"),\n",
    "       (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_tdidf_tri = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_tri.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_tri.pickle\", info_list, tests=test_stats_tdidf_tri)\n",
    "test_stats_tdidf_tri.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>val</td>\n",
       "      <td>0.9526</td>\n",
       "      <td>0.870480</td>\n",
       "      <td>0.872862</td>\n",
       "      <td>0.851522</td>\n",
       "      <td>0.862060</td>\n",
       "      <td>469.50</td>\n",
       "      <td>[[46576, 5895], [7057, 40472]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>test</td>\n",
       "      <td>0.9526</td>\n",
       "      <td>0.868360</td>\n",
       "      <td>0.871823</td>\n",
       "      <td>0.849733</td>\n",
       "      <td>0.860636</td>\n",
       "      <td>469.50</td>\n",
       "      <td>[[46189, 5976], [7188, 40647]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_tfidf_tri</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.9526</td>\n",
       "      <td>0.547025</td>\n",
       "      <td>0.483984</td>\n",
       "      <td>0.365918</td>\n",
       "      <td>0.416751</td>\n",
       "      <td>469.50</td>\n",
       "      <td>[[4927, 2207], [3587, 2070]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name split  train_acc       acc  precision    recall  \\\n",
       "0  content_tfidf_tri   val     0.9526  0.870480   0.872862  0.851522   \n",
       "1  content_tfidf_tri  test     0.9526  0.868360   0.871823  0.849733   \n",
       "2  content_tfidf_tri  liar     0.9526  0.547025   0.483984  0.365918   \n",
       "\n",
       "         f1    time                confusion_matrix  \\\n",
       "0  0.862060  469.50  [[46576, 5895], [7057, 40472]]   \n",
       "1  0.860636  469.50  [[46189, 5976], [7188, 40647]]   \n",
       "2  0.416751  469.50    [[4927, 2207], [3587, 2070]]   \n",
       "\n",
       "                              model  \n",
       "0  LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(max_iter=300)  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats_tdidf_tri.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_title_count\")]),\n",
    "    (unbalanced, \"content_domain\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_domain_count\")]),\n",
    "    (unbalanced, \"content_authors\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_authors_count\")]),\n",
    "    (unbalanced, \"content_domain_authors_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"all_count\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=250), \"all_count_hyper_1\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=0.1), \"all_count_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_meta = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, tests=test_stats_meta)\n",
    "test_stats_meta.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"content_count_hyper\"\n",
    "\n",
    "metrics = test_stats_simple.metrics\n",
    "metrics_test = metrics[metrics[\"split\"] == \"test\"]\n",
    "cm = metrics_test[metrics_test[\"name\"] == model_name][\"confusion_matrix\"].values[0]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sns.heatmap([[tp, fn],[fp, tn]], annot=True, cmap=\"Blues\", xticklabels=[\"Fake\",\"True\"], yticklabels=[\"Fake\",\"True\"], fmt=\"d\", annot_kws={\"size\": 12})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False).to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500]},#[500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(grid, \"content_count\")]),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, X_liar, y_liar) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)\n",
    "# best params\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
    "    if tests == None:\n",
    "        tests = Test_statistic()\n",
    "    for file, name in files:\n",
    "        print(f\"Proccessing: {name}\")\n",
    "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "        data = pd.read_csv(file, usecols=cols_to_read)\n",
    "        print(\"Read data into dataframe\")\n",
    "\n",
    "        for col, entry_name in cols_to_test:\n",
    "            for func, model, func_name in vec_funcs:\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
    "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
    "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
    "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "importlib.reload(pp)\n",
    "\n",
    "def test_on_liar(test, file):\n",
    "    liar_data = pp.apply_pipeline_pd_tqdm(pd.read_csv(file), [(pp.Binary_labels_LIAR(), 'label', 'type_binary')])\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for row in info_list:\n",
    "        model_name = row[-1]\n",
    "        model = test.metrics[test.metrics[\"name\"] == model_name][\"model\"].values[0]\n",
    "        vectorizer = test.metrics[test.metrics[\"name\"] == model_name][\"vectorizer\"].values[0]\n",
    "        X = vectorizer.transform(liar_data[\"statement_combined\"].values)\n",
    "        #print(liar_data[\"type_binary\"].astype(int).value_counts())\n",
    "        metrics = pd.concat([mt.get_predict_metrics(model, X, liar_data[\"type_binary\"].astype(int), name=model_name), metrics])\n",
    "\n",
    "        \n",
    "    return metrics.sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_cleaned.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "for file in [unbalanced, balanced_types, balanced_bin, balanced_reliable_fake]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
