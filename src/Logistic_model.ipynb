{
   "cells": [
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Packages"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "ename": "",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31mFailed to start the Kernel. \n",
                  "\u001b[1;31mKernel fakenews (Python 3.11.0) is not usable. Check the Jupyter output tab for more information. \n",
                  "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
               ]
            }
         ],
         "source": [
            "import pandas as pd\n",
            "import matplotlib.pyplot as plt\n",
            "from ast import literal_eval\n",
            "import numpy as np\n",
            "import nltk\n",
            "from nltk.corpus import stopwords\n",
            "nltk.download('stopwords')\n",
            "from time import time\n",
            "\n",
            "from sklearn.feature_extraction.text import TfidfVectorizer\n",
            "from sklearn.feature_extraction.text import CountVectorizer\n",
            "from sklearn.feature_extraction.text import TfidfTransformer\n",
            "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
            "from sklearn.neighbors import KNeighborsClassifier\n",
            "from sklearn.model_selection import train_test_split\n",
            "from sklearn.svm import SVC\n",
            "from sklearn.neural_network import MLPClassifier\n",
            "from sklearn.model_selection import GridSearchCV\n",
            "from sklearn.linear_model import PassiveAggressiveClassifier\n",
            "from sklearn.preprocessing import normalize\n",
            "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
            "from sklearn.linear_model import LogisticRegression\n",
            "from sklearn.naive_bayes import MultinomialNB\n",
            "from sklearn.ensemble import RandomForestClassifier\n",
            "from sklearn.tree import DecisionTreeClassifier\n",
            "from sklearn.ensemble import GradientBoostingClassifier\n",
            "from sklearn.ensemble import AdaBoostClassifier\n",
            "\n",
            "import transformers as ppb # pytorch-pretrained-bert\n",
            "import torch\n",
            "\n",
            "import pipeline as pp\n",
            "import models as ml\n",
            "\n",
            "import importlib\n",
            "import math\n",
            "import pickle"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Preproccessing"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Covert types to binary labels - either True (reliable) or False (fake news)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "ename": "",
               "evalue": "",
               "output_type": "error",
               "traceback": [
                  "\u001b[1;31mFailed to start the Kernel. \n",
                  "\u001b[1;31mKernel fakenews (Python 3.11.0) is not usable. Check the Jupyter output tab for more information. \n",
                  "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
               ]
            }
         ],
         "source": [
            "pp.apply_pipeline(\n",
            "    \"../datasets/big/dataset.csv\", \n",
            "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
            "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
            "    progress_bar=True\n",
            ")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Create the follwoing input files:\n",
            "* All are unbalanced\n",
            "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
            "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [],
         "source": [
            "# The number of rows to train the model\n",
            "BATCH_SIZE = 5000"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "C:\\Users\\madsv\\AppData\\Local\\Temp\\ipykernel_20684\\331561095.py:1: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  data = pd.read_csv(\"../datasets/big/news_sample_cleaned_num_100k.csv\")\n"
               ]
            }
         ],
         "source": [
            "data = pd.read_csv(\"../datasets/big/news_sample_cleaned_num_100k.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>id</th>\n",
                     "      <th>domain</th>\n",
                     "      <th>type</th>\n",
                     "      <th>url</th>\n",
                     "      <th>content</th>\n",
                     "      <th>scraped_at</th>\n",
                     "      <th>inserted_at</th>\n",
                     "      <th>updated_at</th>\n",
                     "      <th>title</th>\n",
                     "      <th>authors</th>\n",
                     "      <th>keywords</th>\n",
                     "      <th>meta_keywords</th>\n",
                     "      <th>meta_description</th>\n",
                     "      <th>tags</th>\n",
                     "      <th>summary</th>\n",
                     "      <th>source</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>6301449</td>\n",
                     "      <td>nytimes.com</td>\n",
                     "      <td>reliable</td>\n",
                     "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
                     "      <td>['editor', 'drop', 'colleg', 'editori', 'march...</td>\n",
                     "      <td>2018-02-11 00:40:10.316783</td>\n",
                     "      <td>2018-02-11 00:14:20.346838</td>\n",
                     "      <td>2018-02-11 00:14:20.346871</td>\n",
                     "      <td>Time to Scrap the Electoral College?</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['Presidential Elections (US)', 'Electoral Col...</td>\n",
                     "      <td>&lt;br&gt;To the Editor:\\n&lt;p&gt;\\n  Re ''Drop Out of th...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>nytimes</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>3676064</td>\n",
                     "      <td>nationalreview.com</td>\n",
                     "      <td>political</td>\n",
                     "      <td>http://www.nationalreview.com/postmodern-conse...</td>\n",
                     "      <td>['ive', 'written', 'anoth', 'channel', 'advic'...</td>\n",
                     "      <td>2017-11-27T01:14:42.983556</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Antonin Scalia, Donald Trump, Tyler Cowen &amp; Co...</td>\n",
                     "      <td>Peter Augustine Lawler</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['Peter Augustine Lawler']</td>\n",
                     "      <td>Senate Republicans should enter into the battl...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>5192554</td>\n",
                     "      <td>infowars.com</td>\n",
                     "      <td>conspiracy</td>\n",
                     "      <td>https://www.infowars.com/soldiers-nearly-kille...</td>\n",
                     "      <td>['david', 'gutierrez', 'natur', 'news', 'novem...</td>\n",
                     "      <td>2017-12-09T22:10:08.302997</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Soldiers Nearly Killed with Military’s Bioterr...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>1136409</td>\n",
                     "      <td>redstate.com</td>\n",
                     "      <td>political</td>\n",
                     "      <td>https://www.redstate.com/diary/Erick/2010/10/2...</td>\n",
                     "      <td>['colorado', 'close', 'call', 'right', 'union'...</td>\n",
                     "      <td>2017-11-10T11:18:44.524042</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Colorado Should Not Be Too Close To Call</td>\n",
                     "      <td>Erick Erickson, Redstate Insider, Susan Wright...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>Colorado Should Not Be Too Close To Call</td>\n",
                     "      <td>Michael Bennet, Colorado, ken buck</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>62174</td>\n",
                     "      <td>beforeitsnews.com</td>\n",
                     "      <td>fake</td>\n",
                     "      <td>http://beforeitsnews.com/survival/2015/06/lett...</td>\n",
                     "      <td>['letter', 'number', 'beast', 'area', 'code', ...</td>\n",
                     "      <td>2018-01-25 20:13:50.426130</td>\n",
                     "      <td>2018-02-02 01:19:41.756632</td>\n",
                     "      <td>2018-02-02 01:19:41.756664</td>\n",
                     "      <td>Letter Re: Does the Number of the Beast Have a...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>99998</th>\n",
                     "      <td>7240944</td>\n",
                     "      <td>nytimes.com</td>\n",
                     "      <td>reliable</td>\n",
                     "      <td>https://www.nytimes.com/2017/06/23/realestate/...</td>\n",
                     "      <td>['ye', 'ye', 'four', 'year', 'ago', 'children'...</td>\n",
                     "      <td>2018-02-11 00:49:30.951722</td>\n",
                     "      <td>2018-02-11 00:14:20.346838</td>\n",
                     "      <td>2018-02-11 00:14:20.346871</td>\n",
                     "      <td>A Brooklyn-Born Actress Rediscovers New York</td>\n",
                     "      <td>Joanne Kaufman, What I Love</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['Real Estate and Housing (Residential)', 'Int...</td>\n",
                     "      <td>After giving up Manhattan to raise a family in...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>nytimes</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>99999</th>\n",
                     "      <td>2741248</td>\n",
                     "      <td>lifenews.com</td>\n",
                     "      <td>bias</td>\n",
                     "      <td>https://consciouslifenews.com/category/conscio...</td>\n",
                     "      <td>['peopl', 'learn', 'rope', 'law', 'attract', '...</td>\n",
                     "      <td>2017-11-27T01:14:21.395055</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Happiness &amp; Humor Archives</td>\n",
                     "      <td>Tamara Rant, Andrea Schulman, Vicki Howie</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>100000</th>\n",
                     "      <td>6540290</td>\n",
                     "      <td>nytimes.com</td>\n",
                     "      <td>reliable</td>\n",
                     "      <td>https://www.nytimes.com/2008/05/20/business/20...</td>\n",
                     "      <td>['low', 'compani', 'report', '&lt;num&gt;', 'percent...</td>\n",
                     "      <td>2018-02-11 00:42:31.282156</td>\n",
                     "      <td>2018-02-11 00:14:20.346838</td>\n",
                     "      <td>2018-02-11 00:14:20.346871</td>\n",
                     "      <td>Lowe’s, Hurt by the Slump in Housing and the E...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>[\"Lowe's Companies\", 'Company Reports', 'Home ...</td>\n",
                     "      <td>Lowe’s, the second-largest home improvement ch...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>nytimes</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>100001</th>\n",
                     "      <td>1405351</td>\n",
                     "      <td>abovetopsecret.com</td>\n",
                     "      <td>conspiracy</td>\n",
                     "      <td>http://www.abovetopsecret.com/forum/thread5780...</td>\n",
                     "      <td>['investig', 'intens', 'infrar', 'emiss', 'tar...</td>\n",
                     "      <td>2017-11-10T11:18:44.524042</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Oakland Ca. 05-28-10, page 4</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>100002</th>\n",
                     "      <td>4518336</td>\n",
                     "      <td>beforeitsnews.com</td>\n",
                     "      <td>fake</td>\n",
                     "      <td>http://beforeitsnews.com/earthquakes/2012/01/s...</td>\n",
                     "      <td>['strong', 'coastal', 'earthquak', 'near', 'gu...</td>\n",
                     "      <td>2017-11-27T01:14:08.7454</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Strong coastal earthquake near Guiuan, Samar, ...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>100003 rows × 16 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "             id              domain        type  \\\n",
                     "0       6301449         nytimes.com    reliable   \n",
                     "1       3676064  nationalreview.com   political   \n",
                     "2       5192554        infowars.com  conspiracy   \n",
                     "3       1136409        redstate.com   political   \n",
                     "4         62174   beforeitsnews.com        fake   \n",
                     "...         ...                 ...         ...   \n",
                     "99998   7240944         nytimes.com    reliable   \n",
                     "99999   2741248        lifenews.com        bias   \n",
                     "100000  6540290         nytimes.com    reliable   \n",
                     "100001  1405351  abovetopsecret.com  conspiracy   \n",
                     "100002  4518336   beforeitsnews.com        fake   \n",
                     "\n",
                     "                                                      url  \\\n",
                     "0       https://query.nytimes.com/gst/fullpage.html?re...   \n",
                     "1       http://www.nationalreview.com/postmodern-conse...   \n",
                     "2       https://www.infowars.com/soldiers-nearly-kille...   \n",
                     "3       https://www.redstate.com/diary/Erick/2010/10/2...   \n",
                     "4       http://beforeitsnews.com/survival/2015/06/lett...   \n",
                     "...                                                   ...   \n",
                     "99998   https://www.nytimes.com/2017/06/23/realestate/...   \n",
                     "99999   https://consciouslifenews.com/category/conscio...   \n",
                     "100000  https://www.nytimes.com/2008/05/20/business/20...   \n",
                     "100001  http://www.abovetopsecret.com/forum/thread5780...   \n",
                     "100002  http://beforeitsnews.com/earthquakes/2012/01/s...   \n",
                     "\n",
                     "                                                  content  \\\n",
                     "0       ['editor', 'drop', 'colleg', 'editori', 'march...   \n",
                     "1       ['ive', 'written', 'anoth', 'channel', 'advic'...   \n",
                     "2       ['david', 'gutierrez', 'natur', 'news', 'novem...   \n",
                     "3       ['colorado', 'close', 'call', 'right', 'union'...   \n",
                     "4       ['letter', 'number', 'beast', 'area', 'code', ...   \n",
                     "...                                                   ...   \n",
                     "99998   ['ye', 'ye', 'four', 'year', 'ago', 'children'...   \n",
                     "99999   ['peopl', 'learn', 'rope', 'law', 'attract', '...   \n",
                     "100000  ['low', 'compani', 'report', '<num>', 'percent...   \n",
                     "100001  ['investig', 'intens', 'infrar', 'emiss', 'tar...   \n",
                     "100002  ['strong', 'coastal', 'earthquak', 'near', 'gu...   \n",
                     "\n",
                     "                        scraped_at                 inserted_at  \\\n",
                     "0       2018-02-11 00:40:10.316783  2018-02-11 00:14:20.346838   \n",
                     "1       2017-11-27T01:14:42.983556  2018-02-08 19:18:34.468038   \n",
                     "2       2017-12-09T22:10:08.302997  2018-02-08 19:18:34.468038   \n",
                     "3       2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
                     "4       2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
                     "...                            ...                         ...   \n",
                     "99998   2018-02-11 00:49:30.951722  2018-02-11 00:14:20.346838   \n",
                     "99999   2017-11-27T01:14:21.395055  2018-02-07 23:39:33.852671   \n",
                     "100000  2018-02-11 00:42:31.282156  2018-02-11 00:14:20.346838   \n",
                     "100001  2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
                     "100002    2017-11-27T01:14:08.7454  2018-02-08 19:18:34.468038   \n",
                     "\n",
                     "                        updated_at  \\\n",
                     "0       2018-02-11 00:14:20.346871   \n",
                     "1       2018-02-08 19:18:34.468066   \n",
                     "2       2018-02-08 19:18:34.468066   \n",
                     "3       2018-02-07 23:39:33.852696   \n",
                     "4       2018-02-02 01:19:41.756664   \n",
                     "...                            ...   \n",
                     "99998   2018-02-11 00:14:20.346871   \n",
                     "99999   2018-02-07 23:39:33.852696   \n",
                     "100000  2018-02-11 00:14:20.346871   \n",
                     "100001  2018-02-07 23:39:33.852696   \n",
                     "100002  2018-02-08 19:18:34.468066   \n",
                     "\n",
                     "                                                    title  \\\n",
                     "0                    Time to Scrap the Electoral College?   \n",
                     "1       Antonin Scalia, Donald Trump, Tyler Cowen & Co...   \n",
                     "2       Soldiers Nearly Killed with Military’s Bioterr...   \n",
                     "3                Colorado Should Not Be Too Close To Call   \n",
                     "4       Letter Re: Does the Number of the Beast Have a...   \n",
                     "...                                                   ...   \n",
                     "99998        A Brooklyn-Born Actress Rediscovers New York   \n",
                     "99999                          Happiness & Humor Archives   \n",
                     "100000  Lowe’s, Hurt by the Slump in Housing and the E...   \n",
                     "100001                       Oakland Ca. 05-28-10, page 4   \n",
                     "100002  Strong coastal earthquake near Guiuan, Samar, ...   \n",
                     "\n",
                     "                                                  authors  keywords  \\\n",
                     "0                                                     NaN       NaN   \n",
                     "1                                  Peter Augustine Lawler       NaN   \n",
                     "2                                                     NaN       NaN   \n",
                     "3       Erick Erickson, Redstate Insider, Susan Wright...       NaN   \n",
                     "4                                                     NaN       NaN   \n",
                     "...                                                   ...       ...   \n",
                     "99998                         Joanne Kaufman, What I Love       NaN   \n",
                     "99999           Tamara Rant, Andrea Schulman, Vicki Howie       NaN   \n",
                     "100000                                                NaN       NaN   \n",
                     "100001                                                NaN       NaN   \n",
                     "100002                                                NaN       NaN   \n",
                     "\n",
                     "                                            meta_keywords  \\\n",
                     "0       ['Presidential Elections (US)', 'Electoral Col...   \n",
                     "1                              ['Peter Augustine Lawler']   \n",
                     "2                                                    ['']   \n",
                     "3                                                    ['']   \n",
                     "4                                                    ['']   \n",
                     "...                                                   ...   \n",
                     "99998   ['Real Estate and Housing (Residential)', 'Int...   \n",
                     "99999                                                ['']   \n",
                     "100000  [\"Lowe's Companies\", 'Company Reports', 'Home ...   \n",
                     "100001                                               ['']   \n",
                     "100002                                               ['']   \n",
                     "\n",
                     "                                         meta_description  \\\n",
                     "0       <br>To the Editor:\\n<p>\\n  Re ''Drop Out of th...   \n",
                     "1       Senate Republicans should enter into the battl...   \n",
                     "2                                                     NaN   \n",
                     "3                Colorado Should Not Be Too Close To Call   \n",
                     "4                                                     NaN   \n",
                     "...                                                   ...   \n",
                     "99998   After giving up Manhattan to raise a family in...   \n",
                     "99999                                                 NaN   \n",
                     "100000  Lowe’s, the second-largest home improvement ch...   \n",
                     "100001                                                NaN   \n",
                     "100002                                                NaN   \n",
                     "\n",
                     "                                      tags  summary   source  \n",
                     "0                                      NaN      NaN  nytimes  \n",
                     "1                                      NaN      NaN      NaN  \n",
                     "2                                      NaN      NaN      NaN  \n",
                     "3       Michael Bennet, Colorado, ken buck      NaN      NaN  \n",
                     "4                                      NaN      NaN      NaN  \n",
                     "...                                    ...      ...      ...  \n",
                     "99998                                  NaN      NaN  nytimes  \n",
                     "99999                                  NaN      NaN      NaN  \n",
                     "100000                                 NaN      NaN  nytimes  \n",
                     "100001                                 NaN      NaN      NaN  \n",
                     "100002                                 NaN      NaN      NaN  \n",
                     "\n",
                     "[100003 rows x 16 columns]"
                  ]
               },
               "execution_count": 4,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "c:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\pipeline.py:145: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
                  "  for chunk in reader:\n",
                  "100%|██████████| 100003/100003 [00:00<00:00, 799499.43it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "entries read: 200000\n"
               ]
            }
         ],
         "source": [
            "importlib.reload(pp)\n",
            "from_file = \"../datasets/big/shuffled.csv\"\n",
            "\n",
            "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
            "#                                   out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)\n",
            "\n",
            "pp.get_dataframe_with_distribution(\"../datasets/big/news_sample_cleaned_num_100k.csv\", BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
            "                                   out_file=\"../datasets/sample/dataset_unbalanced_new.csv\", get_frame=False)\n",
            "\n",
            "\n",
            "\n",
            "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.6,0.1,0.1], [True, False, False], \n",
            "#                                   out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
            "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
            "#                                   out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
            "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
            "#                                   out_file=\"../datasets/sample/dataset_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [],
         "source": [
            "l = pd.read_csv(\"../datasets/sample/dataset_unbalanced_new.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 11,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>id</th>\n",
                     "      <th>domain</th>\n",
                     "      <th>type</th>\n",
                     "      <th>url</th>\n",
                     "      <th>content</th>\n",
                     "      <th>scraped_at</th>\n",
                     "      <th>inserted_at</th>\n",
                     "      <th>updated_at</th>\n",
                     "      <th>title</th>\n",
                     "      <th>authors</th>\n",
                     "      <th>keywords</th>\n",
                     "      <th>meta_keywords</th>\n",
                     "      <th>meta_description</th>\n",
                     "      <th>tags</th>\n",
                     "      <th>summary</th>\n",
                     "      <th>source</th>\n",
                     "      <th>set</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>6301449</td>\n",
                     "      <td>nytimes.com</td>\n",
                     "      <td>reliable</td>\n",
                     "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
                     "      <td>['editor', 'drop', 'colleg', 'editori', 'march...</td>\n",
                     "      <td>2018-02-11 00:40:10.316783</td>\n",
                     "      <td>2018-02-11 00:14:20.346838</td>\n",
                     "      <td>2018-02-11 00:14:20.346871</td>\n",
                     "      <td>Time to Scrap the Electoral College?</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['Presidential Elections (US)', 'Electoral Col...</td>\n",
                     "      <td>&lt;br&gt;To the Editor:\\n&lt;p&gt;\\n  Re ''Drop Out of th...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>nytimes</td>\n",
                     "      <td>0</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>1</th>\n",
                     "      <td>3676064</td>\n",
                     "      <td>nationalreview.com</td>\n",
                     "      <td>political</td>\n",
                     "      <td>http://www.nationalreview.com/postmodern-conse...</td>\n",
                     "      <td>['ive', 'written', 'anoth', 'channel', 'advic'...</td>\n",
                     "      <td>2017-11-27T01:14:42.983556</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Antonin Scalia, Donald Trump, Tyler Cowen &amp; Co...</td>\n",
                     "      <td>Peter Augustine Lawler</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['Peter Augustine Lawler']</td>\n",
                     "      <td>Senate Republicans should enter into the battl...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>0</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>2</th>\n",
                     "      <td>5192554</td>\n",
                     "      <td>infowars.com</td>\n",
                     "      <td>conspiracy</td>\n",
                     "      <td>https://www.infowars.com/soldiers-nearly-kille...</td>\n",
                     "      <td>['david', 'gutierrez', 'natur', 'news', 'novem...</td>\n",
                     "      <td>2017-12-09T22:10:08.302997</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Soldiers Nearly Killed with Military’s Bioterr...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>0</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>3</th>\n",
                     "      <td>1136409</td>\n",
                     "      <td>redstate.com</td>\n",
                     "      <td>political</td>\n",
                     "      <td>https://www.redstate.com/diary/Erick/2010/10/2...</td>\n",
                     "      <td>['colorado', 'close', 'call', 'right', 'union'...</td>\n",
                     "      <td>2017-11-10T11:18:44.524042</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Colorado Should Not Be Too Close To Call</td>\n",
                     "      <td>Erick Erickson, Redstate Insider, Susan Wright...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>Colorado Should Not Be Too Close To Call</td>\n",
                     "      <td>Michael Bennet, Colorado, ken buck</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>0</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4</th>\n",
                     "      <td>62174</td>\n",
                     "      <td>beforeitsnews.com</td>\n",
                     "      <td>fake</td>\n",
                     "      <td>http://beforeitsnews.com/survival/2015/06/lett...</td>\n",
                     "      <td>['letter', 'number', 'beast', 'area', 'code', ...</td>\n",
                     "      <td>2018-01-25 20:13:50.426130</td>\n",
                     "      <td>2018-02-02 01:19:41.756632</td>\n",
                     "      <td>2018-02-02 01:19:41.756664</td>\n",
                     "      <td>Letter Re: Does the Number of the Beast Have a...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>0</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>...</th>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "      <td>...</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4995</th>\n",
                     "      <td>4692181</td>\n",
                     "      <td>beforeitsnews.com</td>\n",
                     "      <td>fake</td>\n",
                     "      <td>http://beforeitsnews.com/spirit/2016/11/thane-...</td>\n",
                     "      <td>['two', 'remot', 'villag', 'neighbour', 'thane...</td>\n",
                     "      <td>2017-11-27T01:14:08.7454</td>\n",
                     "      <td>2018-02-08 19:18:34.468038</td>\n",
                     "      <td>2018-02-08 19:18:34.468066</td>\n",
                     "      <td>Thane villages get rid of darkness this Diwali</td>\n",
                     "      <td>Wisdom Blog Of Art Of Living</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>2</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4996</th>\n",
                     "      <td>1308332</td>\n",
                     "      <td>chroniclesmagazine.org</td>\n",
                     "      <td>political</td>\n",
                     "      <td>http://www.chroniclesmagazine.org/1998/Februar...</td>\n",
                     "      <td>['peter', 'lababera', 'publish', 'lambda', 're...</td>\n",
                     "      <td>2017-11-10T11:18:44.524042</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Chronicles Magazine</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>2</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4997</th>\n",
                     "      <td>6505182</td>\n",
                     "      <td>nytimes.com</td>\n",
                     "      <td>reliable</td>\n",
                     "      <td>https://query.nytimes.com/gst/fullpage.html?re...</td>\n",
                     "      <td>['saintsa', '?', 'piano', 'concerto', 'no', '&lt;...</td>\n",
                     "      <td>2018-02-11 00:42:12.085863</td>\n",
                     "      <td>2018-02-11 00:14:20.346838</td>\n",
                     "      <td>2018-02-11 00:14:20.346871</td>\n",
                     "      <td>Classical Recordings: Ambient Haze, Romanticis...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>SAINT-SA�NS: PIANO CONCERTOS NOS. 2 &amp; 5; FRANC...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>nytimes</td>\n",
                     "      <td>2</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4998</th>\n",
                     "      <td>73138</td>\n",
                     "      <td>beforeitsnews.com</td>\n",
                     "      <td>fake</td>\n",
                     "      <td>http://beforeitsnews.com/opinion-conservative/...</td>\n",
                     "      <td>['barack', 'terribl', 'horribl', 'good', 'bad'...</td>\n",
                     "      <td>2018-01-25 20:13:50.426130</td>\n",
                     "      <td>2018-02-02 01:19:41.756632</td>\n",
                     "      <td>2018-02-02 01:19:41.756664</td>\n",
                     "      <td>Barack and the Terrible, Horrible, No Good, Ve...</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>2</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>4999</th>\n",
                     "      <td>1935828</td>\n",
                     "      <td>jihadwatch.org</td>\n",
                     "      <td>conspiracy</td>\n",
                     "      <td>https://www.jihadwatch.org/2009/05/sheikh-al-a...</td>\n",
                     "      <td>['yet', 'doesnt', 'explain', 'exactli', 'theyr...</td>\n",
                     "      <td>2017-11-18T20:01:27.400599</td>\n",
                     "      <td>2018-02-07 23:39:33.852671</td>\n",
                     "      <td>2018-02-07 23:39:33.852696</td>\n",
                     "      <td>Sheikh Al-Azhar urges jihad against terrorists</td>\n",
                     "      <td>Robert Spencer, Michael Copeland</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>['']</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>NaN</td>\n",
                     "      <td>2</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "<p>5000 rows × 17 columns</p>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "           id                  domain        type  \\\n",
                     "0     6301449             nytimes.com    reliable   \n",
                     "1     3676064      nationalreview.com   political   \n",
                     "2     5192554            infowars.com  conspiracy   \n",
                     "3     1136409            redstate.com   political   \n",
                     "4       62174       beforeitsnews.com        fake   \n",
                     "...       ...                     ...         ...   \n",
                     "4995  4692181       beforeitsnews.com        fake   \n",
                     "4996  1308332  chroniclesmagazine.org   political   \n",
                     "4997  6505182             nytimes.com    reliable   \n",
                     "4998    73138       beforeitsnews.com        fake   \n",
                     "4999  1935828          jihadwatch.org  conspiracy   \n",
                     "\n",
                     "                                                    url  \\\n",
                     "0     https://query.nytimes.com/gst/fullpage.html?re...   \n",
                     "1     http://www.nationalreview.com/postmodern-conse...   \n",
                     "2     https://www.infowars.com/soldiers-nearly-kille...   \n",
                     "3     https://www.redstate.com/diary/Erick/2010/10/2...   \n",
                     "4     http://beforeitsnews.com/survival/2015/06/lett...   \n",
                     "...                                                 ...   \n",
                     "4995  http://beforeitsnews.com/spirit/2016/11/thane-...   \n",
                     "4996  http://www.chroniclesmagazine.org/1998/Februar...   \n",
                     "4997  https://query.nytimes.com/gst/fullpage.html?re...   \n",
                     "4998  http://beforeitsnews.com/opinion-conservative/...   \n",
                     "4999  https://www.jihadwatch.org/2009/05/sheikh-al-a...   \n",
                     "\n",
                     "                                                content  \\\n",
                     "0     ['editor', 'drop', 'colleg', 'editori', 'march...   \n",
                     "1     ['ive', 'written', 'anoth', 'channel', 'advic'...   \n",
                     "2     ['david', 'gutierrez', 'natur', 'news', 'novem...   \n",
                     "3     ['colorado', 'close', 'call', 'right', 'union'...   \n",
                     "4     ['letter', 'number', 'beast', 'area', 'code', ...   \n",
                     "...                                                 ...   \n",
                     "4995  ['two', 'remot', 'villag', 'neighbour', 'thane...   \n",
                     "4996  ['peter', 'lababera', 'publish', 'lambda', 're...   \n",
                     "4997  ['saintsa', '?', 'piano', 'concerto', 'no', '<...   \n",
                     "4998  ['barack', 'terribl', 'horribl', 'good', 'bad'...   \n",
                     "4999  ['yet', 'doesnt', 'explain', 'exactli', 'theyr...   \n",
                     "\n",
                     "                      scraped_at                 inserted_at  \\\n",
                     "0     2018-02-11 00:40:10.316783  2018-02-11 00:14:20.346838   \n",
                     "1     2017-11-27T01:14:42.983556  2018-02-08 19:18:34.468038   \n",
                     "2     2017-12-09T22:10:08.302997  2018-02-08 19:18:34.468038   \n",
                     "3     2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
                     "4     2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
                     "...                          ...                         ...   \n",
                     "4995    2017-11-27T01:14:08.7454  2018-02-08 19:18:34.468038   \n",
                     "4996  2017-11-10T11:18:44.524042  2018-02-07 23:39:33.852671   \n",
                     "4997  2018-02-11 00:42:12.085863  2018-02-11 00:14:20.346838   \n",
                     "4998  2018-01-25 20:13:50.426130  2018-02-02 01:19:41.756632   \n",
                     "4999  2017-11-18T20:01:27.400599  2018-02-07 23:39:33.852671   \n",
                     "\n",
                     "                      updated_at  \\\n",
                     "0     2018-02-11 00:14:20.346871   \n",
                     "1     2018-02-08 19:18:34.468066   \n",
                     "2     2018-02-08 19:18:34.468066   \n",
                     "3     2018-02-07 23:39:33.852696   \n",
                     "4     2018-02-02 01:19:41.756664   \n",
                     "...                          ...   \n",
                     "4995  2018-02-08 19:18:34.468066   \n",
                     "4996  2018-02-07 23:39:33.852696   \n",
                     "4997  2018-02-11 00:14:20.346871   \n",
                     "4998  2018-02-02 01:19:41.756664   \n",
                     "4999  2018-02-07 23:39:33.852696   \n",
                     "\n",
                     "                                                  title  \\\n",
                     "0                  Time to Scrap the Electoral College?   \n",
                     "1     Antonin Scalia, Donald Trump, Tyler Cowen & Co...   \n",
                     "2     Soldiers Nearly Killed with Military’s Bioterr...   \n",
                     "3              Colorado Should Not Be Too Close To Call   \n",
                     "4     Letter Re: Does the Number of the Beast Have a...   \n",
                     "...                                                 ...   \n",
                     "4995     Thane villages get rid of darkness this Diwali   \n",
                     "4996                                Chronicles Magazine   \n",
                     "4997  Classical Recordings: Ambient Haze, Romanticis...   \n",
                     "4998  Barack and the Terrible, Horrible, No Good, Ve...   \n",
                     "4999     Sheikh Al-Azhar urges jihad against terrorists   \n",
                     "\n",
                     "                                                authors  keywords  \\\n",
                     "0                                                   NaN       NaN   \n",
                     "1                                Peter Augustine Lawler       NaN   \n",
                     "2                                                   NaN       NaN   \n",
                     "3     Erick Erickson, Redstate Insider, Susan Wright...       NaN   \n",
                     "4                                                   NaN       NaN   \n",
                     "...                                                 ...       ...   \n",
                     "4995                       Wisdom Blog Of Art Of Living       NaN   \n",
                     "4996                                                NaN       NaN   \n",
                     "4997                                                NaN       NaN   \n",
                     "4998                                                NaN       NaN   \n",
                     "4999                   Robert Spencer, Michael Copeland       NaN   \n",
                     "\n",
                     "                                          meta_keywords  \\\n",
                     "0     ['Presidential Elections (US)', 'Electoral Col...   \n",
                     "1                            ['Peter Augustine Lawler']   \n",
                     "2                                                  ['']   \n",
                     "3                                                  ['']   \n",
                     "4                                                  ['']   \n",
                     "...                                                 ...   \n",
                     "4995                                               ['']   \n",
                     "4996                                               ['']   \n",
                     "4997                                               ['']   \n",
                     "4998                                               ['']   \n",
                     "4999                                               ['']   \n",
                     "\n",
                     "                                       meta_description  \\\n",
                     "0     <br>To the Editor:\\n<p>\\n  Re ''Drop Out of th...   \n",
                     "1     Senate Republicans should enter into the battl...   \n",
                     "2                                                   NaN   \n",
                     "3              Colorado Should Not Be Too Close To Call   \n",
                     "4                                                   NaN   \n",
                     "...                                                 ...   \n",
                     "4995                                                NaN   \n",
                     "4996                                                NaN   \n",
                     "4997  SAINT-SA�NS: PIANO CONCERTOS NOS. 2 & 5; FRANC...   \n",
                     "4998                                                NaN   \n",
                     "4999                                                NaN   \n",
                     "\n",
                     "                                    tags  summary   source  set  \n",
                     "0                                    NaN      NaN  nytimes    0  \n",
                     "1                                    NaN      NaN      NaN    0  \n",
                     "2                                    NaN      NaN      NaN    0  \n",
                     "3     Michael Bennet, Colorado, ken buck      NaN      NaN    0  \n",
                     "4                                    NaN      NaN      NaN    0  \n",
                     "...                                  ...      ...      ...  ...  \n",
                     "4995                                 NaN      NaN      NaN    2  \n",
                     "4996                                 NaN      NaN      NaN    2  \n",
                     "4997                                 NaN      NaN  nytimes    2  \n",
                     "4998                                 NaN      NaN      NaN    2  \n",
                     "4999                                 NaN      NaN      NaN    2  \n",
                     "\n",
                     "[5000 rows x 17 columns]"
                  ]
               },
               "execution_count": 11,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "l"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Check distribution of labels (just to show that everything works)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 135,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "File: ../datasets/big/dataset_unbalanced.csv ----------------------------------\n",
                  "Distribution of train with size 8000:\n",
                  "fake: 0.121625, conspiracy: 0.123125, junksci: 0.0155, hate: 0.01, unreliable: 0.04575, bias: 0.15475, satire: 0.016125, reliable: 0.259125, clickbait: 0.032, political: 0.222\n",
                  "True: 4105, Fake: 3895\n",
                  "Distribution of val with size 1000:\n",
                  "fake: 0.129, conspiracy: 0.124, junksci: 0.012, hate: 0.014, unreliable: 0.045, bias: 0.147, satire: 0.022, reliable: 0.262, clickbait: 0.021, political: 0.224\n",
                  "True: 507, Fake: 493\n",
                  "Distribution of test with size 1000:\n",
                  "fake: 0.106, conspiracy: 0.125, junksci: 0.017, hate: 0.008, unreliable: 0.05, bias: 0.172, satire: 0.014, reliable: 0.255, clickbait: 0.032, political: 0.221\n",
                  "True: 508, Fake: 492\n",
                  "File: ../datasets/big/dataset_balanced_types.csv ----------------------------------\n",
                  "Distribution of train with size 8000:\n",
                  "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
                  "True: 2400, Fake: 5600\n",
                  "Distribution of val with size 1000:\n",
                  "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
                  "True: 300, Fake: 700\n",
                  "Distribution of test with size 1000:\n",
                  "fake: 0.12, conspiracy: 0.142, junksci: 0.015, hate: 0.013, unreliable: 0.04, bias: 0.144, satire: 0.02, reliable: 0.248, clickbait: 0.034, political: 0.224\n",
                  "True: 506, Fake: 494\n",
                  "File: ../datasets/big/dataset_balanced_bin.csv ----------------------------------\n",
                  "Distribution of train with size 8000:\n",
                  "fake: 0.124625, conspiracy: 0.127, junksci: 0.01575, hate: 0.01025, unreliable: 0.047125, bias: 0.158625, satire: 0.016625, reliable: 0.252625, clickbait: 0.031375, political: 0.216\n",
                  "True: 4000, Fake: 4000\n",
                  "Distribution of val with size 1000:\n",
                  "fake: 0.125, conspiracy: 0.121, junksci: 0.013, hate: 0.017, unreliable: 0.048, bias: 0.154, satire: 0.022, reliable: 0.256, clickbait: 0.026, political: 0.218\n",
                  "True: 500, Fake: 500\n",
                  "Distribution of test with size 1000:\n",
                  "fake: 0.121, conspiracy: 0.115, junksci: 0.022, hate: 0.008, unreliable: 0.044, bias: 0.162, satire: 0.012, reliable: 0.259, clickbait: 0.029, political: 0.228\n",
                  "True: 516, Fake: 484\n"
               ]
            }
         ],
         "source": [
            "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
            "    for i, label in enumerate(pp.labels):\n",
            "        if is_percentage:\n",
            "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
            "        else:\n",
            "            percent = len(data[data[col] == label])\n",
            "        print(f\"{label}: {percent}\", end=\"\")\n",
            "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
            "\n",
            "for file in [\"../datasets/big/dataset_unbalanced.csv\", \"../datasets/big/dataset_balanced_types.csv\", \"../datasets/big/dataset_balanced_bin.csv\"]:\n",
            "    data = pd.read_csv(file)\n",
            "    print(f\"File: {file} ----------------------------------\")\n",
            "    # find distribution of labels\n",
            "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
            "        set = data[data[\"set\"] == i]\n",
            "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
            "        get_distribution(set)\n",
            "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Cleaning the files"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 14,
         "metadata": {},
         "outputs": [
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "100%|██████████| 5000/5000 [00:00<00:00, 312485.40it/s]\n",
                  "100%|██████████| 5000/5000 [00:11<00:00, 435.87it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 22456.69it/s]\n",
                  "100%|██████████| 5000/5000 [00:04<00:00, 1228.44it/s]\n",
                  "100%|██████████| 5000/5000 [00:30<00:00, 162.56it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 96521.52it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 133421.89it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 17120.38it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 404410.59it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 35643.24it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 7458.25it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 617972.65it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 304548.58it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 49098.57it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 45045.70it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 46495.41it/s]\n",
                  "100%|██████████| 5000/5000 [00:00<00:00, 29124.24it/s]\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "processed 200000 rows\n",
                  "finish time: 50.075483083724976\n"
               ]
            }
         ],
         "source": [
            "importlib.reload(pp)\n",
            "\n",
            "def Clean_data(file, new_file):\n",
            "    stopwords_lst = stopwords.words('english')\n",
            "    pp.apply_pipeline(file, [\n",
            "            # binary labels\n",
            "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
            "            # Clean content\n",
            "            (pp.Clean_data(), 'content'),\n",
            "            (pp.Tokenizer(), \"content\"),\n",
            "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
            "            (pp.Stem(), \"content\"),\n",
            "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
            "            # Clean authors\n",
            "            (pp.Clean_author(), \"authors\"),\n",
            "            # Clean title\n",
            "            (pp.Clean_data(), 'title'),\n",
            "            (pp.Tokenizer(), \"title\"),\n",
            "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
            "            (pp.Stem(), \"title\"),\n",
            "            (pp.Combine_Content(), \"title\"),\n",
            "            # Clean domain\n",
            "            (pp.Clean_domain(), 'domain'),\n",
            "            # Combine columns (used as features)\n",
            "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
            "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
            "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
            "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
            "        ],\n",
            "        new_file=new_file,\n",
            "        progress_bar=True,\n",
            "    )\n",
            "\n",
            "Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
            "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
            "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
            "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 107,
         "metadata": {},
         "outputs": [],
         "source": [
            "def split_data(data, features, y, set=\"set\", get_val=True):\n",
            "    train = data[data[set] == 0]\n",
            "    val = data[data[set] == 1]\n",
            "    test = data[data[set] == 2]\n",
            "    X_train, y_train = train[features], (train[y].astype(int))\n",
            "    X_val, y_val = val[features], (val[y].astype(int))\n",
            "    X_test, y_test = test[features], (test[y].astype(int))\n",
            "    if not get_val:\n",
            "        return X_train, X_test, y_train, y_test\n",
            "    return X_train, X_val, X_test, y_train, y_val, y_test"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 108,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train, X_val, X_test, y_train, y_val, y_test = split_data(pd.read_csv(\"../datasets/sample/dataset_unbalanced_cleaned.csv\"), \"content_combined\", \"type_binary\")"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Training the logistic model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_count_vector(X_train, X_val, X_test):\n",
            "    # count vectri\n",
            "    count_vectorizer = CountVectorizer(ngram_range=(1, 1)) # unigram\n",
            "\n",
            "    # fit and transform train data to count vectorizer\n",
            "    count_vectorizer.fit(X_train)\n",
            "\n",
            "    return (count_vectorizer.transform(X_train),\n",
            "            count_vectorizer.transform(X_val),\n",
            "            count_vectorizer.transform(X_test))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [],
         "source": [
            "def create_tdfidf_vector(X_train, X_val, X_test, ngram_range=(1, 1)):\n",
            "    # tfidf vector\n",
            "    tfidf_vectorizer = TfidfVectorizer(ngram_range=ngram_range) # unigram\n",
            "\n",
            "    # fit and transform train data to tfidf vectorizer\n",
            "    tfidf_vectorizer.fit(X_train)\n",
            "\n",
            "    return (tfidf_vectorizer.transform(X_train),\n",
            "            tfidf_vectorizer.transform(X_val),\n",
            "            tfidf_vectorizer.transform(X_test))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 71,
         "metadata": {},
         "outputs": [],
         "source": [
            "X_train_vec, X_val_vec, X_test_vec = create_tdfidf_vector(X_train, X_val, X_test)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 96,
         "metadata": {},
         "outputs": [],
         "source": [
            "def save_csr_picle(file, vectors, append=False):\n",
            "    file_param = \"wb\" if not append else \"ab\"\n",
            "    with open(file, file_param) as f:\n",
            "        for train, val, test in vectors:\n",
            "            pickle.dump(train, f)\n",
            "            pickle.dump(val, f)\n",
            "            pickle.dump(test, f)\n",
            "\n",
            "def load_csr_picle(file, vector_names):\n",
            "    vectors = {}\n",
            "    with open(file, 'rb') as f:\n",
            "        for names in vector_names:\n",
            "            vectors[names] = (pickle.load(f), pickle.load(f), pickle.load(f))\n",
            "        \n",
            "    return vectors"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 101,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Vectorize data\n",
            "def create_vector_file(file, vec_funcs, X_train, X_val, X_test, y_train, y_val, y_test):\n",
            "    # save y\n",
            "    save_csr_picle(file, [(y_train, y_val, y_test)])\n",
            "    # vectorize X and save\n",
            "    for func in vec_funcs:\n",
            "        X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
            "        save_csr_picle(file, [(X_train_vec, X_val_vec, X_test_vec)], append=True)\n",
            "        print(f\"Saved {func.__name__} vectors\")\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 102,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Saved create_count_vector vectors\n",
                  "Saved <lambda> vectors\n",
                  "Saved <lambda> vectors\n"
               ]
            }
         ],
         "source": [
            "vec_funcs = [\n",
            "    create_count_vector,\n",
            "    lambda a, b, c: create_tdfidf_vector(a, b, c, ngram_range=(1, 1)),\n",
            "    lambda a, b, c: create_tdfidf_vector(a, b, c, ngram_range=(1, 2)),\n",
            "]\n",
            "create_vector_file(\"../datasets/sample/dataset_unbalanced_cleaned_vectors.pickle\", vec_funcs, X_train, X_val, X_test, y_train, y_val, y_test)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 109,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "LogisticRegression finished in 0.74 seconds\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>name</th>\n",
                     "      <th>train_acc</th>\n",
                     "      <th>val_acc</th>\n",
                     "      <th>precision</th>\n",
                     "      <th>recall</th>\n",
                     "      <th>f1</th>\n",
                     "      <th>time</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>LogisticRegression</td>\n",
                     "      <td>1.0</td>\n",
                     "      <td>0.69</td>\n",
                     "      <td>0.793103</td>\n",
                     "      <td>0.587591</td>\n",
                     "      <td>0.675052</td>\n",
                     "      <td>0.74</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                 name  train_acc  val_acc  precision    recall        f1  time\n",
                     "0  LogisticRegression        1.0     0.69   0.793103  0.587591  0.675052  0.74"
                  ]
               },
               "execution_count": 109,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "vectors = load_csr_picle(\"../datasets/sample/dataset_unbalanced_cleaned_vectors.pickle\", [\"y\", \"count\", \"tfidf\", \"tfidf_bigram\"])\n",
            "X_train_1, X_val_1, X_test_1 = vectors[\"count\"]\n",
            "y_train_1, y_val_1, y_test_1 = vectors[\"y\"]\n",
            "try_models([LogisticRegression()], X_test_1, X_val_1, y_test, y_val)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 103,
         "metadata": {},
         "outputs": [],
         "source": [
            "def try_models(models, X_train, X_val, y_train, y_val, name=None):\n",
            "    metrics = []\n",
            "    for model in models:\n",
            "        start_time = time() \n",
            "        model.fit(X_train, y_train)\n",
            "        train_time = time() - start_time\n",
            "        y_train_pred = model.predict(X_train)\n",
            "        y_pred = model.predict(X_val)\n",
            "        \n",
            "        if name == None:\n",
            "            name = type(model).__name__\n",
            "        metrics.append({\n",
            "            \"name\": name,\n",
            "            \"train_acc\": accuracy_score(y_train, y_train_pred),\n",
            "            \"val_acc\": accuracy_score(y_val, y_pred),\n",
            "            \"precision\": precision_score(y_val, y_pred),\n",
            "            \"recall\": recall_score(y_val, y_pred),\n",
            "            \"f1\": f1_score(y_val, y_pred), \n",
            "            \"time\": \"{:.2f}\".format(train_time)\n",
            "        })\n",
            "        print(f\"{name} finished in {(time() - start_time):.2f} seconds\")\n",
            "    return pd.DataFrame(metrics)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 81,
         "metadata": {},
         "outputs": [],
         "source": [
            "class Test_statistic():\n",
            "    def __init__(self):\n",
            "        self.metrics = pd.DataFrame()\n",
            "\n",
            "    def test_baseline(self, X_train, X_val, y_train, y_val, name=None, model=None):\n",
            "        if model == None:\n",
            "            model = LogisticRegression()\n",
            "        metric = try_models([model], X_train, X_val, y_train, y_val, name=name)\n",
            "        self.metrics = pd.concat([self.metrics, metric])\n",
            "\n",
            "    def test_col(self, data, col, name, model=None):\n",
            "        self.test_baseline(*split_csr_data(data, features=col, y=\"type_binary\", get_val=False), name=name, model=model)\n",
            "\n",
            "    def test_cols(self, data, cols_to_test, model=None):\n",
            "        for col, name in cols_to_test:\n",
            "            self.test_baseline(*split_csr_data(data, features=col, y=\"type_binary\", get_val=False), name=name, model=model)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 82,
         "metadata": {},
         "outputs": [],
         "source": [
            "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
            "    if tests == None:\n",
            "        tests = Test_statistic()\n",
            "    for file, name in files:\n",
            "        print(f\"Proccessing: {name}\")\n",
            "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
            "        data = pd.read_csv(file, usecols=cols_to_read)\n",
            "        print(\"Read data into dataframe\")\n",
            "\n",
            "        for col, entry_name in cols_to_test:\n",
            "            for func, model, func_name in vec_funcs:\n",
            "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
            "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
            "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
            "                #print(X_train_vec.shape, X_val_vec.shape, X_test_vec.shape, y_train.shape, y_val.shape, y_test.shape)\n",
            "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
            "    return tests"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 49,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Proccessing: \n",
                  "Read data into dataframe\n",
                  "Vectorized content with count_vec\n",
                  "content__count_vec finished in 1.76 seconds\n",
                  "Vectorized content with tfidf_vec\n",
                  "content__tfidf_vec finished in 0.39 seconds\n",
                  "Vectorized content with tfidf_vec_bigram\n",
                  "content__tfidf_vec_bigram finished in 5.58 seconds\n",
                  "Vectorized content_authors with count_vec\n",
                  "content_authors__count_vec finished in 1.70 seconds\n",
                  "Vectorized content_authors with tfidf_vec\n",
                  "content_authors__tfidf_vec finished in 0.40 seconds\n",
                  "Vectorized content_authors with tfidf_vec_bigram\n",
                  "content_authors__tfidf_vec_bigram finished in 4.99 seconds\n",
                  "Vectorized content_title with count_vec\n",
                  "content_title__count_vec finished in 1.61 seconds\n",
                  "Vectorized content_title with tfidf_vec\n",
                  "content_title__tfidf_vec finished in 0.32 seconds\n",
                  "Vectorized content_title with tfidf_vec_bigram\n",
                  "content_title__tfidf_vec_bigram finished in 6.40 seconds\n",
                  "Vectorized content_domain with count_vec\n",
                  "content_domain__count_vec finished in 1.64 seconds\n",
                  "Vectorized content_domain with tfidf_vec\n",
                  "content_domain__tfidf_vec finished in 0.40 seconds\n",
                  "Vectorized content_domain with tfidf_vec_bigram\n",
                  "content_domain__tfidf_vec_bigram finished in 5.47 seconds\n",
                  "Vectorized all with count_vec\n",
                  "all__count_vec finished in 1.60 seconds\n",
                  "Vectorized all with tfidf_vec\n",
                  "all__tfidf_vec finished in 0.32 seconds\n",
                  "Vectorized all with tfidf_vec_bigram\n",
                  "all__tfidf_vec_bigram finished in 5.73 seconds\n"
               ]
            },
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>name</th>\n",
                     "      <th>train_acc</th>\n",
                     "      <th>val_acc</th>\n",
                     "      <th>precision</th>\n",
                     "      <th>recall</th>\n",
                     "      <th>f1</th>\n",
                     "      <th>time</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content__count_vec</td>\n",
                     "      <td>0.99950</td>\n",
                     "      <td>0.764</td>\n",
                     "      <td>0.814516</td>\n",
                     "      <td>0.737226</td>\n",
                     "      <td>0.773946</td>\n",
                     "      <td>1.75</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content__tfidf_vec</td>\n",
                     "      <td>0.93475</td>\n",
                     "      <td>0.784</td>\n",
                     "      <td>0.809701</td>\n",
                     "      <td>0.791971</td>\n",
                     "      <td>0.800738</td>\n",
                     "      <td>0.38</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content__tfidf_vec_bigram</td>\n",
                     "      <td>0.98050</td>\n",
                     "      <td>0.800</td>\n",
                     "      <td>0.815217</td>\n",
                     "      <td>0.821168</td>\n",
                     "      <td>0.818182</td>\n",
                     "      <td>5.56</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors__count_vec</td>\n",
                     "      <td>0.99975</td>\n",
                     "      <td>0.798</td>\n",
                     "      <td>0.841897</td>\n",
                     "      <td>0.777372</td>\n",
                     "      <td>0.808349</td>\n",
                     "      <td>1.69</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors__tfidf_vec</td>\n",
                     "      <td>0.94700</td>\n",
                     "      <td>0.816</td>\n",
                     "      <td>0.842105</td>\n",
                     "      <td>0.817518</td>\n",
                     "      <td>0.829630</td>\n",
                     "      <td>0.39</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors__tfidf_vec_bigram</td>\n",
                     "      <td>0.98350</td>\n",
                     "      <td>0.822</td>\n",
                     "      <td>0.833935</td>\n",
                     "      <td>0.843066</td>\n",
                     "      <td>0.838475</td>\n",
                     "      <td>4.97</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title__count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.784</td>\n",
                     "      <td>0.826772</td>\n",
                     "      <td>0.766423</td>\n",
                     "      <td>0.795455</td>\n",
                     "      <td>1.61</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title__tfidf_vec</td>\n",
                     "      <td>0.93825</td>\n",
                     "      <td>0.794</td>\n",
                     "      <td>0.813187</td>\n",
                     "      <td>0.810219</td>\n",
                     "      <td>0.811700</td>\n",
                     "      <td>0.31</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title__tfidf_vec_bigram</td>\n",
                     "      <td>0.98025</td>\n",
                     "      <td>0.792</td>\n",
                     "      <td>0.805755</td>\n",
                     "      <td>0.817518</td>\n",
                     "      <td>0.811594</td>\n",
                     "      <td>6.38</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain__count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.846</td>\n",
                     "      <td>0.880309</td>\n",
                     "      <td>0.832117</td>\n",
                     "      <td>0.855535</td>\n",
                     "      <td>1.63</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain__tfidf_vec</td>\n",
                     "      <td>0.95625</td>\n",
                     "      <td>0.830</td>\n",
                     "      <td>0.843636</td>\n",
                     "      <td>0.846715</td>\n",
                     "      <td>0.845173</td>\n",
                     "      <td>0.39</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain__tfidf_vec_bigram</td>\n",
                     "      <td>0.98400</td>\n",
                     "      <td>0.832</td>\n",
                     "      <td>0.836879</td>\n",
                     "      <td>0.861314</td>\n",
                     "      <td>0.848921</td>\n",
                     "      <td>5.46</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>all__count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.846</td>\n",
                     "      <td>0.871698</td>\n",
                     "      <td>0.843066</td>\n",
                     "      <td>0.857143</td>\n",
                     "      <td>1.59</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>all__tfidf_vec</td>\n",
                     "      <td>0.96275</td>\n",
                     "      <td>0.844</td>\n",
                     "      <td>0.855072</td>\n",
                     "      <td>0.861314</td>\n",
                     "      <td>0.858182</td>\n",
                     "      <td>0.31</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>all__tfidf_vec_bigram</td>\n",
                     "      <td>0.98700</td>\n",
                     "      <td>0.844</td>\n",
                     "      <td>0.852518</td>\n",
                     "      <td>0.864964</td>\n",
                     "      <td>0.858696</td>\n",
                     "      <td>5.70</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                name  train_acc  val_acc  precision    recall  \\\n",
                     "0                 content__count_vec    0.99950    0.764   0.814516  0.737226   \n",
                     "0                 content__tfidf_vec    0.93475    0.784   0.809701  0.791971   \n",
                     "0          content__tfidf_vec_bigram    0.98050    0.800   0.815217  0.821168   \n",
                     "0         content_authors__count_vec    0.99975    0.798   0.841897  0.777372   \n",
                     "0         content_authors__tfidf_vec    0.94700    0.816   0.842105  0.817518   \n",
                     "0  content_authors__tfidf_vec_bigram    0.98350    0.822   0.833935  0.843066   \n",
                     "0           content_title__count_vec    1.00000    0.784   0.826772  0.766423   \n",
                     "0           content_title__tfidf_vec    0.93825    0.794   0.813187  0.810219   \n",
                     "0    content_title__tfidf_vec_bigram    0.98025    0.792   0.805755  0.817518   \n",
                     "0          content_domain__count_vec    1.00000    0.846   0.880309  0.832117   \n",
                     "0          content_domain__tfidf_vec    0.95625    0.830   0.843636  0.846715   \n",
                     "0   content_domain__tfidf_vec_bigram    0.98400    0.832   0.836879  0.861314   \n",
                     "0                     all__count_vec    1.00000    0.846   0.871698  0.843066   \n",
                     "0                     all__tfidf_vec    0.96275    0.844   0.855072  0.861314   \n",
                     "0              all__tfidf_vec_bigram    0.98700    0.844   0.852518  0.864964   \n",
                     "\n",
                     "         f1  time  \n",
                     "0  0.773946  1.75  \n",
                     "0  0.800738  0.38  \n",
                     "0  0.818182  5.56  \n",
                     "0  0.808349  1.69  \n",
                     "0  0.829630  0.39  \n",
                     "0  0.838475  4.97  \n",
                     "0  0.795455  1.61  \n",
                     "0  0.811700  0.31  \n",
                     "0  0.811594  6.38  \n",
                     "0  0.855535  1.63  \n",
                     "0  0.845173  0.39  \n",
                     "0  0.848921  5.46  \n",
                     "0  0.857143  1.59  \n",
                     "0  0.858182  0.31  \n",
                     "0  0.858696  5.70  "
                  ]
               },
               "execution_count": 49,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "test_stat = test_files(\n",
            "    files = [\n",
            "        (\"../datasets/sample/dataset_unbalanced_cleaned.csv\", \"\"), \n",
            "    ],\n",
            "    cols_to_test = [\n",
            "        (\"content_combined\", \"content\"),\n",
            "        (\"content_authors\", \"content_authors\"), \n",
            "        (\"content_title\", \"content_title\"),\n",
            "        (\"content_domain\", \"content_domain\"),\n",
            "        (\"content_domain_authors_title\", \"all\")\n",
            "    ],\n",
            "    vec_funcs = [\n",
            "        (create_count_vector, LogisticRegression(max_iter=1000), \"count_vec\"),\n",
            "        (lambda a, b, c: create_tdfidf_vector(a, b, c, ngram_range=(1, 1)), LogisticRegression(), \"tfidf_vec\"),\n",
            "        (lambda a, b, c: create_tdfidf_vector(a, b, c, ngram_range=(1, 2)), LogisticRegression(), \"tfidf_vec_bigram\"),\n",
            "    ])\n",
            "test_stat.metrics.sort_values(by=\"f1\", ascending=False)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Old:"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 48,
         "metadata": {},
         "outputs": [],
         "source": [
            "# contains the files to test and the name of the test-group (for the dataframe)\n",
            "files = [\n",
            "    (\"../datasets/sample/dataset_unbalanced_cleaned.csv\", \"unbalanced\"), \n",
            "    #(\"../datasets/big/dataset_unbalanced_cleaned.csv\", \"unbalanced\"), \n",
            "    #(\"../datasets/big/dataset_balanced_types_cleaned.csv\", \"balanced_types\"), \n",
            "    #(\"../datasets/big/dataset_balanced_bin_cleaned.csv\", \"balanced_bin\"),\n",
            "    #(\"../datasets/big/dataset_reliable_fake_cleaned.csv\", \"reliable_fake\")\n",
            "],\n",
            "# contains the columns to test and the name of the specific test (for the dataframe)\n",
            "cols_to_test = [\n",
            "    (\"content_combined\", \"content\"),\n",
            "    (\"content_authors\", \"content_authors\"), \n",
            "    (\"content_title\", \"content_title\"),\n",
            "    (\"content_domain\", \"content_domain\"),\n",
            "    (\"content_domain_authors_title\", \"all\")\n",
            "]"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Proccessing: unbalanced\n",
                  "Read data into dataframe\n"
               ]
            }
         ],
         "source": [
            "for file, name in files:\n",
            "    print(f\"Proccessing: {name}\")\n",
            "    cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
            "    vectorized_data = pd.read_csv(file, usecols=cols_to_read)\n",
            "    print(\"Read data into dataframe\")\n",
            "    for col, entry_name in cols_to_test:\n",
            "        #vectorize_content(vectorized_data, col=col, new_col=f\"{col}_vectorized\")\n",
            "        #tests.test_col(vectorized_data, f\"{col}_vectorized\", f\"{entry_name}_{name}\")\n",
            "        vectorize_content(vectorized_data, col=col, new_col=col)\n",
            "        tests.test_col(vectorized_data, col, f\"{entry_name}_{name}\")\n",
            "        del vectorized_data[col] # free up memory"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 52,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>name</th>\n",
                     "      <th>train_acc</th>\n",
                     "      <th>val_acc</th>\n",
                     "      <th>precision</th>\n",
                     "      <th>recall</th>\n",
                     "      <th>f1</th>\n",
                     "      <th>time</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>all_unbalanced_count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.846</td>\n",
                     "      <td>0.871698</td>\n",
                     "      <td>0.843066</td>\n",
                     "      <td>0.857143</td>\n",
                     "      <td>1.86</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_unbalanced_count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.846</td>\n",
                     "      <td>0.880309</td>\n",
                     "      <td>0.832117</td>\n",
                     "      <td>0.855535</td>\n",
                     "      <td>1.49</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors_unbalanced_count_vec</td>\n",
                     "      <td>0.99975</td>\n",
                     "      <td>0.798</td>\n",
                     "      <td>0.841897</td>\n",
                     "      <td>0.777372</td>\n",
                     "      <td>0.808349</td>\n",
                     "      <td>1.97</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title_unbalanced_count_vec</td>\n",
                     "      <td>1.00000</td>\n",
                     "      <td>0.784</td>\n",
                     "      <td>0.826772</td>\n",
                     "      <td>0.766423</td>\n",
                     "      <td>0.795455</td>\n",
                     "      <td>1.73</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_unbalanced_count_vec</td>\n",
                     "      <td>0.99950</td>\n",
                     "      <td>0.764</td>\n",
                     "      <td>0.814516</td>\n",
                     "      <td>0.737226</td>\n",
                     "      <td>0.773946</td>\n",
                     "      <td>1.79</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                   name  train_acc  val_acc  precision  \\\n",
                     "0              all_unbalanced_count_vec    1.00000    0.846   0.871698   \n",
                     "0   content_domain_unbalanced_count_vec    1.00000    0.846   0.880309   \n",
                     "0  content_authors_unbalanced_count_vec    0.99975    0.798   0.841897   \n",
                     "0    content_title_unbalanced_count_vec    1.00000    0.784   0.826772   \n",
                     "0          content_unbalanced_count_vec    0.99950    0.764   0.814516   \n",
                     "\n",
                     "     recall        f1  time  \n",
                     "0  0.843066  0.857143  1.86  \n",
                     "0  0.832117  0.855535  1.49  \n",
                     "0  0.777372  0.808349  1.97  \n",
                     "0  0.766423  0.795455  1.73  \n",
                     "0  0.737226  0.773946  1.79  "
                  ]
               },
               "execution_count": 52,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tests.metrics.sort_values(by=\"f1\", ascending=False)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Best file and features"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "best_file = \"../datasets/big/dataset_unbalanced_cleaned.csv\"\n",
            "best_col = \"content_domain_authors_title\""
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 271,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Time elapsed of TF IDF transform for content_domain_authors_title: 9.814547061920166\n"
               ]
            }
         ],
         "source": [
            "data = pd.read_csv(best_file)\n",
            "vectorize_content(data, col=best_col, new_col=best_col)"
         ]
      },
      {
         "attachments": {},
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Hyperparameter tuning - the best found was C=300 and max_iter=200"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
                  "[CV] END ................................C=300, max_iter=100; total time=  10.0s\n",
                  "[CV] END ................................C=300, max_iter=100; total time=  10.8s\n",
                  "[CV] END ................................C=300, max_iter=100; total time=   7.1s\n",
                  "[CV] END ................................C=300, max_iter=150; total time=   8.8s\n",
                  "[CV] END ................................C=300, max_iter=150; total time=  11.2s\n",
                  "[CV] END ................................C=300, max_iter=150; total time=   6.7s\n",
                  "[CV] END ................................C=300, max_iter=200; total time=   8.6s\n",
                  "[CV] END ................................C=300, max_iter=200; total time=  10.6s\n",
                  "[CV] END ................................C=300, max_iter=200; total time=   7.2s\n",
                  "hyper_1 finished in 88.89 seconds\n",
                  "LogisticRegression(C=300)\n"
               ]
            }
         ],
         "source": [
            "model = LogisticRegression()\n",
            "param_grid = {\"C\": [250, 300, 350], \"max_iter\": [150, 200, 250]} #200 won - det samme\n",
            "#param_grid = {'penalty': ['l1', 'l2'],'C': [350], \"maxiter\": [200], 'solver': ['liblinear', 'saga']}\n",
            "\n",
            "grid = GridSearchCV(estimator=model,\n",
            "                    param_grid=param_grid,\n",
            "                    cv=3,\n",
            "                    scoring=['f1'],\n",
            "                    refit='f1',\n",
            "                    verbose=2) #'accuracy'\n",
            "\n",
            "\n",
            "tests.test_col(data, best_col, \"hyper_1\", model=grid)\n",
            "print(grid.best_estimator_)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "hyper_all finished in 8.66 seconds\n",
                  "Time elapsed of TF IDF transform for content_combined: 7.822778701782227\n",
                  "hyper_content finished in 9.59 seconds\n"
               ]
            }
         ],
         "source": [
            "data = pd.read_csv(best_file)\n",
            "vectorize_content(data, col=best_col, new_col=best_col)\n",
            "tests.test_col(data, best_col, \"hyper_all\", model=LogisticRegression(C=300, max_iter=200))\n",
            "# test the best parameters on the other featues\n",
            "vectorize_content(data, col=\"content_combined\", new_col=\"content_combined\")\n",
            "tests.test_col(data, \"content_combined\", \"hyper_content\", model=LogisticRegression(C=300, max_iter=200))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [
            {
               "data": {
                  "text/html": [
                     "<div>\n",
                     "<style scoped>\n",
                     "    .dataframe tbody tr th:only-of-type {\n",
                     "        vertical-align: middle;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe tbody tr th {\n",
                     "        vertical-align: top;\n",
                     "    }\n",
                     "\n",
                     "    .dataframe thead th {\n",
                     "        text-align: right;\n",
                     "    }\n",
                     "</style>\n",
                     "<table border=\"1\" class=\"dataframe\">\n",
                     "  <thead>\n",
                     "    <tr style=\"text-align: right;\">\n",
                     "      <th></th>\n",
                     "      <th>name</th>\n",
                     "      <th>train_acc</th>\n",
                     "      <th>test_acc</th>\n",
                     "      <th>precision</th>\n",
                     "      <th>recall</th>\n",
                     "      <th>f1</th>\n",
                     "      <th>time</th>\n",
                     "    </tr>\n",
                     "  </thead>\n",
                     "  <tbody>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_reliable_fake</td>\n",
                     "      <td>0.9965</td>\n",
                     "      <td>0.934</td>\n",
                     "      <td>0.979332</td>\n",
                     "      <td>0.920777</td>\n",
                     "      <td>0.949153</td>\n",
                     "      <td>4.29</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_authors_title_reliable_fake</td>\n",
                     "      <td>0.9970</td>\n",
                     "      <td>0.929</td>\n",
                     "      <td>0.985390</td>\n",
                     "      <td>0.907324</td>\n",
                     "      <td>0.944747</td>\n",
                     "      <td>4.51</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors_reliable_fake</td>\n",
                     "      <td>0.9955</td>\n",
                     "      <td>0.903</td>\n",
                     "      <td>0.976667</td>\n",
                     "      <td>0.875934</td>\n",
                     "      <td>0.923562</td>\n",
                     "      <td>3.97</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_reliable_fake</td>\n",
                     "      <td>0.9955</td>\n",
                     "      <td>0.898</td>\n",
                     "      <td>0.970149</td>\n",
                     "      <td>0.874439</td>\n",
                     "      <td>0.919811</td>\n",
                     "      <td>3.21</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title_reliable_fake</td>\n",
                     "      <td>0.9955</td>\n",
                     "      <td>0.896</td>\n",
                     "      <td>0.976391</td>\n",
                     "      <td>0.865471</td>\n",
                     "      <td>0.917591</td>\n",
                     "      <td>4.10</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hyper_all</td>\n",
                     "      <td>1.0000</td>\n",
                     "      <td>0.867</td>\n",
                     "      <td>0.881553</td>\n",
                     "      <td>0.863118</td>\n",
                     "      <td>0.872238</td>\n",
                     "      <td>8.65</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hyper_all</td>\n",
                     "      <td>1.0000</td>\n",
                     "      <td>0.867</td>\n",
                     "      <td>0.881553</td>\n",
                     "      <td>0.863118</td>\n",
                     "      <td>0.872238</td>\n",
                     "      <td>8.21</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_authors_title_balanced_bin</td>\n",
                     "      <td>0.9960</td>\n",
                     "      <td>0.841</td>\n",
                     "      <td>0.854127</td>\n",
                     "      <td>0.842803</td>\n",
                     "      <td>0.848427</td>\n",
                     "      <td>4.34</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_authors_title_unbalanced</td>\n",
                     "      <td>0.9960</td>\n",
                     "      <td>0.838</td>\n",
                     "      <td>0.838290</td>\n",
                     "      <td>0.857414</td>\n",
                     "      <td>0.847744</td>\n",
                     "      <td>5.14</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_balanced_bin</td>\n",
                     "      <td>0.9950</td>\n",
                     "      <td>0.830</td>\n",
                     "      <td>0.836466</td>\n",
                     "      <td>0.842803</td>\n",
                     "      <td>0.839623</td>\n",
                     "      <td>4.29</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_unbalanced</td>\n",
                     "      <td>0.9950</td>\n",
                     "      <td>0.825</td>\n",
                     "      <td>0.812834</td>\n",
                     "      <td>0.866920</td>\n",
                     "      <td>0.839006</td>\n",
                     "      <td>4.42</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors_balanced_bin</td>\n",
                     "      <td>0.9955</td>\n",
                     "      <td>0.818</td>\n",
                     "      <td>0.839216</td>\n",
                     "      <td>0.810606</td>\n",
                     "      <td>0.824663</td>\n",
                     "      <td>4.06</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors_unbalanced</td>\n",
                     "      <td>0.9945</td>\n",
                     "      <td>0.813</td>\n",
                     "      <td>0.820416</td>\n",
                     "      <td>0.825095</td>\n",
                     "      <td>0.822749</td>\n",
                     "      <td>4.10</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title_balanced_bin</td>\n",
                     "      <td>0.9940</td>\n",
                     "      <td>0.803</td>\n",
                     "      <td>0.820116</td>\n",
                     "      <td>0.803030</td>\n",
                     "      <td>0.811483</td>\n",
                     "      <td>3.76</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title_unbalanced</td>\n",
                     "      <td>0.9940</td>\n",
                     "      <td>0.793</td>\n",
                     "      <td>0.791590</td>\n",
                     "      <td>0.823194</td>\n",
                     "      <td>0.807083</td>\n",
                     "      <td>5.74</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_balanced_bin</td>\n",
                     "      <td>0.9945</td>\n",
                     "      <td>0.794</td>\n",
                     "      <td>0.808429</td>\n",
                     "      <td>0.799242</td>\n",
                     "      <td>0.803810</td>\n",
                     "      <td>4.24</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_unbalanced</td>\n",
                     "      <td>0.9945</td>\n",
                     "      <td>0.785</td>\n",
                     "      <td>0.780180</td>\n",
                     "      <td>0.823194</td>\n",
                     "      <td>0.801110</td>\n",
                     "      <td>5.57</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>hyper_content</td>\n",
                     "      <td>1.0000</td>\n",
                     "      <td>0.793</td>\n",
                     "      <td>0.834382</td>\n",
                     "      <td>0.756654</td>\n",
                     "      <td>0.793619</td>\n",
                     "      <td>9.57</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_balanced_types</td>\n",
                     "      <td>0.8370</td>\n",
                     "      <td>0.628</td>\n",
                     "      <td>1.000000</td>\n",
                     "      <td>0.294118</td>\n",
                     "      <td>0.454545</td>\n",
                     "      <td>4.29</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_authors_balanced_types</td>\n",
                     "      <td>0.8385</td>\n",
                     "      <td>0.626</td>\n",
                     "      <td>0.993548</td>\n",
                     "      <td>0.292220</td>\n",
                     "      <td>0.451613</td>\n",
                     "      <td>3.39</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_domain_authors_title_balanced_types</td>\n",
                     "      <td>0.8495</td>\n",
                     "      <td>0.625</td>\n",
                     "      <td>1.000000</td>\n",
                     "      <td>0.288425</td>\n",
                     "      <td>0.447717</td>\n",
                     "      <td>3.89</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_balanced_types</td>\n",
                     "      <td>0.8225</td>\n",
                     "      <td>0.625</td>\n",
                     "      <td>1.000000</td>\n",
                     "      <td>0.288425</td>\n",
                     "      <td>0.447717</td>\n",
                     "      <td>3.77</td>\n",
                     "    </tr>\n",
                     "    <tr>\n",
                     "      <th>0</th>\n",
                     "      <td>content_title_balanced_types</td>\n",
                     "      <td>0.8215</td>\n",
                     "      <td>0.615</td>\n",
                     "      <td>1.000000</td>\n",
                     "      <td>0.269450</td>\n",
                     "      <td>0.424514</td>\n",
                     "      <td>4.41</td>\n",
                     "    </tr>\n",
                     "  </tbody>\n",
                     "</table>\n",
                     "</div>"
                  ],
                  "text/plain": [
                     "                                          name  train_acc  test_acc  \\\n",
                     "0                 content_domain_reliable_fake     0.9965     0.934   \n",
                     "0   content_domain_authors_title_reliable_fake     0.9970     0.929   \n",
                     "0                content_authors_reliable_fake     0.9955     0.903   \n",
                     "0                        content_reliable_fake     0.9955     0.898   \n",
                     "0                  content_title_reliable_fake     0.9955     0.896   \n",
                     "0                                    hyper_all     1.0000     0.867   \n",
                     "0                                    hyper_all     1.0000     0.867   \n",
                     "0    content_domain_authors_title_balanced_bin     0.9960     0.841   \n",
                     "0      content_domain_authors_title_unbalanced     0.9960     0.838   \n",
                     "0                  content_domain_balanced_bin     0.9950     0.830   \n",
                     "0                    content_domain_unbalanced     0.9950     0.825   \n",
                     "0                 content_authors_balanced_bin     0.9955     0.818   \n",
                     "0                   content_authors_unbalanced     0.9945     0.813   \n",
                     "0                   content_title_balanced_bin     0.9940     0.803   \n",
                     "0                     content_title_unbalanced     0.9940     0.793   \n",
                     "0                         content_balanced_bin     0.9945     0.794   \n",
                     "0                           content_unbalanced     0.9945     0.785   \n",
                     "0                                hyper_content     1.0000     0.793   \n",
                     "0                content_domain_balanced_types     0.8370     0.628   \n",
                     "0               content_authors_balanced_types     0.8385     0.626   \n",
                     "0  content_domain_authors_title_balanced_types     0.8495     0.625   \n",
                     "0                       content_balanced_types     0.8225     0.625   \n",
                     "0                 content_title_balanced_types     0.8215     0.615   \n",
                     "\n",
                     "   precision    recall        f1  time  \n",
                     "0   0.979332  0.920777  0.949153  4.29  \n",
                     "0   0.985390  0.907324  0.944747  4.51  \n",
                     "0   0.976667  0.875934  0.923562  3.97  \n",
                     "0   0.970149  0.874439  0.919811  3.21  \n",
                     "0   0.976391  0.865471  0.917591  4.10  \n",
                     "0   0.881553  0.863118  0.872238  8.65  \n",
                     "0   0.881553  0.863118  0.872238  8.21  \n",
                     "0   0.854127  0.842803  0.848427  4.34  \n",
                     "0   0.838290  0.857414  0.847744  5.14  \n",
                     "0   0.836466  0.842803  0.839623  4.29  \n",
                     "0   0.812834  0.866920  0.839006  4.42  \n",
                     "0   0.839216  0.810606  0.824663  4.06  \n",
                     "0   0.820416  0.825095  0.822749  4.10  \n",
                     "0   0.820116  0.803030  0.811483  3.76  \n",
                     "0   0.791590  0.823194  0.807083  5.74  \n",
                     "0   0.808429  0.799242  0.803810  4.24  \n",
                     "0   0.780180  0.823194  0.801110  5.57  \n",
                     "0   0.834382  0.756654  0.793619  9.57  \n",
                     "0   1.000000  0.294118  0.454545  4.29  \n",
                     "0   0.993548  0.292220  0.451613  3.39  \n",
                     "0   1.000000  0.288425  0.447717  3.89  \n",
                     "0   1.000000  0.288425  0.447717  3.77  \n",
                     "0   1.000000  0.269450  0.424514  4.41  "
                  ]
               },
               "execution_count": 273,
               "metadata": {},
               "output_type": "execute_result"
            }
         ],
         "source": [
            "tests.metrics.sort_values(by=\"f1\", ascending=False)"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "penguin",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.10.9"
      },
      "orig_nbformat": 4
   },
   "nbformat": 4,
   "nbformat_minor": 2
}