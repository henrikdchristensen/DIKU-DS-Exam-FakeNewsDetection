{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel fakenews (Python 3.11.0) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel fakenews (Python 3.11.0) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/shuffled.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:00<00:00, 478143.29it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 500210.08it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 505946.19it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 559480.36it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 547053.64it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 485774.86it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 531737.31it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 579723.72it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 545575.68it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 560786.81it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 523044.14it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 513207.53it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 560848.67it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 572232.50it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 576521.18it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 561715.83it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 560417.41it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 561837.72it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 595919.37it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 567563.28it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 568014.85it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 563827.46it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 550842.75it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 595686.20it/s]\n",
      "100%|██████████| 200000/200000 [00:00<00:00, 660695.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 5000000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "                                   out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 768849.38it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 539.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 18019.53it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1205.96it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 161.14it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 82612.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 169198.84it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 15205.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 124929.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 51255.62it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6579.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 497940.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 167259.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 38457.73it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37036.88it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37947.78it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 26879.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 97.20679092407227\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector: content_count (data read in 131.38808488845825 seconds)\n",
      "Saved content_count vectors in 411.59032940864563 seconds\n",
      "Creating vector: content_count_balanced_types (data read in 82.55208921432495 seconds)\n",
      "Saved content_count_balanced_types vectors in 277.50120639801025 seconds\n",
      "Creating vector: content_count_balanced_bin (data read in 118.06746077537537 seconds)\n",
      "Saved content_count_balanced_bin vectors in 432.63299679756165 seconds\n",
      "Creating vector: content_count_reliable_fake (data read in 130.97999382019043 seconds)\n",
      "Saved content_count_reliable_fake vectors in 436.5514030456543 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count finished in 371.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count_balanced_types finished in 214.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count_balanced_bin finished in 355.67 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count_reliable_fake finished in 369.90 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>model</th>\n",
       "      <th>vectorizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.87215</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.858969</td>\n",
       "      <td>0.874852</td>\n",
       "      <td>370.58</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>0.900223</td>\n",
       "      <td>0.87024</td>\n",
       "      <td>0.896534</td>\n",
       "      <td>0.848402</td>\n",
       "      <td>0.871804</td>\n",
       "      <td>354.43</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>0.916062</td>\n",
       "      <td>0.83759</td>\n",
       "      <td>0.912774</td>\n",
       "      <td>0.762503</td>\n",
       "      <td>0.830899</td>\n",
       "      <td>213.71</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>0.977344</td>\n",
       "      <td>0.71353</td>\n",
       "      <td>0.810682</td>\n",
       "      <td>0.587953</td>\n",
       "      <td>0.681583</td>\n",
       "      <td>368.65</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name  train_acc  val_acc  precision    recall  \\\n",
       "0                 content_count   0.900499  0.87215   0.891333  0.858969   \n",
       "0    content_count_balanced_bin   0.900223  0.87024   0.896534  0.848402   \n",
       "0  content_count_balanced_types   0.916062  0.83759   0.912774  0.762503   \n",
       "0   content_count_reliable_fake   0.977344  0.71353   0.810682  0.587953   \n",
       "\n",
       "         f1    time                             model         vectorizer  \n",
       "0  0.874852  370.58  LogisticRegression(max_iter=300)  CountVectorizer()  \n",
       "0  0.871804  354.43  LogisticRegression(max_iter=300)  CountVectorizer()  \n",
       "0  0.830899  213.71  LogisticRegression(max_iter=300)  CountVectorizer()  \n",
       "0  0.681583  368.65  LogisticRegression(max_iter=300)  CountVectorizer()  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_count\"),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_count_balanced_types\"),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_count_balanced_bin\"),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_count_reliable_fake\"),\n",
    "    #(unbalanced, \"content_title\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_title_count\"),\n",
    "    #(unbalanced, \"content_domain\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_domain_count\"),\n",
    "    #(unbalanced, \"content_authors\", mt.create_count_vector, LogisticRegression(max_iter=300), \"content_authors_count\"),\n",
    "    #(unbalanced, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=300), \"all_count\"),\n",
    "    #(unbalanced, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=300, C=250), \"all_count_hyper\"),\n",
    "    #(balanced_types, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=350), \"all_count_balanced_types\"),\n",
    "    #(balanced_types, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=300, C=250), \"all_count_hyper_balanced_types\"),\n",
    "    #(balanced_bin, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=300, C=250), \"all_count_hyper_balanced_bin\"),\n",
    "    #(balanced_reliable_fake, \"content_domain_authors_title\", mt.create_count_vector, LogisticRegression(max_iter=300, C=250), \"all_count_hyper_balanced_reliable_fake\"),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "vectorizers = mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.add_vectorizer_col(vectorizers)\n",
    "test_stats_simple.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count finished in 385.17 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[249], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m info_list \u001b[39m=\u001b[39m [\n\u001b[0;32m      9\u001b[0m     (unbalanced, \u001b[39m\"\u001b[39m\u001b[39mcontent_combined\u001b[39m\u001b[39m\"\u001b[39m, mt\u001b[39m.\u001b[39mcreate_count_vector, LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mcontent_count\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     10\u001b[0m     (balanced_types, \u001b[39m\"\u001b[39m\u001b[39mcontent_combined\u001b[39m\u001b[39m\"\u001b[39m, mt\u001b[39m.\u001b[39mcreate_count_vector, LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m700\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mcontent_count_balanced_types\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     11\u001b[0m     (balanced_bin, \u001b[39m\"\u001b[39m\u001b[39mcontent_combined\u001b[39m\u001b[39m\"\u001b[39m, mt\u001b[39m.\u001b[39mcreate_count_vector, LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m700\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mcontent_count_balanced_bin\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     12\u001b[0m     (balanced_reliable_fake, \u001b[39m\"\u001b[39m\u001b[39mcontent_combined\u001b[39m\u001b[39m\"\u001b[39m, mt\u001b[39m.\u001b[39mcreate_count_vector, LogisticRegression(max_iter\u001b[39m=\u001b[39m\u001b[39m700\u001b[39m, C\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m), \u001b[39m\"\u001b[39m\u001b[39mcontent_count_reliable_fake\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m test_stats_hyper \u001b[39m=\u001b[39m mt\u001b[39m.\u001b[39mTest_statistic()\n\u001b[1;32m---> 17\u001b[0m mt\u001b[39m.\u001b[39;49mtest_vectors_from_infolist(\u001b[39m\"\u001b[39;49m\u001b[39m../datasets/sample/dataset_count_vectors.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m, info_list, tests\u001b[39m=\u001b[39;49mtest_stats_hyper)\n\u001b[0;32m     18\u001b[0m test_stats_hyper\u001b[39m.\u001b[39madd_vectorizer_col(vectorizers)\n\u001b[0;32m     19\u001b[0m test_stats_hyper\u001b[39m.\u001b[39mmetrics\u001b[39m.\u001b[39msort_values(by\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mf1\u001b[39m\u001b[39m\"\u001b[39m, ascending\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\model_tests.py:227\u001b[0m, in \u001b[0;36mtest_vectors_from_infolist\u001b[1;34m(from_file, info_list, tests, use_standard)\u001b[0m\n\u001b[0;32m    225\u001b[0m         y_train, y_val, _ \u001b[39m=\u001b[39m (pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f))\n\u001b[0;32m    226\u001b[0m         X_train, X_val, _ \u001b[39m=\u001b[39m (pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f), pickle\u001b[39m.\u001b[39mload(f))\n\u001b[1;32m--> 227\u001b[0m         tests\u001b[39m.\u001b[39;49mtest_baseline(X_train, X_val, y_train, y_val, model, name\u001b[39m=\u001b[39;49mtest_name)\n\u001b[0;32m    228\u001b[0m \u001b[39mreturn\u001b[39;00m tests\n",
      "File \u001b[1;32mc:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\model_tests.py:171\u001b[0m, in \u001b[0;36mTest_statistic.test_baseline\u001b[1;34m(self, X_train, X_val, y_train, y_val, model, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtest_baseline\u001b[39m(\u001b[39mself\u001b[39m, X_train, X_val, y_train, y_val, model, name\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 171\u001b[0m     metric \u001b[39m=\u001b[39m try_models([model], X_train, X_val, y_train, y_val, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    172\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetrics, metric])\n",
      "File \u001b[1;32mc:\\Users\\madsv\\Documents\\Documents\\University\\DataScience\\FakeNews\\src\\model_tests.py:128\u001b[0m, in \u001b[0;36mtry_models\u001b[1;34m(models, X_train, X_val, y_train, y_val, name, predict_only)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39mfor\u001b[39;00m model \u001b[39min\u001b[39;00m models:\n\u001b[0;32m    127\u001b[0m     start_time \u001b[39m=\u001b[39m time() \n\u001b[1;32m--> 128\u001b[0m     model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m    129\u001b[0m     train_time \u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    130\u001b[0m     y_train_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mpredict(X_train)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1291\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1288\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1289\u001b[0m     n_threads \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m-> 1291\u001b[0m fold_coefs_ \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose, prefer\u001b[39m=\u001b[39;49mprefer)(\n\u001b[0;32m   1292\u001b[0m     path_func(\n\u001b[0;32m   1293\u001b[0m         X,\n\u001b[0;32m   1294\u001b[0m         y,\n\u001b[0;32m   1295\u001b[0m         pos_class\u001b[39m=\u001b[39;49mclass_,\n\u001b[0;32m   1296\u001b[0m         Cs\u001b[39m=\u001b[39;49m[C_],\n\u001b[0;32m   1297\u001b[0m         l1_ratio\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ml1_ratio,\n\u001b[0;32m   1298\u001b[0m         fit_intercept\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_intercept,\n\u001b[0;32m   1299\u001b[0m         tol\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol,\n\u001b[0;32m   1300\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m   1301\u001b[0m         solver\u001b[39m=\u001b[39;49msolver,\n\u001b[0;32m   1302\u001b[0m         multi_class\u001b[39m=\u001b[39;49mmulti_class,\n\u001b[0;32m   1303\u001b[0m         max_iter\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_iter,\n\u001b[0;32m   1304\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m   1305\u001b[0m         check_input\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m   1306\u001b[0m         random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state,\n\u001b[0;32m   1307\u001b[0m         coef\u001b[39m=\u001b[39;49mwarm_start_coef_,\n\u001b[0;32m   1308\u001b[0m         penalty\u001b[39m=\u001b[39;49mpenalty,\n\u001b[0;32m   1309\u001b[0m         max_squared_sum\u001b[39m=\u001b[39;49mmax_squared_sum,\n\u001b[0;32m   1310\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1311\u001b[0m         n_threads\u001b[39m=\u001b[39;49mn_threads,\n\u001b[0;32m   1312\u001b[0m     )\n\u001b[0;32m   1313\u001b[0m     \u001b[39mfor\u001b[39;49;00m class_, warm_start_coef_ \u001b[39min\u001b[39;49;00m \u001b[39mzip\u001b[39;49m(classes_, warm_start_coef)\n\u001b[0;32m   1314\u001b[0m )\n\u001b[0;32m   1316\u001b[0m fold_coefs_, _, n_iter_ \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mfold_coefs_)\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_iter_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(n_iter_, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint32)[:, \u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\parallel.py:1085\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1077\u001b[0m     \u001b[39m# Only set self._iterating to True if at least a batch\u001b[39;00m\n\u001b[0;32m   1078\u001b[0m     \u001b[39m# was dispatched. In particular this covers the edge\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1082\u001b[0m     \u001b[39m# was very quick and its callback already dispatched all the\u001b[39;00m\n\u001b[0;32m   1083\u001b[0m     \u001b[39m# remaining jobs.\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 1085\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1088\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\utils\\parallel.py:123\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    121\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    122\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:450\u001b[0m, in \u001b[0;36m_logistic_regression_path\u001b[1;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[0;32m    446\u001b[0m l2_reg_strength \u001b[39m=\u001b[39m \u001b[39m1.0\u001b[39m \u001b[39m/\u001b[39m C\n\u001b[0;32m    447\u001b[0m iprint \u001b[39m=\u001b[39m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m50\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m101\u001b[39m][\n\u001b[0;32m    448\u001b[0m     np\u001b[39m.\u001b[39msearchsorted(np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]), verbose)\n\u001b[0;32m    449\u001b[0m ]\n\u001b[1;32m--> 450\u001b[0m opt_res \u001b[39m=\u001b[39m optimize\u001b[39m.\u001b[39;49mminimize(\n\u001b[0;32m    451\u001b[0m     func,\n\u001b[0;32m    452\u001b[0m     w0,\n\u001b[0;32m    453\u001b[0m     method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mL-BFGS-B\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    454\u001b[0m     jac\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m    455\u001b[0m     args\u001b[39m=\u001b[39;49m(X, target, sample_weight, l2_reg_strength, n_threads),\n\u001b[0;32m    456\u001b[0m     options\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39miprint\u001b[39;49m\u001b[39m\"\u001b[39;49m: iprint, \u001b[39m\"\u001b[39;49m\u001b[39mgtol\u001b[39;49m\u001b[39m\"\u001b[39;49m: tol, \u001b[39m\"\u001b[39;49m\u001b[39mmaxiter\u001b[39;49m\u001b[39m\"\u001b[39;49m: max_iter},\n\u001b[0;32m    457\u001b[0m )\n\u001b[0;32m    458\u001b[0m n_iter_i \u001b[39m=\u001b[39m _check_optimize_result(\n\u001b[0;32m    459\u001b[0m     solver,\n\u001b[0;32m    460\u001b[0m     opt_res,\n\u001b[0;32m    461\u001b[0m     max_iter,\n\u001b[0;32m    462\u001b[0m     extra_warning_msg\u001b[39m=\u001b[39m_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n\u001b[0;32m    463\u001b[0m )\n\u001b[0;32m    464\u001b[0m w0, loss \u001b[39m=\u001b[39m opt_res\u001b[39m.\u001b[39mx, opt_res\u001b[39m.\u001b[39mfun\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_minimize.py:696\u001b[0m, in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    693\u001b[0m     res \u001b[39m=\u001b[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[0;32m    694\u001b[0m                              \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    695\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39ml-bfgs-b\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m--> 696\u001b[0m     res \u001b[39m=\u001b[39m _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0;32m    697\u001b[0m                            callback\u001b[39m=\u001b[39mcallback, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n\u001b[0;32m    698\u001b[0m \u001b[39melif\u001b[39;00m meth \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtnc\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    699\u001b[0m     res \u001b[39m=\u001b[39m _minimize_tnc(fun, x0, args, jac, bounds, callback\u001b[39m=\u001b[39mcallback,\n\u001b[0;32m    700\u001b[0m                         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39moptions)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_lbfgsb_py.py:359\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[1;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[0;32m    353\u001b[0m task_str \u001b[39m=\u001b[39m task\u001b[39m.\u001b[39mtobytes()\n\u001b[0;32m    354\u001b[0m \u001b[39mif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mFG\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    355\u001b[0m     \u001b[39m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[0;32m    356\u001b[0m     \u001b[39m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[0;32m    357\u001b[0m     \u001b[39m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m     \u001b[39m# Overwrite f and g:\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m     f, g \u001b[39m=\u001b[39m func_and_grad(x)\n\u001b[0;32m    360\u001b[0m \u001b[39melif\u001b[39;00m task_str\u001b[39m.\u001b[39mstartswith(\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m\u001b[39mNEW_X\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m    361\u001b[0m     \u001b[39m# new iteration\u001b[39;00m\n\u001b[0;32m    362\u001b[0m     n_iterations \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:285\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39marray_equal(x, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx):\n\u001b[0;32m    284\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m--> 285\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun()\n\u001b[0;32m    286\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_grad()\n\u001b[0;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mg\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001b[0m, in \u001b[0;36mScalarFunction._update_fun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_update_fun\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    250\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated:\n\u001b[1;32m--> 251\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update_fun_impl()\n\u001b[0;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf_updated \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_fun\u001b[1;34m()\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_fun\u001b[39m():\n\u001b[1;32m--> 155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf \u001b[39m=\u001b[39m fun_wrapped(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnfev \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    134\u001b[0m \u001b[39m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[0;32m    135\u001b[0m \u001b[39m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m fx \u001b[39m=\u001b[39m fun(np\u001b[39m.\u001b[39;49mcopy(x), \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m    138\u001b[0m \u001b[39m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39misscalar(fx):\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_optimize.py:76\u001b[0m, in \u001b[0;36mMemoizeJac.__call__\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, x, \u001b[39m*\u001b[39margs):\n\u001b[0;32m     75\u001b[0m     \u001b[39m\"\"\" returns the function value \"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_compute_if_needed(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\optimize\\_optimize.py:70\u001b[0m, in \u001b[0;36mMemoizeJac._compute_if_needed\u001b[1;34m(self, x, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39mall(x \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(x)\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m---> 70\u001b[0m     fg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfun(x, \u001b[39m*\u001b[39;49margs)\n\u001b[0;32m     71\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjac \u001b[39m=\u001b[39m fg[\u001b[39m1\u001b[39m]\n\u001b[0;32m     72\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value \u001b[39m=\u001b[39m fg[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:274\u001b[0m, in \u001b[0;36mLinearModelLoss.loss_gradient\u001b[1;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[0;32m    271\u001b[0m n_dof \u001b[39m=\u001b[39m n_features \u001b[39m+\u001b[39m \u001b[39mint\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_intercept)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m raw_prediction \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m     weights, intercept, raw_prediction \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight_intercept_raw(coef, X)\n\u001b[0;32m    275\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    276\u001b[0m     weights, intercept \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_intercept(coef)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\linear_model\\_linear_loss.py:162\u001b[0m, in \u001b[0;36mLinearModelLoss.weight_intercept_raw\u001b[1;34m(self, coef, X)\u001b[0m\n\u001b[0;32m    159\u001b[0m weights, intercept \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight_intercept(coef)\n\u001b[0;32m    161\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_loss\u001b[39m.\u001b[39mis_multiclass:\n\u001b[1;32m--> 162\u001b[0m     raw_prediction \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39;49m weights \u001b[39m+\u001b[39m intercept\n\u001b[0;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[39m# weights has shape (n_classes, n_dof)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     raw_prediction \u001b[39m=\u001b[39m X \u001b[39m@\u001b[39m weights\u001b[39m.\u001b[39mT \u001b[39m+\u001b[39m intercept  \u001b[39m# ndarray, likely C-contiguous\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\sparse\\_base.py:630\u001b[0m, in \u001b[0;36mspmatrix.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m isscalarlike(other):\n\u001b[0;32m    628\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mScalar operands are not allowed, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    629\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39muse \u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m'\u001b[39m\u001b[39m instead\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 630\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_dispatch(other)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\sparse\\_base.py:528\u001b[0m, in \u001b[0;36mspmatrix._mul_dispatch\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    525\u001b[0m \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m \u001b[39mis\u001b[39;00m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    526\u001b[0m     \u001b[39m# Fast path for the most common case\u001b[39;00m\n\u001b[0;32m    527\u001b[0m     \u001b[39mif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N,):\n\u001b[1;32m--> 528\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mul_vector(other)\n\u001b[0;32m    529\u001b[0m     \u001b[39melif\u001b[39;00m other\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (N, \u001b[39m1\u001b[39m):\n\u001b[0;32m    530\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mul_vector(other\u001b[39m.\u001b[39mravel())\u001b[39m.\u001b[39mreshape(M, \u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\scipy\\sparse\\_compressed.py:489\u001b[0m, in \u001b[0;36m_cs_matrix._mul_vector\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[39m# csr_matvec or csc_matvec\u001b[39;00m\n\u001b[0;32m    488\u001b[0m fn \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(_sparsetools, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m_matvec\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m--> 489\u001b[0m fn(M, N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, other, result)\n\u001b[0;32m    491\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=300, C=300), \"content_count\"),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=700, C=300), \"content_count_balanced_types\"),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=700, C=300), \"content_count_balanced_bin\"),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, LogisticRegression(max_iter=700, C=300), \"content_count_reliable_fake\"),\n",
    "]\n",
    "\n",
    "test_stats_hyper = mt.Test_statistic()\n",
    "\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_hyper)\n",
    "test_stats_hyper.add_vectorizer_col(vectorizers)\n",
    "test_stats_hyper.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "importlib.reload(pp)\n",
    "\n",
    "def test_on_liar(test, file):\n",
    "    liar_data = pp.apply_pipeline_pd_tqdm(pd.read_csv(file), [(pp.Binary_labels_LIAR(), 'label', 'type_binary')])\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for row in info_list:\n",
    "        model_name = row[-1]\n",
    "        model = test.metrics[test.metrics[\"name\"] == model_name][\"model\"].values[0]\n",
    "        vectorizer = test.metrics[test.metrics[\"name\"] == model_name][\"vectorizer\"].values[0]\n",
    "        X = vectorizer.transform(liar_data[\"statement_combined\"].values)\n",
    "        #print(liar_data[\"type_binary\"].astype(int).value_counts())\n",
    "        metrics = pd.concat([mt.get_predict_metrics(model, X, liar_data[\"type_binary\"].astype(int), name=model_name), metrics])\n",
    "\n",
    "        \n",
    "    return metrics.sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12791/12791 [00:00<00:00, 799221.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.212363</td>\n",
       "      <td>0.311280</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>0.462278</td>\n",
       "      <td>0.585447</td>\n",
       "      <td>0.122932</td>\n",
       "      <td>0.203197</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>0.596056</td>\n",
       "      <td>0.114382</td>\n",
       "      <td>0.191932</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>0.450004</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.062375</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name       acc  precision    recall        f1  \\\n",
       "0                 content_count  0.475881   0.582692  0.212363  0.311280   \n",
       "0    content_count_balanced_bin  0.462278   0.585447  0.122932  0.203197   \n",
       "0   content_count_reliable_fake  0.462825   0.596056  0.114382  0.191932   \n",
       "0  content_count_balanced_types  0.450004   0.634146  0.032801  0.062375   \n",
       "\n",
       "                              model  \n",
       "0  LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(max_iter=300)  "
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = test_on_liar(test_stats_simple, \"../datasets/big/combined_cleaned.csv\")\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12791/12791 [00:00<00:00, 752181.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.587971</td>\n",
       "      <td>0.201430</td>\n",
       "      <td>0.300063</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=400)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>0.460636</td>\n",
       "      <td>0.573300</td>\n",
       "      <td>0.128820</td>\n",
       "      <td>0.210370</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=400)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>0.458369</td>\n",
       "      <td>0.579109</td>\n",
       "      <td>0.105691</td>\n",
       "      <td>0.178758</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=400)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>0.453835</td>\n",
       "      <td>0.608504</td>\n",
       "      <td>0.058172</td>\n",
       "      <td>0.106192</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=400)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name       acc  precision    recall        f1  \\\n",
       "0                 content_count  0.475881   0.587971  0.201430  0.300063   \n",
       "0    content_count_balanced_bin  0.460636   0.573300  0.128820  0.210370   \n",
       "0   content_count_reliable_fake  0.458369   0.579109  0.105691  0.178758   \n",
       "0  content_count_balanced_types  0.453835   0.608504  0.058172  0.106192   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=250, max_iter=400)  \n",
       "0  LogisticRegression(C=250, max_iter=400)  \n",
       "0  LogisticRegression(C=250, max_iter=400)  \n",
       "0  LogisticRegression(C=250, max_iter=400)  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = test_on_liar(test_stats_hyper, \"../datasets/big/combined_cleaned.csv\")\n",
    "metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, grid, \"content_count\"),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 300, 'max_iter': 500}"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)[\"model\"].values[0].best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_hyper_tfidf_vecto...</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.896296</td>\n",
       "      <td>0.883212</td>\n",
       "      <td>0.889706</td>\n",
       "      <td>16.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_tfidf_vector_bigram</td>\n",
       "      <td>0.98700</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.852518</td>\n",
       "      <td>0.864964</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>9.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_tfidf_vector</td>\n",
       "      <td>0.96275</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.855072</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.858182</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_authors_title_count_vector</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.871698</td>\n",
       "      <td>0.843066</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>3.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_count_vector</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.880309</td>\n",
       "      <td>0.832117</td>\n",
       "      <td>0.855535</td>\n",
       "      <td>3.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_tfidf_vector_bigram</td>\n",
       "      <td>0.98400</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>0.861314</td>\n",
       "      <td>0.848921</td>\n",
       "      <td>9.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_tfidf_vector</td>\n",
       "      <td>0.95625</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.843636</td>\n",
       "      <td>0.846715</td>\n",
       "      <td>0.845173</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_tfidf_vector_bigram</td>\n",
       "      <td>0.98350</td>\n",
       "      <td>0.822</td>\n",
       "      <td>0.833935</td>\n",
       "      <td>0.843066</td>\n",
       "      <td>0.838475</td>\n",
       "      <td>9.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_tfidf_vector</td>\n",
       "      <td>0.94700</td>\n",
       "      <td>0.816</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.829630</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_tfidf_vector_bigram</td>\n",
       "      <td>0.98050</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.821168</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>9.51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_tfidf_vector</td>\n",
       "      <td>0.93825</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.813187</td>\n",
       "      <td>0.810219</td>\n",
       "      <td>0.811700</td>\n",
       "      <td>0.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_tfidf_vector_bigram</td>\n",
       "      <td>0.98025</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.817518</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>9.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_count_vector</td>\n",
       "      <td>0.99975</td>\n",
       "      <td>0.798</td>\n",
       "      <td>0.841897</td>\n",
       "      <td>0.777372</td>\n",
       "      <td>0.808349</td>\n",
       "      <td>3.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_tfidf_vector</td>\n",
       "      <td>0.93475</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.809701</td>\n",
       "      <td>0.791971</td>\n",
       "      <td>0.800738</td>\n",
       "      <td>1.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_count_vector</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.826772</td>\n",
       "      <td>0.766423</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>3.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_combined_count_vector</td>\n",
       "      <td>0.99950</td>\n",
       "      <td>0.764</td>\n",
       "      <td>0.814516</td>\n",
       "      <td>0.737226</td>\n",
       "      <td>0.773946</td>\n",
       "      <td>3.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  train_acc  val_acc  \\\n",
       "0  content_domain_authors_title_hyper_tfidf_vecto...    1.00000    0.880   \n",
       "0   content_domain_authors_title_tfidf_vector_bigram    0.98700    0.844   \n",
       "0          content_domain_authors_title_tfidf_vector    0.96275    0.844   \n",
       "0          content_domain_authors_title_count_vector    1.00000    0.846   \n",
       "0                        content_domain_count_vector    1.00000    0.846   \n",
       "0                 content_domain_tfidf_vector_bigram    0.98400    0.832   \n",
       "0                        content_domain_tfidf_vector    0.95625    0.830   \n",
       "0                content_authors_tfidf_vector_bigram    0.98350    0.822   \n",
       "0                       content_authors_tfidf_vector    0.94700    0.816   \n",
       "0               content_combined_tfidf_vector_bigram    0.98050    0.800   \n",
       "0                         content_title_tfidf_vector    0.93825    0.794   \n",
       "0                  content_title_tfidf_vector_bigram    0.98025    0.792   \n",
       "0                       content_authors_count_vector    0.99975    0.798   \n",
       "0                      content_combined_tfidf_vector    0.93475    0.784   \n",
       "0                         content_title_count_vector    1.00000    0.784   \n",
       "0                      content_combined_count_vector    0.99950    0.764   \n",
       "\n",
       "   precision    recall        f1   time  \n",
       "0   0.896296  0.883212  0.889706  16.08  \n",
       "0   0.852518  0.864964  0.858696   9.38  \n",
       "0   0.855072  0.861314  0.858182   0.63  \n",
       "0   0.871698  0.843066  0.857143   3.35  \n",
       "0   0.880309  0.832117  0.855535   3.51  \n",
       "0   0.836879  0.861314  0.848921   9.72  \n",
       "0   0.843636  0.846715  0.845173   0.78  \n",
       "0   0.833935  0.843066  0.838475   9.36  \n",
       "0   0.842105  0.817518  0.829630   0.79  \n",
       "0   0.815217  0.821168  0.818182   9.51  \n",
       "0   0.813187  0.810219  0.811700   0.57  \n",
       "0   0.805755  0.817518  0.811594   9.26  \n",
       "0   0.841897  0.777372  0.808349   3.94  \n",
       "0   0.809701  0.791971  0.800738   1.94  \n",
       "0   0.826772  0.766423  0.795455   3.43  \n",
       "0   0.814516  0.737226  0.773946   3.55  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([test_stats_hyper.metrics, test_stats.metrics]).sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
    "    if tests == None:\n",
    "        tests = Test_statistic()\n",
    "    for file, name in files:\n",
    "        print(f\"Proccessing: {name}\")\n",
    "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "        data = pd.read_csv(file, usecols=cols_to_read)\n",
    "        print(\"Read data into dataframe\")\n",
    "\n",
    "        for col, entry_name in cols_to_test:\n",
    "            for func, model, func_name in vec_funcs:\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
    "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
    "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
    "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/sample/dataset_unbalanced_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.12475, conspiracy: 0.112, junksci: 0.0165, hate: 0.01025, unreliable: 0.03825, bias: 0.15625, satire: 0.01725, reliable: 0.257, clickbait: 0.03075, political: 0.237\n",
      "True: 2099, Fake: 1901\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.098, junksci: 0.01, hate: 0.008, unreliable: 0.044, bias: 0.174, satire: 0.01, reliable: 0.25, clickbait: 0.044, political: 0.254\n",
      "True: 274, Fake: 226\n",
      "Distribution of test with size 500:\n",
      "fake: 0.134, conspiracy: 0.128, junksci: 0.026, hate: 0.01, unreliable: 0.056, bias: 0.134, satire: 0.012, reliable: 0.244, clickbait: 0.034, political: 0.222\n",
      "True: 250, Fake: 250\n",
      "File: ../datasets/sample/dataset_balanced_types_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 1200, Fake: 2800\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.112, junksci: 0.006, hate: 0.008, unreliable: 0.05, bias: 0.166, satire: 0.014, reliable: 0.27, clickbait: 0.026, political: 0.24\n",
      "True: 268, Fake: 232\n",
      "Distribution of test with size 500:\n",
      "fake: 0.132, conspiracy: 0.096, junksci: 0.014, hate: 0.01, unreliable: 0.044, bias: 0.158, satire: 0.01, reliable: 0.244, clickbait: 0.022, political: 0.27\n",
      "True: 268, Fake: 232\n",
      "File: ../datasets/sample/dataset_balanced_bin_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.13175, conspiracy: 0.11775, junksci: 0.017, hate: 0.0105, unreliable: 0.0405, bias: 0.165, satire: 0.0175, reliable: 0.18275, clickbait: 0.022, political: 0.16425\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.088, conspiracy: 0.088, junksci: 0.016, hate: 0.008, unreliable: 0.034, bias: 0.138, satire: 0.014, reliable: 0.22, clickbait: 0.038, political: 0.202\n",
      "True: 307, Fake: 193\n",
      "Distribution of test with size 500:\n",
      "fake: 0.122, conspiracy: 0.096, junksci: 0.016, hate: 0.008, unreliable: 0.05, bias: 0.116, satire: 0.006, reliable: 0.214, clickbait: 0.026, political: 0.202\n",
      "True: 293, Fake: 207\n",
      "File: ../datasets/sample/dataset_reliable_fake_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.5, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.5, clickbait: 0.0, political: 0.0\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.306, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.694, clickbait: 0.0, political: 0.0\n",
      "True: 347, Fake: 153\n",
      "Distribution of test with size 500:\n",
      "fake: 0.302, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.698, clickbait: 0.0, political: 0.0\n",
      "True: 349, Fake: 151\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_cleaned.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "for file in [unbalanced, balanced_types, balanced_bin, balanced_reliable_fake]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbformat": 4,
   "nbformat_minor": 2
}