{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mKernel fakenews (Python 3.11.0) is not usable. Check the Jupyter output tab for more information. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/shuffled.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"thi\" in pd.read_csv(\"../datasets/big/dataset.csv\", nrows=2)[\"content_combined\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 416626.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 624877.69it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 499952.80it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 832872.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 625180.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 20000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 625035.99it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 624998.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 40000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_unbalanced.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 768849.38it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 539.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 18019.53it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1205.96it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 161.14it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 82612.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 169198.84it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 15205.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 124929.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 51255.62it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6579.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 497940.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 167259.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 38457.73it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37036.88it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37947.78it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 26879.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 97.20679092407227\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count finished in 8.55 seconds\n",
      "content_count_hyper finished in 5.22 seconds\n",
      "content_count_balanced_types finished in 4.85 seconds\n",
      "content_count_balanced_bin finished in 8.11 seconds\n",
      "content_count_reliable_fake finished in 6.42 seconds\n",
      "content_title_count finished in 8.76 seconds\n",
      "content_domain_count finished in 7.51 seconds\n",
      "content_authors_count finished in 8.47 seconds\n",
      "all_count finished in 7.38 seconds\n",
      "all_count_hyper_1 finished in 8.96 seconds\n",
      "all_count_hyper_2 finished in 4.70 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.918406</td>\n",
       "      <td>0.920152</td>\n",
       "      <td>0.919278</td>\n",
       "      <td>7.29</td>\n",
       "      <td>[[431, 43], [42, 484]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.912381</td>\n",
       "      <td>0.910646</td>\n",
       "      <td>0.911513</td>\n",
       "      <td>8.87</td>\n",
       "      <td>[[428, 46], [47, 479]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.903000</td>\n",
       "      <td>0.911708</td>\n",
       "      <td>0.903042</td>\n",
       "      <td>0.907354</td>\n",
       "      <td>4.61</td>\n",
       "      <td>[[428, 46], [51, 475]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>val</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.905123</td>\n",
       "      <td>0.906844</td>\n",
       "      <td>0.905983</td>\n",
       "      <td>7.42</td>\n",
       "      <td>[[424, 50], [49, 477]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.851000</td>\n",
       "      <td>0.845872</td>\n",
       "      <td>0.876426</td>\n",
       "      <td>0.860878</td>\n",
       "      <td>8.37</td>\n",
       "      <td>[[390, 84], [65, 461]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.809000</td>\n",
       "      <td>0.820268</td>\n",
       "      <td>0.815589</td>\n",
       "      <td>0.817922</td>\n",
       "      <td>8.66</td>\n",
       "      <td>[[380, 94], [97, 429]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.817143</td>\n",
       "      <td>0.815589</td>\n",
       "      <td>0.816365</td>\n",
       "      <td>5.12</td>\n",
       "      <td>[[378, 96], [97, 429]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>val</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.803000</td>\n",
       "      <td>0.802239</td>\n",
       "      <td>0.825336</td>\n",
       "      <td>0.813623</td>\n",
       "      <td>8.01</td>\n",
       "      <td>[[373, 106], [91, 430]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.805970</td>\n",
       "      <td>0.821293</td>\n",
       "      <td>0.813559</td>\n",
       "      <td>8.44</td>\n",
       "      <td>[[370, 104], [94, 432]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>val</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.764000</td>\n",
       "      <td>0.862319</td>\n",
       "      <td>0.666045</td>\n",
       "      <td>0.751579</td>\n",
       "      <td>4.77</td>\n",
       "      <td>[[407, 57], [179, 357]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.684000</td>\n",
       "      <td>0.753589</td>\n",
       "      <td>0.596591</td>\n",
       "      <td>0.665962</td>\n",
       "      <td>6.33</td>\n",
       "      <td>[[369, 103], [213, 315]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914000</td>\n",
       "      <td>0.911824</td>\n",
       "      <td>0.915493</td>\n",
       "      <td>0.913655</td>\n",
       "      <td>8.87</td>\n",
       "      <td>[[459, 44], [42, 455]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.907000</td>\n",
       "      <td>0.908907</td>\n",
       "      <td>0.903421</td>\n",
       "      <td>0.906155</td>\n",
       "      <td>7.29</td>\n",
       "      <td>[[458, 45], [48, 449]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>test</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.899598</td>\n",
       "      <td>0.901408</td>\n",
       "      <td>0.900503</td>\n",
       "      <td>7.42</td>\n",
       "      <td>[[453, 50], [49, 448]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.901000</td>\n",
       "      <td>0.904472</td>\n",
       "      <td>0.895372</td>\n",
       "      <td>0.899899</td>\n",
       "      <td>4.61</td>\n",
       "      <td>[[456, 47], [52, 445]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.819000</td>\n",
       "      <td>0.829167</td>\n",
       "      <td>0.800805</td>\n",
       "      <td>0.814739</td>\n",
       "      <td>8.37</td>\n",
       "      <td>[[421, 82], [99, 398]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.806000</td>\n",
       "      <td>0.808554</td>\n",
       "      <td>0.798793</td>\n",
       "      <td>0.803644</td>\n",
       "      <td>8.44</td>\n",
       "      <td>[[409, 94], [100, 397]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.807000</td>\n",
       "      <td>0.817992</td>\n",
       "      <td>0.786720</td>\n",
       "      <td>0.802051</td>\n",
       "      <td>5.12</td>\n",
       "      <td>[[416, 87], [106, 391]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.802000</td>\n",
       "      <td>0.804481</td>\n",
       "      <td>0.794769</td>\n",
       "      <td>0.799595</td>\n",
       "      <td>8.66</td>\n",
       "      <td>[[407, 96], [102, 395]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.798000</td>\n",
       "      <td>0.804481</td>\n",
       "      <td>0.788423</td>\n",
       "      <td>0.796371</td>\n",
       "      <td>8.01</td>\n",
       "      <td>[[403, 96], [106, 395]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>test</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.775000</td>\n",
       "      <td>0.878173</td>\n",
       "      <td>0.661568</td>\n",
       "      <td>0.754635</td>\n",
       "      <td>4.77</td>\n",
       "      <td>[[429, 48], [177, 346]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.695000</td>\n",
       "      <td>0.750594</td>\n",
       "      <td>0.612403</td>\n",
       "      <td>0.674493</td>\n",
       "      <td>6.33</td>\n",
       "      <td>[[379, 105], [200, 316]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.998500</td>\n",
       "      <td>0.480807</td>\n",
       "      <td>0.609410</td>\n",
       "      <td>0.192459</td>\n",
       "      <td>0.292532</td>\n",
       "      <td>8.44</td>\n",
       "      <td>[[4777, 880], [5761, 1373]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.998125</td>\n",
       "      <td>0.477132</td>\n",
       "      <td>0.613544</td>\n",
       "      <td>0.168909</td>\n",
       "      <td>0.264893</td>\n",
       "      <td>8.01</td>\n",
       "      <td>[[4898, 759], [5929, 1205]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.980500</td>\n",
       "      <td>0.475803</td>\n",
       "      <td>0.624927</td>\n",
       "      <td>0.150407</td>\n",
       "      <td>0.242458</td>\n",
       "      <td>5.12</td>\n",
       "      <td>[[5013, 644], [6061, 1073]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.474318</td>\n",
       "      <td>0.618360</td>\n",
       "      <td>0.150126</td>\n",
       "      <td>0.241597</td>\n",
       "      <td>8.66</td>\n",
       "      <td>[[4996, 661], [6063, 1071]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999625</td>\n",
       "      <td>0.470174</td>\n",
       "      <td>0.603358</td>\n",
       "      <td>0.146061</td>\n",
       "      <td>0.235188</td>\n",
       "      <td>6.33</td>\n",
       "      <td>[[4972, 685], [6092, 1042]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999375</td>\n",
       "      <td>0.465093</td>\n",
       "      <td>0.627622</td>\n",
       "      <td>0.100645</td>\n",
       "      <td>0.173472</td>\n",
       "      <td>8.37</td>\n",
       "      <td>[[5231, 426], [6416, 718]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>0.462044</td>\n",
       "      <td>0.625372</td>\n",
       "      <td>0.088450</td>\n",
       "      <td>0.154980</td>\n",
       "      <td>4.77</td>\n",
       "      <td>[[5279, 378], [6503, 631]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.448440</td>\n",
       "      <td>0.665272</td>\n",
       "      <td>0.022288</td>\n",
       "      <td>0.043130</td>\n",
       "      <td>7.42</td>\n",
       "      <td>[[5577, 80], [6975, 159]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.446642</td>\n",
       "      <td>0.637255</td>\n",
       "      <td>0.018223</td>\n",
       "      <td>0.035432</td>\n",
       "      <td>8.87</td>\n",
       "      <td>[[5583, 74], [7004, 130]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.445939</td>\n",
       "      <td>0.659864</td>\n",
       "      <td>0.013597</td>\n",
       "      <td>0.026645</td>\n",
       "      <td>7.29</td>\n",
       "      <td>[[5607, 50], [7037, 97]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.997500</td>\n",
       "      <td>0.444140</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.006728</td>\n",
       "      <td>0.013322</td>\n",
       "      <td>4.61</td>\n",
       "      <td>[[5633, 24], [7086, 48]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name split  train_acc       acc  precision  \\\n",
       "0                     all_count   val   1.000000  0.915000   0.918406   \n",
       "0             all_count_hyper_1   val   1.000000  0.907000   0.912381   \n",
       "0             all_count_hyper_2   val   0.997500  0.903000   0.911708   \n",
       "0          content_domain_count   val   1.000000  0.901000   0.905123   \n",
       "0         content_authors_count   val   0.999375  0.851000   0.845872   \n",
       "0           content_title_count   val   0.999375  0.809000   0.820268   \n",
       "0           content_count_hyper   val   0.980500  0.807000   0.817143   \n",
       "0    content_count_balanced_bin   val   0.998125  0.803000   0.802239   \n",
       "0                 content_count   val   0.998500  0.802000   0.805970   \n",
       "0  content_count_balanced_types   val   0.998200  0.764000   0.862319   \n",
       "0   content_count_reliable_fake   val   0.999625  0.684000   0.753589   \n",
       "1             all_count_hyper_1  test   1.000000  0.914000   0.911824   \n",
       "1                     all_count  test   1.000000  0.907000   0.908907   \n",
       "1          content_domain_count  test   1.000000  0.901000   0.899598   \n",
       "1             all_count_hyper_2  test   0.997500  0.901000   0.904472   \n",
       "1         content_authors_count  test   0.999375  0.819000   0.829167   \n",
       "1                 content_count  test   0.998500  0.806000   0.808554   \n",
       "1           content_count_hyper  test   0.980500  0.807000   0.817992   \n",
       "1           content_title_count  test   0.999375  0.802000   0.804481   \n",
       "1    content_count_balanced_bin  test   0.998125  0.798000   0.804481   \n",
       "1  content_count_balanced_types  test   0.998200  0.775000   0.878173   \n",
       "1   content_count_reliable_fake  test   0.999625  0.695000   0.750594   \n",
       "2                 content_count  liar   0.998500  0.480807   0.609410   \n",
       "2    content_count_balanced_bin  liar   0.998125  0.477132   0.613544   \n",
       "2           content_count_hyper  liar   0.980500  0.475803   0.624927   \n",
       "2           content_title_count  liar   0.999375  0.474318   0.618360   \n",
       "2   content_count_reliable_fake  liar   0.999625  0.470174   0.603358   \n",
       "2         content_authors_count  liar   0.999375  0.465093   0.627622   \n",
       "2  content_count_balanced_types  liar   0.998200  0.462044   0.625372   \n",
       "2          content_domain_count  liar   1.000000  0.448440   0.665272   \n",
       "2             all_count_hyper_1  liar   1.000000  0.446642   0.637255   \n",
       "2                     all_count  liar   1.000000  0.445939   0.659864   \n",
       "2             all_count_hyper_2  liar   0.997500  0.444140   0.666667   \n",
       "\n",
       "     recall        f1  time             confusion_matrix  \\\n",
       "0  0.920152  0.919278  7.29       [[431, 43], [42, 484]]   \n",
       "0  0.910646  0.911513  8.87       [[428, 46], [47, 479]]   \n",
       "0  0.903042  0.907354  4.61       [[428, 46], [51, 475]]   \n",
       "0  0.906844  0.905983  7.42       [[424, 50], [49, 477]]   \n",
       "0  0.876426  0.860878  8.37       [[390, 84], [65, 461]]   \n",
       "0  0.815589  0.817922  8.66       [[380, 94], [97, 429]]   \n",
       "0  0.815589  0.816365  5.12       [[378, 96], [97, 429]]   \n",
       "0  0.825336  0.813623  8.01      [[373, 106], [91, 430]]   \n",
       "0  0.821293  0.813559  8.44      [[370, 104], [94, 432]]   \n",
       "0  0.666045  0.751579  4.77      [[407, 57], [179, 357]]   \n",
       "0  0.596591  0.665962  6.33     [[369, 103], [213, 315]]   \n",
       "1  0.915493  0.913655  8.87       [[459, 44], [42, 455]]   \n",
       "1  0.903421  0.906155  7.29       [[458, 45], [48, 449]]   \n",
       "1  0.901408  0.900503  7.42       [[453, 50], [49, 448]]   \n",
       "1  0.895372  0.899899  4.61       [[456, 47], [52, 445]]   \n",
       "1  0.800805  0.814739  8.37       [[421, 82], [99, 398]]   \n",
       "1  0.798793  0.803644  8.44      [[409, 94], [100, 397]]   \n",
       "1  0.786720  0.802051  5.12      [[416, 87], [106, 391]]   \n",
       "1  0.794769  0.799595  8.66      [[407, 96], [102, 395]]   \n",
       "1  0.788423  0.796371  8.01      [[403, 96], [106, 395]]   \n",
       "1  0.661568  0.754635  4.77      [[429, 48], [177, 346]]   \n",
       "1  0.612403  0.674493  6.33     [[379, 105], [200, 316]]   \n",
       "2  0.192459  0.292532  8.44  [[4777, 880], [5761, 1373]]   \n",
       "2  0.168909  0.264893  8.01  [[4898, 759], [5929, 1205]]   \n",
       "2  0.150407  0.242458  5.12  [[5013, 644], [6061, 1073]]   \n",
       "2  0.150126  0.241597  8.66  [[4996, 661], [6063, 1071]]   \n",
       "2  0.146061  0.235188  6.33  [[4972, 685], [6092, 1042]]   \n",
       "2  0.100645  0.173472  8.37   [[5231, 426], [6416, 718]]   \n",
       "2  0.088450  0.154980  4.77   [[5279, 378], [6503, 631]]   \n",
       "2  0.022288  0.043130  7.42    [[5577, 80], [6975, 159]]   \n",
       "2  0.018223  0.035432  8.87    [[5583, 74], [7004, 130]]   \n",
       "2  0.013597  0.026645  7.29     [[5607, 50], [7037, 97]]   \n",
       "2  0.006728  0.013322  4.61     [[5633, 24], [7086, 48]]   \n",
       "\n",
       "                                     model  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count\"), \n",
    "                                                              (LogisticRegression(max_iter=300, C=0.1), \"content_count_hyper\")]),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_types\")]),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_bin\")]),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_reliable_fake\")]),\n",
    "\n",
    "    (unbalanced, \"content_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_title_count\")]),\n",
    "    (unbalanced, \"content_domain\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_domain_count\")]),\n",
    "    (unbalanced, \"content_authors\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_authors_count\")]),\n",
    "    (unbalanced, \"content_domain_authors_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"all_count\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=250), \"all_count_hyper_1\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=0.1), \"all_count_hyper_2\")]),\n",
    "    \n",
    "    # (unbalanced, \"content_combined\", mt.create_tdfidf_vector_unigram, [(LogisticRegression(max_iter=300), \"content_tfidf\"),\n",
    "    #                                                                    (LogisticRegression(max_iter=300, C=250), \"content_tfidf_hyper_1\"),\n",
    "    #                                                                    (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "    # (unbalanced, \"content_combined\", mt.create_tdfidf_vector_bigram, [(LogisticRegression(max_iter=300), \"content_tfidf_bi\")]),\n",
    "    # (unbalanced, \"content_combined\", mt.create_tdfidf_vector_trigram, [(LogisticRegression(max_iter=300), \"content_tfidf_tri\")]),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi0AAAGwCAYAAABl+VVyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAABYaUlEQVR4nO3deVxU1fsH8M/IMiLKsA7DlLkkkgSZYQIuiRu4IJoVFjZfKUPTEklIv9pmPxNy30glKlHEsDIrvyphmRYpoiQlai7lAskWDoMgDojz+8O8NQ4q2L0Oo593r/vKOfe59547pT4855x7ZQaDwQAiIiKiZq6FuTtARERE1BhMWoiIiMgiMGkhIiIii8CkhYiIiCwCkxYiIiKyCExaiIiIyCIwaSEiIiKLwKSFiIiILIK1uTsgBbtuL5u7C0TN0rmcRHN3gajZsbO5DdcQ6e+lmgN39+9hVlqIiIjIItyRlRYiIqJmRcYagRiYtBAREUlNJjN3D+4ITP2IiIikJmshzvYvJCQkQCaTISYmRmiLjIyETCYz2gICAoyO0+v1mDx5MlxdXWFvb4+wsDAUFhYaxWi1Wmg0GigUCigUCmg0GlRUVBjFnDlzBsOHD4e9vT1cXV0RHR2N2traJt0DkxYiIqI73L59+/D+++/joYceMtk3ePBgFBUVCdvWrVuN9sfExGDTpk1IT09HVlYWqqqqEBoaivr6eiEmIiICeXl5yMjIQEZGBvLy8qDRaIT99fX1GDZsGKqrq5GVlYX09HRs3LgRsbGxTboPDg8RERFJzYzDQ1VVVRgzZgySk5PxzjvvmOyXy+VQqVQNHqvT6fDhhx8iNTUVAwcOBACsW7cObdu2xTfffIOQkBAcOXIEGRkZyM7Ohr+/PwAgOTkZgYGBOHr0KLy8vJCZmYnDhw+joKAAarUaALBw4UJERkZizpw5cHBwaNS9sNJCREQkNZGGh/R6PSorK402vV5/w0u/9NJLGDZsmJB0XGvnzp1QKpXo3LkzoqKiUFpaKuzLzc1FXV0dgoODhTa1Wg0fHx/s3r0bALBnzx4oFAohYQGAgIAAKBQKoxgfHx8hYQGAkJAQ6PV65ObmNvprZNJCRERkIRISEoR5I1e3hISE68anp6fjp59+um7MkCFDkJaWhh07dmDhwoXYt28f+vfvLyRCxcXFsLW1hZOTk9Fx7u7uKC4uFmKUSqXJuZVKpVGMu7u70X4nJyfY2toKMY3B4SEiIiKpiTQ8NGPGDEydOtWoTS6XNxhbUFCAKVOmIDMzEy1btmwwZvTo0cKvfXx80L17d7Rr1w5btmzBqFGjrtsPg8EA2T/uSdbA/d1KzM2w0kJERCQ1kYaH5HI5HBwcjLbrJS25ubkoLS2Fn58frK2tYW1tjV27dmHZsmWwtrY2mkh7lYeHB9q1a4fjx48DAFQqFWpra6HVao3iSktLhcqJSqVCSUmJybnKysqMYq6tqGi1WtTV1ZlUYG6ESQsREdEdaMCAATh48CDy8vKErXv37hgzZgzy8vJgZWVlckx5eTkKCgrg4eEBAPDz84ONjQ22b98uxBQVFSE/Px89e/YEAAQGBkKn0yEnJ0eI2bt3L3Q6nVFMfn4+ioqKhJjMzEzI5XL4+fk1+p44PERERCQ1M6weatOmDXx8fIza7O3t4eLiAh8fH1RVVWHWrFl44okn4OHhgVOnTmHmzJlwdXXF448/DgBQKBQYN24cYmNj4eLiAmdnZ8TFxcHX11eY2NulSxcMHjwYUVFRSEpKAgCMHz8eoaGh8PLyAgAEBwfD29sbGo0G8+fPx7lz5xAXF4eoqKhGrxwCmLQQERFJrxk+xt/KygoHDx7E2rVrUVFRAQ8PD/Tr1w8bNmxAmzZthLjFixfD2toa4eHhqKmpwYABA5CSkmJUqUlLS0N0dLSwyigsLAyJiYlG19qyZQsmTZqEXr16wc7ODhEREViwYEGT+iwzGAyGf3nfzQ7f8kzUML7lmcjUbXnLc8+ZopynZne8KOexVKy0EBERSY3vHhIFkxYiIiKpNcPhIUvEpIWIiEhqrLSIgqkfERERWQRWWoiIiKTG4SFRMGkhIiKSGpMWUfBbJCIiIovASgsREZHUWnAirhiYtBAREUmNw0Oi4LdIREREFoGVFiIiIqnxOS2iYNJCREQkNQ4PiYLfIhEREVkEVlqIiIikxuEhUTBpISIikhqHh0TBpIWIiEhqrLSIgqkfERERWQRWWoiIiKTG4SFRMGkhIiKSGoeHRMHUj4iIiCwCKy1ERERS4/CQKJi0EBERSY3DQ6Jg6kdEREQWgZUWIiIiqXF4SBRMWoiIiKTGpEUU/BaJiIjIIrDSQkREJDVOxBUFkxYiIiKpcXhIFExaiIiIpMZKiyiY+hEREZFFYKWFiIhIahweEgWTFiIiIqlxeEgUTP2IiIjIIrDSQkREJDEZKy2iYKWFiIhIYjKZTJTt30hISIBMJkNMTIzQZjAYMGvWLKjVatjZ2SEoKAiHDh0yOk6v12Py5MlwdXWFvb09wsLCUFhYaBSj1Wqh0WigUCigUCig0WhQUVFhFHPmzBkMHz4c9vb2cHV1RXR0NGpra5t0D0xaiIiI7nD79u3D+++/j4ceesiofd68eVi0aBESExOxb98+qFQqDBo0COfPnxdiYmJisGnTJqSnpyMrKwtVVVUIDQ1FfX29EBMREYG8vDxkZGQgIyMDeXl50Gg0wv76+noMGzYM1dXVyMrKQnp6OjZu3IjY2Ngm3YfMYDAYbvE7aLbsur1s7i4QNUvnchLN3QWiZsfORvpr2D+1WpTznFsXAb1eb9Qml8shl8uve0xVVRUeeeQRrFixAu+88w4efvhhLFmyBAaDAWq1GjExMZg+fTqAK1UVd3d3zJ07FxMmTIBOp4ObmxtSU1MxevRoAMDZs2fRtm1bbN26FSEhIThy5Ai8vb2RnZ0Nf39/AEB2djYCAwPx66+/wsvLC9u2bUNoaCgKCgqgVqsBAOnp6YiMjERpaSkcHBwadf+stBAREUlMrOGhhIQEYQjm6paQkHDDa7/00ksYNmwYBg4caNR+8uRJFBcXIzg4WGiTy+Xo27cvdu/eDQDIzc1FXV2dUYxarYaPj48Qs2fPHigUCiFhAYCAgAAoFAqjGB8fHyFhAYCQkBDo9Xrk5uY2+nvkRFwiIiILMWPGDEydOtWo7UZVlvT0dPz000/Yt2+fyb7i4mIAgLu7u1G7u7s7Tp8+LcTY2trCycnJJObq8cXFxVAqlSbnVyqVRjHXXsfJyQm2trZCTGMwaSEiIpKYWKuHbjYU9E8FBQWYMmUKMjMz0bJly0b3zWAw3LS/18Y0FH8rMTfD4SEiIiKJmWP1UG5uLkpLS+Hn5wdra2tYW1tj165dWLZsGaytrYXKx7WVjtLSUmGfSqVCbW0ttFrtDWNKSkpMrl9WVmYUc+11tFot6urqTCowN8KkhYiISGLmSFoGDBiAgwcPIi8vT9i6d++OMWPGIC8vDx07doRKpcL27duFY2pra7Fr1y707NkTAODn5wcbGxujmKKiIuTn5wsxgYGB0Ol0yMnJEWL27t0LnU5nFJOfn4+ioiIhJjMzE3K5HH5+fo2+Jw4PERER3YHatGkDHx8fozZ7e3u4uLgI7TExMYiPj4enpyc8PT0RHx+PVq1aISIiAgCgUCgwbtw4xMbGwsXFBc7OzoiLi4Ovr68wsbdLly4YPHgwoqKikJSUBAAYP348QkND4eXlBQAIDg6Gt7c3NBoN5s+fj3PnziEuLg5RUVGNXjkEMGkhIiKSXjN9IO60adNQU1ODSZMmQavVwt/fH5mZmWjTpo0Qs3jxYlhbWyM8PBw1NTUYMGAAUlJSYGVlJcSkpaUhOjpaWGUUFhaGxMS/H7FgZWWFLVu2YNKkSejVqxfs7OwQERGBBQsWNKm/fE4L0V2Ez2khMnU7ntPiOGadKOepSHtWlPNYKs5pISIiIovA4SEiIiKJ8YWJ4mDSQkREJDEmLeLg8BARERFZBFZaiIiIJMZKiziYtBAREUmNOYsoODxEREREFoGVFiIiIolxeEgcTFqIiIgkxqRFHExaiIiIJMakRRyc00JEREQWgZUWIiIiqbHQIgomLURERBLj8JA4ODxEREREFoGVFiIiIomx0iIOJi1EREQSY9IiDg4PERERkUVgpYWIiEhirLSIg0kLERGR1JiziILDQ0RERGQRWGkhIiKSGIeHxNFsKi0//PADnn32WQQGBuKPP/4AAKSmpiIrK8vMPSMiIvp3ZDKZKNvdrlkkLRs3bkRISAjs7Oxw4MAB6PV6AMD58+cRHx9v5t4RERH9O0xaxNEskpZ33nkHq1atQnJyMmxsbIT2nj174qeffjJjz4iIiKi5aBZzWo4ePYrHHnvMpN3BwQEVFRW3v0NERERiYpFEFM2i0uLh4YETJ06YtGdlZaFjx45m6BEREZF4ODwkjmaRtEyYMAFTpkzB3r17IZPJcPbsWaSlpSEuLg6TJk0yd/fuKpGPB6LmQCLKflxo1P7+28+i5kCiyZb3+esm51C5OuD9t5/F6W8ToM1ejJwNMzB2ZGCD13Nzao33334WBTveRfnuRdi5JhZBPTo3GNvP3ws718SifPciFOx4F++//SzcnFr/+5smukWff/YpHvbxQuCj3a4bYzAY8PzYMXjYxwsJc/6vwZiP01IxcvhgPNrNB0ND+mPVikTU1dWZxJ0rL8cbr/0XQb39EdC9K/4zZjT2Zu8R7X6ImrtmMTw0bdo06HQ69OvXDxcvXsRjjz0GuVyOuLg4vPzyy+bu3l1D7aZAwiuP42xpBRxa25nsv1BTiyETlhm11eiN/2B1aN0S3370CmxtrPHaki9Q/Gclwgf7YdVbY6BobYdl63YIsbY21tiaFA3HNnZ4df5nKD1XhRdH98FXiS9h6MTlyMr9u/rW268Tvlw+CRlZ+XhqxRYonVvjnSkjsDUpGr3GzENt3SWRvw2iGyspKcGihXPhplSi6nzVdeM2fJyGgjOnr7s/OWklViQuxXPjxiOwZy8cyj+I95YvQWlpCd6cNVuIq62txfgXInG+shKv/vc1ODu7YEN6Gl568QWsSl6N7o/2EPX+SFyskoijWSQttbW1mDNnDl577TUcPnwYly9fhre3N1q3bo0///wTrq6u5u7iXWHZa08j66ffoNVV4/GBpj85XjYYkHPw1A3PMf6pPujY1g09I+biwJECAMA3e45A5arAGxOHYc0Xe6CrqgEARI4MhI+nGkFjF2LvLycBALv2H0POhhmInzISj/1ngXDehJiROH6mFM+8+iHq6y8DAE6dLcd3KbEYOzIAyZ9yaTzdXnP+7y34+XWHg8IR32R+3WDMH38UYtmShXgnfh6mxpj+AFZRocUH76/EqCfDER0zFQDwaA9/XLp0Ce8tX4IxmrG4//5OAIBNn3+KE8ePYc26dHR9uJsQG/7ECCxZNB/rPv5UojslMTBpEUezGB4KDw/H5cuX0apVK3Tv3h09evRA69atUVJSgqCgIHN3767w9NBH0cevE2LiN/yr8wR07YjiPyuFhOWqbT/ko3UrOYJ7eQttYf274ujJYiFhAYD6+sv4eOs+POrbHmo3BYArFaDuPu3x8ZYcIWEBgOyfT+LYqRKE9ev6r/pM1FRbNn+J3P05mPn6rBvGzZ71JgICe6H/wEEN7v8x6wfo9XqMGDnKqH3E46NgMBjw3bffCG07vvkG7Tt0EBIWALC2tsaw0DDkH/wFJSUlt35DRBaiWSQtRUVFGDdunElbUFAQHnjgATP16u7h5tQa8+OewBvLvsIfpRXXjbOT2+Dk9nhU7V+GExmzsXj6U3ByaGUUY2tj3eBQjb72SpuPp1po877fA/nHz5rE5h+78nDBLvd7XInrdOWYg8f/MI09/gce7KQ2aSeSyrnycsyfG4/oV2LhrlJdN+7zzz5Ffv4v+O9rb1w35rfjxwEAnp7G87jc3JRwcnLCiRPH/449cRyenb1MznG17bd/xFLzw4m44mgWScvWrVuRk5ODV155BQDwxx9/ICgoCL6+vvjkk0/M3Ls739KZo3H8dAne//SH68YcPPYHZizehHGvr0XYSyuQ+tVeaEYEYMfqqbC3sxXifv29CPcoHdFW5WR0fM9uV1aBOSvshTYXR3ucq7xgci3tX20ujldiXf46RqszjT2nuwBnRSuTdiKpxL/zNtq174Dw0RHXjbk63yVm6qtQKt2vG1ehq4CtrS3sWpn+P+ygUED3j0c+VFRUQOGgMIlTKK606XQVJvuoGZGJtN3lmsWcFhcXF3z99dfo3bs3AGDLli145JFHkJaWhhYtbpxX6fV64Qm6Vxku10PWwkqy/t5JRg54GEMf80HAM3NvGLc87Tujzzv2/oqfjxbi4wUv4PlRvYT9H37+I6Ke6oPVc8Zi8px0lJSfx1Mhfngy+BEAV+bF/JPhms/G+6793HDsDU5BJKpvtn+NXTt3IP2zL274U++c/3sLXl4P4Iknw296zhud59pdN4zl32h0F2gWlRYAuPfee7F9+3asX78ePXr0wMcffwwrq5snHgkJCVAoFEbbpZLc29Bjy2dvZ4vF/w3HyvTvUVSqg6K1HRSt7WBrcyWXVbS2Q6uWttc9/ssdP6Pqgh49fNsLbUdPlmB0bDLu83DGTxtfxx875yI2ciD+u2gTAODsP4afyiuqhSrKP10dcjqnq74S99e/nR1NY50VrYTKDJGULlyoRsI7/4dnIjRwc1OisrISlZWVwtLkyspK1Fy4gO2ZGdj94w+Imfoqzp8/L8QBQF1dndExjgpH6PV61NTUmFyvUqeDg8JR+Ozo6IiKBqopOp0OwJXKDDVf5hgeWrlyJR566CE4ODjAwcEBgYGB2LZtm7A/MjLS5PwBAQFG59Dr9Zg8eTJcXV1hb2+PsLAwFBYWGsVotVpoNBrh72CNRmPyYNgzZ85g+PDhsLe3h6urK6Kjo1FbW9u0LxFmrLQ4OTk1+B/gwoUL2Lx5M1xcXIS2c+fOXfc8M2bMwNSpU43alH2mi9fRO5iLY2uoXB0Q858BiPnPAJP9xT/Mx+bvfkb41OTrnkMmM62eZP54GJ2Hvon773ODtVULHD9dKlRasn76exnzoRNnG5yPcnXey+Hfiq78+8SVeS8+ndT4OuuwUeyDndQ4dMJ0XgyR2LRaLcrL/8TaNR9h7ZqPTPY/1vNRBPUfAC+vLrh06RI0EaZVls8/+wSff/YJFi19D/0HDESnzlfmspw4fgy+D/09ofzPP8ug1WrRqZOn0NbJszNOHDtmcs4Tx4/9td/TZB81H+aYj3Lvvffi3XffRadOV1agrVmzBiNGjMCBAwfw4IMPAgAGDx6M1atXC8fY2hr/oBoTE4PNmzcjPT0dLi4uiI2NRWhoKHJzc4XCQkREBAoLC5GRkQEAGD9+PDQaDTZv3gwAqK+vx7Bhw+Dm5oasrCyUl5dj7NixMBgMWL58eZPuyWxJy5IlS0Q5j1wuh1wuN2rj0FDjlJRXIviFpSbtcc8NQh+/Thjx8kqUV1z/+ROjBj4Mezs5cn451eD+386UAQBsrK3w0jNByPu1AD/+9Juw/6vvfsaymU/jUZ922Jd/5TkWVlYt8PTQR5Hzy0kUlV35CfJsmQ77Dp7C00N7YPHab3H58pUkqYdve3h1UCFx/c5buX2iJnF1dUPyR2tN2ld/+D5y9+9D4spkODk5wa5VqwafmRL1/H/Qr/9ARDz7HyHB6NW7D+RyOb764nOjpOWrLzZBJpOh34CBQlv/AQMR/87bOPjLz0LspUuXsOV/X8H3oa43nDtD5meOObTDhw83+jxnzhysXLkS2dnZQtIil8uhus6Ecp1Ohw8//BCpqakYOPDK/4vr1q1D27Zt8c033yAkJARHjhxBRkYGsrOz4e/vDwBITk5GYGAgjh49Ci8vL2RmZuLw4cMoKCiAWn3lh9KFCxciMjISc+bMgYODQ6PvyWxJy9ixY811afqLvvYSfsg1XXGgCfNH/WWDsO8+DyekxEfi069/wm8FZTAYDOjj54mXI4Jw6MRZrN602+j4RdOfwvf7j6O8ohod7nXBpGeCcI/SEcEvLDGKW/NFNiaEP4a0eePwxrKvUKo9jwlP9UHndu4YOtE4+35t6ZfYsvJlrJ83Dkmf/gClUxvMjg5D/vGzWPtltrhfDFED5HI5Hu3hb9L+1Zeb0KKFldG+e+65t8FzKN3djeIUCke8MH4iViQuhYPCUXi43KoVy/H4E08Jz2gBgJGjnsSG9PV4deoURL8SC2dnF3ySvh6nT53EquTVDV2O7kANzeNs6If3a9XX1+PTTz9FdXU1AgP/fkL5zp07oVQq4ejoiL59+2LOnDlQKpUAgNzcXNTV1SE4OFiIV6vV8PHxwe7duxESEoI9e/ZAoVAICQsABAQEQKFQYPfu3fDy8sKePXvg4+MjJCwAEBISAr1ej9zcXPTr16/R998sJuL+U01Njcnjq5uShZH4KqsuorT8PKKf7QelswOsrGQ4U6TFio93Yd5HX+PCReNxyXvdHbFo+lNwcbRHeUU1tu8+gvBXknCmSGsUV1t3CUMnLMecmJFYOP0ptGppg1+O/YERk1cYPQ0XAH7IPY6Rk1fizUnDsHHJBFy4WIdtP+Rj5uJNfBouWbSoCRNhb2+PDelpWJvyIVxd3fDcuPF4YfyLRnG2trZ4/4MULF40H3Pj38HFizXweqALElcm82m4FkCs4aGEhAS8/fbbRm1vvfUWZs2a1WD8wYMHERgYiIsXL6J169bYtGkTvL2vPC9ryJAheOqpp9CuXTucPHkSb7zxBvr374/c3FzI5XIUFxfD1tYWTk7Gq0Hd3d1RXFwMACguLhaSnH9SKpVGMe7uxpVAJycn2NraCjGNJTPcaPnGbVJdXY3p06fjk08+QXl5ucn++vr6Jp3Prhsf/U/UkHM5iebuAlGzY2cj/TU6T8sQ5TwHZ/drUqWltrYWZ86cQUVFBTZu3IgPPvgAu3btEhKXfyoqKkK7du2Qnp6OUaNGYf369XjuuedMrjdo0CDcf//9WLVqFeLj47FmzRocPXrUKMbT0xPjxo3Df//7X4wfPx6nT5/G118bPzna1tYWa9euxdNPP93o+28Wq4emTZuGHTt2YMWKFZDL5fjggw/w9ttvQ61WY+1a0zFkIiKiu5FcLhdWA13dbjQ0ZGtri06dOqF79+5ISEhA165dsXSp6VxGAPDw8EC7du1w/K+HHqpUKtTW1kKrNa6Sl5aWCpUTlUrV4NOYy8rKjGKurahotVrU1dWZVGBuplkkLZs3b8aKFSvw5JNPwtraGn369MHrr7+O+Ph4pKWlmbt7RERE/0pzeSKuwWAwqZxcVV5ejoKCAnh4XHkauZ+fH2xsbLB9+3YhpqioCPn5+ejZsycAIDAwEDqdDjk5OULM3r17odPpjGLy8/NRVFQkxGRmZkIul8PPz69J/W8Wc1rOnTuHDh06ALgyf+XqEufevXtj4sSJ5uwaERHRv2aO1UMzZ87EkCFD0LZtW5w/fx7p6enYuXMnMjIyUFVVhVmzZuGJJ56Ah4cHTp06hZkzZ8LV1RWPP/44gCtPWx43bhxiY2Ph4uICZ2dnxMXFwdfXV1hN1KVLFwwePBhRUVFISkoCcGXJc2hoKLy8rrxiIjg4GN7e3tBoNJg/fz7OnTuHuLg4REVFNXnOarOotHTs2BGnTp0CAHh7ewuP7t+8eTMcHR3N1zEiIiILVVJSAo1GAy8vLwwYMAB79+5FRkYGBg0aBCsrKxw8eBAjRoxA586dMXbsWHTu3Bl79uxBmzZthHMsXrwYI0eORHh4OHr16oVWrVph8+bNRg9/TUtLg6+vL4KDgxEcHIyHHnoIqampwn4rKyts2bIFLVu2RK9evRAeHo6RI0diwYIFTb4ns07E/f3339G+fXssXboUVlZWiI6OxnfffYdhw4ahvr4ely5dwqJFizBlypQmnZcTcYkaxom4RKZux0Rc75mZopzncHzwzYPuYGYdHvL09ERRUZHwosTRo0dj2bJl+PXXX7F//37cf//96Nq1603OQkRE1LzxBc3iMOvw0LVFnq1bt6K6uhr33XcfRo0axYSFiIiIBM1iIi4REdGdzBzvHroTmTVpaWgJF//DEhHRnYZ/tYnDrEmLwWBAZGSk8GCcixcv4sUXX4S9vb1R3Oeff26O7hEREYmCP5CLw6xJy7UvTXz22WfN1BMiIiJq7syatKxezTeTEhHRnY+VFnFwIi4REZHEmLOIo1k8EZeIiIjoZlhpISIikhiHh8TBpIWIiEhizFnEweEhIiIisgistBAREUmMw0PiYNJCREQkMeYs4uDwEBEREVkEVlqIiIgkxuEhcTBpISIikhhzFnEwaSEiIpIYKy3i4JwWIiIisgistBAREUmMhRZxMGkhIiKSGIeHxMHhISIiIrIIrLQQERFJjIUWcTBpISIikhiHh8TB4SEiIiKyCKy0EBERSYyFFnEwaSEiIpIYh4fEweEhIiIisgistBAREUmMlRZxMGkhIiKSGHMWcTBpISIikhgrLeLgnBYiIiKyCKy0EBERSYyFFnGw0kJERCQxmUwmytYUK1euxEMPPQQHBwc4ODggMDAQ27ZtE/YbDAbMmjULarUadnZ2CAoKwqFDh4zOodfrMXnyZLi6usLe3h5hYWEoLCw0itFqtdBoNFAoFFAoFNBoNKioqDCKOXPmDIYPHw57e3u4uroiOjoatbW1TfsSwaSFiIjojnTvvffi3Xffxf79+7F//370798fI0aMEBKTefPmYdGiRUhMTMS+ffugUqkwaNAgnD9/XjhHTEwMNm3ahPT0dGRlZaGqqgqhoaGor68XYiIiIpCXl4eMjAxkZGQgLy8PGo1G2F9fX49hw4ahuroaWVlZSE9Px8aNGxEbG9vke5IZDAbDv/hOmiW7bi+buwtEzdK5nERzd4Go2bGzkf4aA5bvEeU8304O/FfHOzs7Y/78+Xj++eehVqsRExOD6dOnA7hSVXF3d8fcuXMxYcIE6HQ6uLm5ITU1FaNHjwYAnD17Fm3btsXWrVsREhKCI0eOwNvbG9nZ2fD39wcAZGdnIzAwEL/++iu8vLywbds2hIaGoqCgAGq1GgCQnp6OyMhIlJaWwsHBodH9Z6WFiIhIYi1kMlE2vV6PyspKo02v19/0+vX19UhPT0d1dTUCAwNx8uRJFBcXIzg4WIiRy+Xo27cvdu/eDQDIzc1FXV2dUYxarYaPj48Qs2fPHigUCiFhAYCAgAAoFAqjGB8fHyFhAYCQkBDo9Xrk5uY27XtsUjQRERGZTUJCgjB35OqWkJBw3fiDBw+idevWkMvlePHFF7Fp0yZ4e3ujuLgYAODu7m4U7+7uLuwrLi6Gra0tnJycbhijVCpNrqtUKo1irr2Ok5MTbG1thZjG4uohIiIiiYm1emjGjBmYOnWqUZtcLr9uvJeXF/Ly8lBRUYGNGzdi7Nix2LVr1z/6Zdwxg8Fw0wm/18Y0FH8rMY3BSgsREZHExFo9JJfLhdVAV7cbJS22trbo1KkTunfvjoSEBHTt2hVLly6FSqUCAJNKR2lpqVAVUalUqK2thVarvWFMSUmJyXXLysqMYq69jlarRV1dnUkF5maYtBAREUmshUyc7d8yGAzQ6/Xo0KEDVCoVtm/fLuyrra3Frl270LNnTwCAn58fbGxsjGKKioqQn58vxAQGBkKn0yEnJ0eI2bt3L3Q6nVFMfn4+ioqKhJjMzEzI5XL4+fk1qf8cHiIiIroDzZw5E0OGDEHbtm1x/vx5pKenY+fOncjIyIBMJkNMTAzi4+Ph6ekJT09PxMfHo1WrVoiIiAAAKBQKjBs3DrGxsXBxcYGzszPi4uLg6+uLgQMHAgC6dOmCwYMHIyoqCklJSQCA8ePHIzQ0FF5eXgCA4OBgeHt7Q6PRYP78+Th37hzi4uIQFRXVpJVDAJMWIiIiyZnj3UMlJSXQaDQoKiqCQqHAQw89hIyMDAwaNAgAMG3aNNTU1GDSpEnQarXw9/dHZmYm2rRpI5xj8eLFsLa2Rnh4OGpqajBgwACkpKTAyspKiElLS0N0dLSwyigsLAyJiX8/XsHKygpbtmzBpEmT0KtXL9jZ2SEiIgILFixo8j3xOS1EdxE+p4XI1O14TsuwpJybBzXClgk9RDmPpeKcFiIiIrIIHB4iIiKSmAx8Y6IYmLQQERFJTIyVP8ThISIiIrIQrLQQERFJzByrh+5ETFqIiIgkxpxFHBweIiIiIovASgsREZHEWrDUIgomLURERBJjziIOJi1EREQS40RccXBOCxEREVkEVlqIiIgkxkKLOJi0EBERSYwTccXB4SEiIiKyCKy0EBERSYx1FnEwaSEiIpIYVw+Jg8NDREREZBFYaSEiIpJYCxZaRMGkhYiISGIcHhJHo5KWr776qtEnDAsLu+XOEBEREV1Po5KWkSNHNupkMpkM9fX1/6Y/REREdxwWWsTRqKTl8uXLUveDiIjojsXhIXFwTgsREZHEOBFXHLeUtFRXV2PXrl04c+YMamtrjfZFR0eL0jEiIiKif2py0nLgwAEMHToUFy5cQHV1NZydnfHnn3+iVatWUCqVTFqIiIiuweEhcTT54XKvvPIKhg8fjnPnzsHOzg7Z2dk4ffo0/Pz8sGDBAin6SEREZNFkIm13uyYnLXl5eYiNjYWVlRWsrKyg1+vRtm1bzJs3DzNnzpSij0RERERNT1psbGyEMpe7uzvOnDkDAFAoFMKviYiI6G8tZDJRtrtdk+e0dOvWDfv370fnzp3Rr18/vPnmm/jzzz+RmpoKX19fKfpIRERk0ZhviKPJlZb4+Hh4eHgAAGbPng0XFxdMnDgRpaWleP/990XvIBERERFwC5WW7t27C792c3PD1q1bRe0QERHRnYarh8TBh8sRERFJjDmLOJqctHTo0OGGGePvv//+rzpERERE1JAmJy0xMTFGn+vq6nDgwAFkZGTg1VdfFatfREREdwyu/BFHkyfiTpkyxWiLi4tDWloa/u///g9Hjx6Voo9EREQWTSYTZ2uKhIQEPProo2jTpg2USiVGjhxp8vd0ZGQkZDKZ0RYQEGAUo9frMXnyZLi6usLe3h5hYWEoLCw0itFqtdBoNFAoFFAoFNBoNKioqDCKOXPmDIYPHw57e3u4uroiOjra5FVAN9PkpOV6hgwZgo0bN4p1OiIiojvGtYnBrW5NsWvXLrz00kvIzs7G9u3bcenSJQQHB6O6utoobvDgwSgqKhK2axfYxMTEYNOmTUhPT0dWVhaqqqoQGhqK+vp6ISYiIgJ5eXnIyMhARkYG8vLyoNFohP319fUYNmwYqqurkZWVhfT0dGzcuBGxsbFNuifRJuJ+9tlncHZ2Fut0REREdA29Xg+9Xm/UJpfLIZfLTWIzMjKMPq9evRpKpRK5ubl47LHHjI5XqVQNXk+n0+HDDz9EamoqBg4cCABYt24d2rZti2+++QYhISE4cuQIMjIykJ2dDX9/fwBAcnIyAgMDcfToUXh5eSEzMxOHDx9GQUEB1Go1AGDhwoWIjIzEnDlz4ODg0Kj7v6WHy/0z2zMYDCguLkZZWRlWrFjR1NNJQrsv0dxdIGqWnPynmLsLRM1OTe5Sya8h1rBGQkIC3n77baO2t956C7NmzbrpsTqdDgBMCgw7d+6EUqmEo6Mj+vbtizlz5kCpVAIAcnNzUVdXh+DgYCFerVbDx8cHu3fvRkhICPbs2QOFQiEkLAAQEBAAhUKB3bt3w8vLC3v27IGPj4+QsABASEgI9Ho9cnNz0a9fv0bdf5OTlhEjRhglLS1atICbmxuCgoLwwAMPNPV0REREdzyxntMyY8YMTJ061aitoSrLtQwGA6ZOnYrevXvDx8dHaB8yZAieeuoptGvXDidPnsQbb7yB/v37Izc3F3K5HMXFxbC1tYWTk5PR+dzd3VFcXAwAKC4uFpKcf1IqlUYx7u7uRvudnJxga2srxDRGk5OWxmRzREREJL7rDQXdzMsvv4xffvkFWVlZRu2jR48Wfu3j44Pu3bujXbt22LJlC0aNGnXd8xkMBqNErKGk7FZibqbJFSsrKyuUlpaatJeXl8PKyqqppyMiIrrjtZCJs92KyZMn46uvvsJ3332He++994axHh4eaNeuHY4fPw4AUKlUqK2thVarNYorLS0VKicqlQolJSUm5yorKzOKubaiotVqUVdXZ1KBuZEmJy0Gg6HBdr1eD1tb26aejoiI6I5njqTFYDDg5Zdfxueff44dO3agQ4cONz2mvLwcBQUFwjsG/fz8YGNjg+3btwsxRUVFyM/PR8+ePQEAgYGB0Ol0yMnJEWL27t0LnU5nFJOfn4+ioiIhJjMzE3K5HH5+fo2+p0YPDy1btgzAlfLOBx98gNatWwv76uvr8f3333NOCxERUTPx0ksvYf369fjyyy/Rpk0bodKhUChgZ2eHqqoqzJo1C0888QQ8PDxw6tQpzJw5E66urnj88ceF2HHjxiE2NhYuLi5wdnZGXFwcfH19hdVEXbp0weDBgxEVFYWkpCQAwPjx4xEaGgovLy8AQHBwMLy9vaHRaDB//nycO3cOcXFxiIqKavTKIaAJScvixYsBXMncVq1aZTQUZGtri/bt22PVqlWNvjAREdHdwhwvTFy5ciUAICgoyKh99erViIyMhJWVFQ4ePIi1a9eioqICHh4e6NevHzZs2IA2bdoI8YsXL4a1tTXCw8NRU1ODAQMGICUlxSgPSEtLQ3R0tLDKKCwsDImJf6/ktbKywpYtWzBp0iT06tULdnZ2iIiIwIIFC5p0TzLD9cZ7rqNfv374/PPPTWYSNycXL5m7B0TNE5c8E5m6HUueX/2fOE+Mnx/qJcp5LFWTVw999913UvSDiIiI6IaaPBH3ySefxLvvvmvSPn/+fDz11FOidIqIiOhOYo53D92Jmpy07Nq1C8OGDTNpHzx4ML7//ntROkVERHQnaSGTibLd7Zo8PFRVVdXg0mYbGxtUVlaK0ikiIqI7iWhvJ77LNfl79PHxwYYNG0za09PT4e3tLUqniIiIiK7V5ErLG2+8gSeeeAK//fYb+vfvDwD49ttvsX79enz22Weid5CIiMjScWRHHE1OWsLCwvDFF18gPj4en332Gezs7NC1a1fs2LGjSQ+IISIiultwPoo4mpy0AMCwYcOEybgVFRVIS0tDTEwMfv75Z9TX14vaQSIiIiLgX8wN2rFjB5599lmo1WokJiZi6NCh2L9/v5h9IyIiuiNwybM4mlRpKSwsREpKCj766CNUV1cjPDwcdXV12LhxIyfhEhERXcetvqGZjDW60jJ06FB4e3vj8OHDWL58Oc6ePYvly5dL2TciIiIiQaMrLZmZmYiOjsbEiRPh6ekpZZ+IiIjuKJyIK45GV1p++OEHnD9/Ht27d4e/vz8SExNRVlYmZd+IiIjuCJzTIo5GJy2BgYFITk5GUVERJkyYgPT0dNxzzz24fPkytm/fjvPnz0vZTyIiIrrLNXn1UKtWrfD8888jKysLBw8eRGxsLN59910olUqEhYVJ0UciIiKL1kImzna3+1evQ/Dy8sK8efNQWFiIjz/+WKw+ERER3VFkIv1zt7ulh8tdy8rKCiNHjsTIkSPFOB0REdEdhVUScfDFk0RERGQRRKm0EBER0fWx0iIOJi1EREQSk3G9sig4PEREREQWgZUWIiIiiXF4SBxMWoiIiCTG0SFxcHiIiIiILAIrLURERBLjCxPFwaSFiIhIYpzTIg4ODxEREZFFYKWFiIhIYhwdEgeTFiIiIom14MsORcGkhYiISGKstIiDc1qIiIjIIrDSQkREJDGuHhIHkxYiIiKJ8Tkt4uDwEBER0R0oISEBjz76KNq0aQOlUomRI0fi6NGjRjEGgwGzZs2CWq2GnZ0dgoKCcOjQIaMYvV6PyZMnw9XVFfb29ggLC0NhYaFRjFarhUajgUKhgEKhgEajQUVFhVHMmTNnMHz4cNjb28PV1RXR0dGora1t0j0xaSEiIpKYTCbO1hS7du3CSy+9hOzsbGzfvh2XLl1CcHAwqqurhZh58+Zh0aJFSExMxL59+6BSqTBo0CCcP39eiImJicGmTZuQnp6OrKwsVFVVITQ0FPX19UJMREQE8vLykJGRgYyMDOTl5UGj0Qj76+vrMWzYMFRXVyMrKwvp6enYuHEjYmNjm/Y9GgwGQ9O+hubv4iVz94CoeXLyn2LuLhA1OzW5SyW/xoc5Z0Q5z7ge993ysWVlZVAqldi1axcee+wxGAwGqNVqxMTEYPr06QCuVFXc3d0xd+5cTJgwATqdDm5ubkhNTcXo0aMBAGfPnkXbtm2xdetWhISE4MiRI/D29kZ2djb8/f0BANnZ2QgMDMSvv/4KLy8vbNu2DaGhoSgoKIBarQYApKenIzIyEqWlpXBwcGjUPbDSQkREZCH0ej0qKyuNNr1e36hjdTodAMDZ2RkAcPLkSRQXFyM4OFiIkcvl6Nu3L3bv3g0AyM3NRV1dnVGMWq2Gj4+PELNnzx4oFAohYQGAgIAAKBQKoxgfHx8hYQGAkJAQ6PV65ObmNvr+mbQQERFJTKzhoYSEBGHeyNUtISHhptc3GAyYOnUqevfuDR8fHwBAcXExAMDd3d0o1t3dXdhXXFwMW1tbODk53TBGqVSaXFOpVBrFXHsdJycn2NraCjGNwdVDREREEhOrQjBjxgxMnTrVqE0ul9/0uJdffhm//PILsrKyTPbJrpksYzAYTNqudW1MQ/G3EnMzrLQQERFZCLlcDgcHB6PtZknL5MmT8dVXX+G7777DvffeK7SrVCoAMKl0lJaWClURlUqF2tpaaLXaG8aUlJSYXLesrMwo5trraLVa1NXVmVRgboRJCxERkcRkMpkoW1MYDAa8/PLL+Pzzz7Fjxw506NDBaH+HDh2gUqmwfft2oa22tha7du1Cz549AQB+fn6wsbExiikqKkJ+fr4QExgYCJ1Oh5ycHCFm79690Ol0RjH5+fkoKioSYjIzMyGXy+Hn59foe+LwEBERkcTM8Wi5l156CevXr8eXX36JNm3aCJUOhUIBOzs7yGQyxMTEID4+Hp6envD09ER8fDxatWqFiIgIIXbcuHGIjY2Fi4sLnJ2dERcXB19fXwwcOBAA0KVLFwwePBhRUVFISkoCAIwfPx6hoaHw8vICAAQHB8Pb2xsajQbz58/HuXPnEBcXh6ioqEavHAKYtBAREUnOHE/EXblyJQAgKCjIqH316tWIjIwEAEybNg01NTWYNGkStFot/P39kZmZiTZt2gjxixcvhrW1NcLDw1FTU4MBAwYgJSUFVlZWQkxaWhqio6OFVUZhYWFITEwU9ltZWWHLli2YNGkSevXqBTs7O0RERGDBggVNuic+p4XoLsLntBCZuh3PaVmXW3jzoEZ41u/emwfdwVhpISIikhjfPCQOJi1EREQS4/sSxcHVQ0RERGQRWGkhIiKSWFOXK1PDmLQQERFJjMMa4uD3SERERBaBlRYiIiKJcXhIHExaiIiIJMaURRwcHiIiIiKLwEoLERGRxDg8JA4mLURERBLjsIY4mLQQERFJjJUWcTD5IyIiIovASgsREZHEWGcRB5MWIiIiiXF0SBwcHiIiIiKLwEoLERGRxFpwgEgUTFqIiIgkxuEhcXB4iIiIiCwCKy1EREQSk3F4SBRMWoiIiCTG4SFxcHiIiIiILAIrLURERBLj6iFxMGkhIiKSGIeHxMGkhYiISGJMWsTBOS1ERERkEVhpISIikhiXPIuDSQsREZHEWjBnEUWzGB5KTU1Fr169oFarcfr0aQDAkiVL8OWXX5q5Z0RERNRcmD1pWblyJaZOnYqhQ4eioqIC9fX1AABHR0csWbLEvJ0jIiISgUykf+52Zk9ali9fjuTkZLz22muwsrIS2rt3746DBw+asWdERETikMnE2e52Zk9aTp48iW7dupm0y+VyVFdXm6FHRERE1ByZPWnp0KED8vLyTNq3bdsGb2/v298hIiIikXF4SBxmT1peffVVvPTSS9iwYQMMBgNycnIwZ84czJw5E6+++qq5u0dERPSvtZCJszXV999/j+HDh0OtVkMmk+GLL74w2h8ZGQmZTGa0BQQEGMXo9XpMnjwZrq6usLe3R1hYGAoLC41itFotNBoNFAoFFAoFNBoNKioqjGLOnDmD4cOHw97eHq6uroiOjkZtbW2T7sfsS56fe+45XLp0CdOmTcOFCxcQERGBe+65B0uXLsXTTz9t7u4RgM8/+xRvv/U67OxaIXv/AQBAfX090lLXYs/uLJw4cRyVOh081GoE9RuA518YDwcHB5PzrE9LxYaP0/BHYSHclEqMGDkK46ImwMbGxiiuvLwcSxbOx/e7vsPFixfR2esBvBwdA/+AwNtyv0TXihwZgJVvPIOqC3q49ZkmtL8/KwKa4f4m8UdPleDhJ+KN2mpylzZ47jeWb8aClG+M2tycWmPOlDAM6fMgWrW0xcFjf2DWiq3Yue+YyfH9enTGWxOHwrfzPbhwsRbbfjiE15Z+hTJt1a3cKt1hqqur0bVrVzz33HN44oknGowZPHgwVq9eLXy2tbU12h8TE4PNmzcjPT0dLi4uiI2NRWhoKHJzc4W5qBERESgsLERGRgYAYPz48dBoNNi8eTOAK39nDBs2DG5ubsjKykJ5eTnGjh0Lg8GA5cuXN/p+ZAaDwdCkb0BCf/75Jy5fvgylUvmvznPxkkgdIpSUlOCJEcPQ0s4OVeerhKTlQnU1Bvbrg8FDQxEY2BOOTk44cvgwkpNWwtXNDR9/shEtW7YUzpOctBLvLV+K518Yj8CevXAo/yASly1B2IjH8ebbs4W42tpaPBP+BM6fr8SUV2Lh7OyCDR+n4YfvdyHpg9Xo/miP2/4d3Emc/KeYuwsWR+2mQO6n/8WFmlo4tLYzSVqeGNQNQyYkGh1To6/DweNnjdtyl+Lzbw5gaep3Ru0FxVoU/VkpfLa1scKP6+Lg2NoObyRuRum5KrwY3huDez+IoRPfQ9ZPvwmxvR+5H1tXvoSMrENY9UkWlM6t8c7kMGjPX0CvZxegtq5ezK/ijnW9hFJMPxzTinKePp2dbvlYmUyGTZs2YeTIkUJbZGQkKioqTCowV+l0Ori5uSE1NRWjR48GAJw9exZt27bF1q1bERISgiNHjsDb2xvZ2dnw97+SxGdnZyMwMBC//vorvLy8sG3bNoSGhqKgoABqtRoAkJ6ejsjISJSWljb4g25DzF5p+SdXV1dzd4Gu8c7bb+ERv+5QKByxPfNroV3esiW2Zn4LR8e/fwM92sMfHh4eiJs6Bd9s/xqhw0cAACoqtEhOWolRT4YjOmaqEHvp0iUkLluCMZqxuL9TJwDApo2f4sTxY1iblo6uD3cTYp8aNQKLF85HWvqnt+vWiQAAy2aGI+un36CtvIDHBzxssv/yZQNy8k836lyl5edvGhs5MhA+ndQIilyMvQdPAQB27T+OnI+nIX5KGB4bu1iITZgyAsdPl+KZaatRX38ZAHDqj3J8t/oVjB0RgOTPfmzcTZLkxFr5o9frodfrjdrkcjnkcvktn3Pnzp1QKpVwdHRE3759MWfOHKF4kJubi7q6OgQHBwvxarUaPj4+2L17N0JCQrBnzx4oFAohYQGAgIAAKBQK7N69G15eXtizZw98fHyEhAUAQkJCoNfrkZubi379+jWqr2af09KhQwd07NjxuhuZz/82f4nc/Tl47Y1ZJvusrKyMEparfHwfAgCUFBcLbT9m/QC9Xo+Rj48yih3x+CgYDAZ8t+Pv0viOb79B+w4dhIQFAKytrRE6PAz5B39BSUnJv70tokZ7ekh39HmkE2LevX3JcljQQzh6qkRIWACgvv4yPt62H4/6tIfaTQHgSgWou087fLx1v5CwAED2L6dw7FQJwvo9dNv6TDcnE2lLSEgQ5o1c3RISEm65X0OGDEFaWhp27NiBhQsXYt++fejfv7+QGBUXF8PW1hZOTsZ/3ru7u6P4rz/ni4uLGxwhUSqVRjHu7u5G+52cnGBrayvENIbZKy0xMTFGn+vq6nDgwAFkZGRwIq4ZlZeXY/678ZjySizcVapGH5ezNxsAcP/9nYS2E8ePAwA6eXY2inVzU8LJyUnYfzX2ET8/k/N6dvYCAPx24rjJ//hEUnBzao35cY/jjcTN+KNUd904O7kNTn49G25OrVH8ZyU27/wF/7dqG7SVF0xiwwf7YeyIALRo0QKHfivCqg0/IHXzXqMY704q7D7wu8mx+X8NN3W5X4WzZTp4d/IAAJNhKADIP3EWgV35Q9+daMaMGZg6dapR27+pslwd8gEAHx8fdO/eHe3atcOWLVswatSo6x5nMBgg+0f5SNZAKelWYm7G7EnLlCkNj7G/99572L9//02Pb6hUZrD6d6UyAuJnv4327Tsg/OmIRh9TUlKCpYsX4sEHffBY0N+lPl1FBWxtbdGqVSuTYxwUCqMZ5hUVFXBQKEziFH+16a6ZjU4klaX/fQrHT5Xi/U+zrhtz8NhZzDj2JQ79VgQA6PPI/Zg8JghBPTqjt2Yhqmv+XhmRvm0/MrIOo7BYCzfnNhg7IgDvz4pAh3td8H8rtwpxLgp7nNOZJjzav9pcFPZG/9bqTJ9ndU53Ac5/7afmoYVI40P/dijoZjw8PNCuXTsc/+uHSZVKhdraWmi1WqNqS2lpKXr27CnENFQFLysrE37IVKlU2LvXOEHXarWoq6tr0g+iZh8eup4hQ4Zg48aNN41rqFQ2f+6tl8oI+Cbza+zauQNvvv1OozNgXUUFXn4xCgYYMG/hErRoYfy/1o3Oc+2uG16Tj4Sk22Bk/64Y+pgPJr2TfsO45et3Yvn6ndix9yh27D2Kt1duxQtvpuGBDio8/3hPo9jnXk/Fhoxc/Jj3O77Y8TMen5KELd/nIy5yIFwdjRMMA66/PuLatRPXi2xGaywI4g0PSa28vBwFBQXw8LhSyfPz84ONjQ22b98uxBQVFSE/P19IWgIDA6HT6ZCTkyPE7N27FzqdzigmPz8fRUVFQkxmZibkcjn8GqiuX0+zTVo+++wzODs73zRuxowZ0Ol0Rtur02fchh7emS5UVyP+nf/DM2M0cFMqUVlZicrKStTV1QEAKisrceGC8U+BlTodJkQ9j9LSEiQlf4R727Y12q9wdIRer0dNTY3J9Sp1OigUjsJnR0fHBqspOt2V8ryigSoMkZjs7WyxePqTWLnhexSVVULR2g6K1nawtblSmFa0tkOrlrbXPf7L735B1QU9evi2u+m10rfuh421FR7xvk9oK9dVC1WUf3JSXKlUnvtr2Kn8rwpLQxUVZ0WrBoen6O5TVVWFvLw84SGuJ0+eRF5eHs6cOYOqqirExcVhz549OHXqFHbu3Inhw4fD1dUVjz/+OIArf+aOGzcOsbGx+Pbbb3HgwAE8++yz8PX1xcCBAwEAXbp0weDBgxEVFYXs7GxkZ2cjKioKoaGh8PK6MrQfHBwMb29vaDQaHDhwAN9++y3i4uIQFRXV6JVDQDMYHurWrZvRT9YGgwHFxcUoKyvDihUrbnp8Q6UyLnm+ddoKLcrL/8TalI+wNuUjk/19Ah9Fv/4DsGT5lf82lTodxr/wHP4oLMT7H6Wgs9cDJsd4/jWX5fjxY3jooa5C+59lZdBqtejk6Sm0dercGcePmz6L4vixK23/jCWSgotja6hcHRCj6Y8YTX+T/cW73sXmnb8gPPbD655DJruyquhmrv7Rd/kfVZFDJ4rw4F/zVf7Jp9OVVReHTxQZ/dunkwe+/vGwUeyDndTCkBU1E2YqEu/fv99oZc7V+TBjx47FypUrcfDgQaxduxYVFRXw8PBAv379sGHDBrRp00Y4ZvHixbC2tkZ4eDhqamowYMAApKSkGL0vMC0tDdHR0cIqo7CwMCQm/v0oACsrK2zZsgWTJk1Cr169YGdnh4iICCxYsKBJ92P2pOWf68UBoEWLFnBzc0NQUBAeeMD0L0CSlqurGz5Yvdak/aMP3kfu/n14b1UyHP8a17yasBQWFiAp+SN06dLwaxd69e4DuVyOr7743Chp+fKLTZDJZOjXf6DQNmDAQMyZ/TZ++eVnIfbSpUvY8r+v4PtQVyiVnIRL0iopr0TweNOHXcVFDkSfR+7HiOgklFdc/71oowZ2hb2dvFHLoJ8Z9ihq6y7hwJECoe2r737BshnheNSnHfb9dQ4rqxZ4ekh35Bw8JTzT5WyZDvvyT+Hpod2xOHWHkCT18GkHr/buSFy/q0n3TdIy1yP4g4KCbjhU+PXXX19331UtW7bE8uXLb/gQOGdnZ6xbt+6G57nvvvvwv//976bXuxGzJi2XLl1C+/btERISAlUTVqiQdORyOR7tYfqEz6++2IQWLayEfRcvXsSL48fh1yOH8ep/Z6K+vh6//JwnxDs5OaPtfVdK3gpHR0RNmIj3li+FQuEoPFxu1YrlGPXEU8IzWgBg5Kgnkf7xerz6ypQrD5dzccGG9PU4feokkj5YDSKp6Wsv4YfcEybtmuE9UH/ZIOy7T+WElDn/waeZP+G3gj9hMBjQx68TXn6mLw6dKMLqTXuEY1/R9McDHVX4LucY/iitgNK5NcaOCMCgwC6YvWqbURK05stsTHiqD9LmPoc3lm9G6bnzmPBUb3Rur8TQie8Z9em1ZZuxZcUkrJ/7HJI+zYLSuQ1mTx6O/BNnsfarbIm+ISLzMWvSYm1tjYkTJ+LIkSPm7AbdgvLyP3Eo/yAAYF7CHJP9YSMex+z4d4XPURMmopW9PTZ8nIY1qz+Eq6sbnn9hPF4Y/6LRcba2tkj+MAWLF87Hu/Hv4OLFGng90AXvrUrm03CpWamsvojSc+cRPaYflC5tYNWiBc4UncOK9O8xb/V2XLj498qho6dKMKyvDwb39oaTQyvUXKzDL8cK8Z8ZKfg084DReWvr6jF04nuYMyUMC199Aq1a2uCXY39gxORVRk/DBYAfck9gZHQS3nxxCDYujsKFi3XYlnUIM5d8yafhNjNcQyAOsz/Gv1+/fpgyZYrJMNG/wTktRA3jY/yJTN2Ox/jv+/36z/ppikc73t2LEcw+p2XSpEmIjY1FYWEh/Pz8YG9vPBP+oYf4VEciIiIyY9Ly/PPPY8mSJcLT+KKjo4V9MplMeEpefT1LnEREZOE4PCQKsyUta9aswbvvvouTJ0+aqwtERES3hblWD91pzJa0XJ1K067dzR/AREREZMk4EVccZn0iblNekkRERER3N7NOxO3cufNNE5dz587dpt4QERFJgz+ii8OsScvbb7/Nd8kQEdGdj1mLKMyatDz99NNQKpXm7AIRERFZCLMlLZzPQkREdwuuHhKH2VcPERER3en4c7o4zJa0XL582VyXJiIiIgtk9sf4ExER3elYaBEHkxYiIiKpMWsRhVkfLkdERETUWKy0EBERSYyrh8TBpIWIiEhiXD0kDiYtREREEmPOIg7OaSEiIiKLwEoLERGR1FhqEQWTFiIiIolxIq44ODxEREREFoGVFiIiIolx9ZA4mLQQERFJjDmLODg8RERERBaBlRYiIiKpsdQiCiYtREREEuPqIXFweIiIiIgsAistREREEuPqIXEwaSEiIpIYcxZxMGkhIiKSGrMWUXBOCxEREVkEJi1EREQSk4n0T1N9//33GD58ONRqNWQyGb744guj/QaDAbNmzYJarYadnR2CgoJw6NAhoxi9Xo/JkyfD1dUV9vb2CAsLQ2FhoVGMVquFRqOBQqGAQqGARqNBRUWFUcyZM2cwfPhw2Nvbw9XVFdHR0aitrW3S/TBpISIikphMJs7WVNXV1ejatSsSExMb3D9v3jwsWrQIiYmJ2LdvH1QqFQYNGoTz588LMTExMdi0aRPS09ORlZWFqqoqhIaGor6+XoiJiIhAXl4eMjIykJGRgby8PGg0GmF/fX09hg0bhurqamRlZSE9PR0bN25EbGxsk+5HZjAYDE38Dpq9i5fM3QOi5snJf4q5u0DU7NTkLpX8GidKa0Q5T1tFC+j1eqM2uVwOuVx+02NlMhk2bdqEkSNHArhSZVGr1YiJicH06dMBXKmquLu7Y+7cuZgwYQJ0Oh3c3NyQmpqK0aNHAwDOnj2Ltm3bYuvWrQgJCcGRI0fg7e2N7Oxs+Pv7AwCys7MRGBiIX3/9FV5eXti2bRtCQ0NRUFAAtVoNAEhPT0dkZCRKS0vh4ODQqPtnpYWIiEhiMpG2hIQEYQjm6paQkHBLfTp58iSKi4sRHBwstMnlcvTt2xe7d+8GAOTm5qKurs4oRq1Ww8fHR4jZs2cPFAqFkLAAQEBAABQKhVGMj4+PkLAAQEhICPR6PXJzcxvdZ64eIiIikppIq4dmzJiBqVOnGrU1psrSkOLiYgCAu7u7Ubu7uztOnz4txNja2sLJyckk5urxxcXFUCqVJudXKpVGMddex8nJCba2tkJMYzBpISIishCNHQpqCtk1k2UMBoNJ27WujWko/lZibobDQ0RERBIz1+qhG1GpVABgUukoLS0VqiIqlQq1tbXQarU3jCkpKTE5f1lZmVHMtdfRarWoq6szqcDcCJMWIiIiiZlr9dCNdOjQASqVCtu3bxfaamtrsWvXLvTs2RMA4OfnBxsbG6OYoqIi5OfnCzGBgYHQ6XTIyckRYvbu3QudTmcUk5+fj6KiIiEmMzMTcrkcfn5+je4zh4eIiIjuUFVVVThx4oTw+eTJk8jLy4OzszPuu+8+xMTEID4+Hp6envD09ER8fDxatWqFiIgIAIBCocC4ceMQGxsLFxcXODs7Iy4uDr6+vhg4cCAAoEuXLhg8eDCioqKQlJQEABg/fjxCQ0Ph5eUFAAgODoa3tzc0Gg3mz5+Pc+fOIS4uDlFRUY1eOQQwaSEiIpKcuZ7iv3//fvTr10/4fHUS79ixY5GSkoJp06ahpqYGkyZNglarhb+/PzIzM9GmTRvhmMWLF8Pa2hrh4eGoqanBgAEDkJKSAisrKyEmLS0N0dHRwiqjsLAwo2fDWFlZYcuWLZg0aRJ69eoFOzs7REREYMGCBU26Hz6nheguwue0EJm6Hc9pOVV+UZTztHdpKcp5LBUrLURERBITexLt3YoTcYmIiMgisNJCREQkMbFX/tytmLQQERFJjDmLODg8RERERBaBlRYiIiKJcXhIHExaiIiIJMesRQwcHiIiIiKLwEoLERGRxDg8JA4mLURERBJjziIODg8RERGRRWClhYiISGIcHhIHkxYiIiKJ8d1D4mDSQkREJDXmLKLgnBYiIiKyCKy0EBERSYyFFnEwaSEiIpIYJ+KKg8NDREREZBFYaSEiIpIYVw+Jg0kLERGR1JiziILDQ0RERGQRWGkhIiKSGAst4mDSQkREJDGuHhIHh4eIiIjIIrDSQkREJDGuHhIHkxYiIiKJcXhIHBweIiIiIovApIWIiIgsAoeHiIiIJMbhIXEwaSEiIpIYJ+KKg8NDREREZBFYaSEiIpIYh4fEwaSFiIhIYsxZxMHhISIiojvQrFmzIJPJjDaVSiXsNxgMmDVrFtRqNezs7BAUFIRDhw4ZnUOv12Py5MlwdXWFvb09wsLCUFhYaBSj1Wqh0WigUCigUCig0WhQUVEhyT0xaSEiIpKaTKStiR588EEUFRUJ28GDB4V98+bNw6JFi5CYmIh9+/ZBpVJh0KBBOH/+vBATExODTZs2IT09HVlZWaiqqkJoaCjq6+uFmIiICOTl5SEjIwMZGRnIy8uDRqNpemcbgcNDREREEjPX6iFra2uj6spVBoMBS5YswWuvvYZRo0YBANasWQN3d3esX78eEyZMgE6nw4cffojU1FQMHDgQALBu3Tq0bdsW33zzDUJCQnDkyBFkZGQgOzsb/v7+AIDk5GQEBgbi6NGj8PLyEvV+WGkhIiKyEHq9HpWVlUabXq+/bvzx48ehVqvRoUMHPP300/j9998BACdPnkRxcTGCg4OFWLlcjr59+2L37t0AgNzcXNTV1RnFqNVq+Pj4CDF79uyBQqEQEhYACAgIgEKhEGLExKSFiIhIYjKZOFtCQoIwd+TqlpCQ0OA1/f39sXbtWnz99ddITk5GcXExevbsifLychQXFwMA3N3djY5xd3cX9hUXF8PW1hZOTk43jFEqlSbXViqVQoyYODxEREQkMbEGh2bMmIGpU6catcnl8gZjhwwZIvza19cXgYGBuP/++7FmzRoEBARc6dc1a7ENBoNJ27WujWkovjHnuRWstBAREUlNpIm4crkcDg4ORtv1kpZr2dvbw9fXF8ePHxfmuVxbDSktLRWqLyqVCrW1tdBqtTeMKSkpMblWWVmZSRVHDExaiIiI7gJ6vR5HjhyBh4cHOnToAJVKhe3btwv7a2trsWvXLvTs2RMA4OfnBxsbG6OYoqIi5OfnCzGBgYHQ6XTIyckRYvbu3QudTifEiInDQ0RERBIzx+qhuLg4DB8+HPfddx9KS0vxzjvvoLKyEmPHjoVMJkNMTAzi4+Ph6ekJT09PxMfHo1WrVoiIiAAAKBQKjBs3DrGxsXBxcYGzszPi4uLg6+srrCbq0qULBg8ejKioKCQlJQEAxo8fj9DQUNFXDgFMWoiIiCRnjsf4FxYW4plnnsGff/4JNzc3BAQEIDs7G+3atQMATJs2DTU1NZg0aRK0Wi38/f2RmZmJNm3aCOdYvHgxrK2tER4ejpqaGgwYMAApKSmwsrISYtLS0hAdHS2sMgoLC0NiYqIk9yQzGAwGSc5sRhcvmbsHRM2Tk/8Uc3eBqNmpyV0q+TXE+nup5V1eargjkxZqHvR6PRISEjBjxoxGTxQjuhvw9wbRrWHSQpKprKyEQqGATqeDg4ODubtD1Gzw9wbRreHqISIiIrIITFqIiIjIIjBpISIiIovApIUkI5fL8dZbb3GiIdE1+HuD6NZwIi4RERFZBFZaiIiIyCIwaSEiIiKLwKSFiIiILAKTFpJUSkoKHB0dzd0NIiK6AzBpoUaJjIyETCYz2U6cOGHurhGZTUO/J/65RUZGmruLRHeUu/zVS9QUgwcPxurVq43a3NzczNQbIvMrKioSfr1hwwa8+eabOHr0qNBmZ2dnFF9XVwcbG5vb1j+iOw0rLdRocrkcKpXKaFu6dCl8fX1hb2+Ptm3bYtKkSaiqqrruOcrLy9GjRw+EhYXh4sWLMBgMmDdvHjp27Ag7Ozt07doVn3322W28K6Jb98/fCwqFAjKZTPh88eJFODo64pNPPkFQUBBatmyJdevWYdasWXj44YeNzrNkyRK0b9/eqG316tXo0qULWrZsiQceeAArVqy4fTdG1EwxaaF/pUWLFli2bBny8/OxZs0a7NixA9OmTWswtrCwEH369MEDDzyAzz//HC1btsTrr7+O1atXY+XKlTh06BBeeeUVPPvss9i1a9dtvhMiaUyfPh3R0dE4cuQIQkJCGnVMcnIyXnvtNcyZMwdHjhxBfHw83njjDaxZs0bi3hI1bxweokb73//+h9atWwufhwwZgk8//VT43KFDB8yePRsTJ040+anw2LFjGDRoEEaMGIGlS5dCJpOhuroaixYtwo4dOxAYGAgA6NixI7KyspCUlIS+ffvenhsjklBMTAxGjRrVpGNmz56NhQsXCsd16NABhw8fRlJSEsaOHStFN4ksApMWarR+/fph5cqVwmd7e3t89913iI+Px+HDh1FZWYlLly7h4sWLqK6uhr29PQCgpqYGvXv3xjPPPIOlS5cKxx8+fBgXL17EoEGDjK5TW1uLbt263Z6bIpJY9+7dmxRfVlaGgoICjBs3DlFRUUL7pUuXoFAoxO4ekUVh0kKNZm9vj06dOgmfT58+jaFDh+LFF1/E7Nmz4ezsjKysLIwbNw51dXVCnFwux8CBA7Flyxa8+uqruPfeewEAly9fBgBs2bIF99xzj9G1+E4WulNcTd6vatGiBa59e8o/f79c/X2RnJwMf39/ozgrKyuJeklkGZi00C3bv38/Ll26hIULF6JFiyvToz755BOTuBYtWiA1NRURERHo378/du7cCbVaDW9vb8jlcpw5c4ZDQXTXcHNzQ3FxMQwGA2QyGQAgLy9P2O/u7o577rkHv//+O8aMGWOmXhI1T0xa6Jbdf//9uHTpEpYvX47hw4fjxx9/xKpVqxqMtbKyQlpaGp555hkhcVGpVIiLi8Mrr7yCy5cvo3fv3qisrMTu3bvRunVrjt3THSkoKAhlZWWYN28ennzySWRkZGDbtm1wcHAQYmbNmoXo6Gg4ODhgyJAh0Ov12L9/P7RaLaZOnWrG3hOZF1cP0S17+OGHsWjRIsydOxc+Pj5IS0tDQkLCdeOtra3x8ccf48EHH0T//v1RWlqK2bNn480330RCQgK6dOmCkJAQbN68GR06dLiNd0J0+3Tp0gUrVqzAe++9h65duyInJwdxcXFGMS+88AI++OADpKSkwNfXF3379kVKSgp/X9BdT2a4dnCViIiIqBlipYWIiIgsApMWIiIisghMWoiIiMgiMGkhIiIii8CkhYiIiCwCkxYiIiKyCExaiIiIyCIwaSEiIiKLwKSF6A40a9YsPPzww8LnyMhIjBw58rb349SpU5DJZEbv1iEiulVMWohuo8jISMhkMshkMtjY2KBjx46Ii4tDdXW1pNddunQpUlJSGhXLRIOImiu+MJHoNhs8eDBWr16Nuro6/PDDD3jhhRdQXV2NlStXGsXV1dXBxsZGlGsqFApRzkNEZE6stBDdZnK5HCqVCm3btkVERATGjBmDL774QhjS+eijj9CxY0fI5XIYDAbodDqMHz8eSqUSDg4O6N+/P37++Wejc7777rtwd3dHmzZtMG7cOFy8eNFo/7XDQ5cvX8bcuXPRqVMnyOVy3HfffZgzZw4ACC/l69atG2QyGYKCgoTjVq9ejS5duqBly5Z44IEHsGLFCqPr5OTkoFu3bmjZsiW6d++OAwcOiPjNEdHdjpUWIjOzs7NDXV0dAODEiRP45JNPsHHjRlhZWQEAhg0bBmdnZ2zduhUKhQJJSUkYMGAAjh07BmdnZ3zyySd466238N5776FPnz5ITU3FsmXL0LFjx+tec8aMGUhOTsbixYvRu3dvFBUV4ddffwVwJfHo0aMHvvnmGzz44IOwtbUFACQnJ+Ott95CYmIiunXrhgMHDiAqKgr29vYYO3YsqqurERoaiv79+2PdunU4efIkpkyZIvG3R0R3FQMR3TZjx441jBgxQvi8d+9eg4uLiyE8PNzw1ltvGWxsbAylpaXC/m+//dbg4OBguHjxotF57r//fkNSUpLBYDAYAgMDDS+++KLRfn9/f0PXrl0bvG5lZaVBLpcbkpOTG+zjyZMnDQAMBw4cMGpv27atYf369UZts2fPNgQGBhoMBoMhKSnJ4OzsbKiurhb2r1y5ssFzERHdCg4PEd1m//vf/9C6dWu0bNkSgYGBeOyxx7B8+XIAQLt27eDm5ibE5ubmoqqqCi4uLmjdurWwnTx5Er/99hsA4MiRIwgMDDS6xrWf/+nIkSPQ6/UYMGBAo/tcVlaGgoICjBs3zqgf77zzjlE/unbtilatWjWqH0RETcXhIaLbrF+/fli5ciVsbGygVquNJtva29sbxV6+fBkeHh7YuXOnyXkcHR1v6fp2dnZNPuby5csArgwR+fv7G+27OoxlMBhuqT9ERI3FpIXoNrO3t0enTp0aFfvII4+guLgY1tbWaN++fYMxXbp0QXZ2Nv7zn/8IbdnZ2dc9p6enJ+zs7PDtt9/ihRdeMNl/dQ5LfX290Obu7o577rkHv//+O8aMGdPgeb29vZGamoqamhohMbpRP4iImorDQ0TN2MCBAxEYGIiRI0fi66+/xqlTp7B79268/vrr2L9/PwBgypQp+Oijj/DRRx/h2LFjeOutt3Do0KHrnrNly5aYPn06pk2bhrVr1+K3335DdnY2PvzwQwCAUqmEnZ0dMjIyUFJSAp1OB+DKA+sSEhKwdOlSHDt2DAcPHsTq1auxaNEiAEBERARatGiBcePG4fDhw9i6dSsWLFgg8TdERHcTJi1EzZhMJsPWrVvx2GOP4fnnn0fnzp3x9NNP49SpU3B3dwcAjB49Gm+++SamT58OPz8/nD59GhMnTrzhed944w3ExsbizTffRJcuXTB69GiUlpYCAKytrbFs2TIkJSVBrVZjxIgRAIAXXngBH3zwAVJSUuDr64u+ffsiJSVFWCLdunVrbN68GYcPH0a3bt3w2muvYe7cuRJ+O0R0t5EZOBBNREREFoCVFiIiIrIITFqIiIjIIjBpISIiIovApIWIiIgsApMWIiIisghMWoiIiMgiMGkhIiIii8CkhYiIiCwCkxYiIiKyCExaiIiIyCIwaSEiIiKL8P/5kThChj8vewAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"all_count_hyper_1\"\n",
    "\n",
    "metrics = test_stats_simple.metrics\n",
    "metrics_test = metrics[metrics[\"split\"] == \"test\"]\n",
    "cm = metrics_test[metrics_test[\"name\"] == model_name][\"confusion_matrix\"].values[0] * 100\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=[\"Fake\",\"True\"], yticklabels=[\"Fake\",\"True\"], fmt=\"d\", annot_kws={\"size\": 12})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrlll}\n",
      "\\toprule\n",
      "                        name & split &  train\\_acc &      acc &  precision &   recall &       f1 &  time &             confusion\\_matrix &                                   model \\\\\n",
      "\\midrule\n",
      "                   all\\_count &   val &   1.000000 & 0.915000 &   0.918406 & 0.920152 & 0.919278 &  4.07 &       [[431, 43], [42, 484]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &   val &   1.000000 & 0.907000 &   0.912381 & 0.910646 & 0.911513 &  4.97 &       [[428, 46], [47, 479]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &   val &   0.997500 & 0.903000 &   0.911708 & 0.903042 & 0.907354 &  2.61 &       [[428, 46], [51, 475]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &   val &   1.000000 & 0.901000 &   0.905123 & 0.906844 & 0.905983 &  3.18 &       [[424, 50], [49, 477]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &   val &   0.999375 & 0.851000 &   0.845872 & 0.876426 & 0.860878 &  4.05 &       [[390, 84], [65, 461]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &   val &   0.965250 & 0.830000 &   0.815603 & 0.874525 & 0.844037 &  8.01 &      [[370, 104], [66, 460]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &   val &   0.978250 & 0.825000 &   0.805217 & 0.880228 & 0.841054 & 16.49 &      [[362, 112], [63, 463]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &   val &   0.919375 & 0.823000 &   0.822551 & 0.846008 & 0.834114 &  0.56 &       [[378, 96], [81, 445]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &   val &   1.000000 & 0.810000 &   0.808824 & 0.836502 & 0.822430 &  3.55 &      [[370, 104], [86, 440]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &   val &   0.999375 & 0.809000 &   0.820268 & 0.815589 & 0.817922 &  4.24 &       [[380, 94], [97, 429]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &   val &   0.980500 & 0.807000 &   0.817143 & 0.815589 & 0.816365 &  2.56 &       [[378, 96], [97, 429]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &   val &   0.998125 & 0.803000 &   0.802239 & 0.825336 & 0.813623 &  3.83 &      [[373, 106], [91, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_count &   val &   0.998500 & 0.802000 &   0.805970 & 0.821293 & 0.813559 &  5.31 &      [[370, 104], [94, 432]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &   val &   0.838500 & 0.782000 &   0.767361 & 0.840304 & 0.802178 &  0.35 &      [[340, 134], [84, 442]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &   val &   0.998200 & 0.764000 &   0.862319 & 0.666045 & 0.751579 &  2.31 &      [[407, 57], [179, 357]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &   val &   0.999625 & 0.684000 &   0.753589 & 0.596591 & 0.665962 &  3.46 &     [[369, 103], [213, 315]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  test &   1.000000 & 0.914000 &   0.911824 & 0.915493 & 0.913655 &  4.97 &       [[459, 44], [42, 455]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  test &   1.000000 & 0.907000 &   0.908907 & 0.903421 & 0.906155 &  4.07 &       [[458, 45], [48, 449]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  test &   1.000000 & 0.901000 &   0.899598 & 0.901408 & 0.900503 &  3.18 &       [[453, 50], [49, 448]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  test &   0.997500 & 0.901000 &   0.904472 & 0.895372 & 0.899899 &  2.61 &       [[456, 47], [52, 445]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  test &   0.978250 & 0.820000 &   0.781528 & 0.885312 & 0.830189 & 16.49 &      [[380, 123], [57, 440]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  test &   0.919375 & 0.828000 &   0.816764 & 0.843058 & 0.829703 &  0.56 &       [[409, 94], [78, 419]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  test &   0.965250 & 0.822000 &   0.794824 & 0.865191 & 0.828516 &  8.01 &      [[392, 111], [67, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  test &   0.999375 & 0.819000 &   0.829167 & 0.800805 & 0.814739 &  4.05 &       [[421, 82], [99, 398]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  test &   1.000000 & 0.808000 &   0.804391 & 0.810865 & 0.807615 &  3.55 &       [[405, 98], [94, 403]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  test &   0.998500 & 0.806000 &   0.808554 & 0.798793 & 0.803644 &  5.31 &      [[409, 94], [100, 397]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  test &   0.980500 & 0.807000 &   0.817992 & 0.786720 & 0.802051 &  2.56 &      [[416, 87], [106, 391]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  test &   0.838500 & 0.790000 &   0.759494 & 0.845070 & 0.800000 &  0.35 &      [[370, 133], [77, 420]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  test &   0.999375 & 0.802000 &   0.804481 & 0.794769 & 0.799595 &  4.24 &      [[407, 96], [102, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  test &   0.998125 & 0.798000 &   0.804481 & 0.788423 & 0.796371 &  3.83 &      [[403, 96], [106, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  test &   0.998200 & 0.775000 &   0.878173 & 0.661568 & 0.754635 &  2.31 &      [[429, 48], [177, 346]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  test &   0.999625 & 0.695000 &   0.750594 & 0.612403 & 0.674493 &  3.46 &     [[379, 105], [200, 316]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  liar &   0.978250 & 0.549292 &   0.583020 & 0.673816 & 0.625138 & 16.49 & [[2219, 3438], [2327, 4807]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  liar &   0.965250 & 0.542647 &   0.588699 & 0.597281 & 0.592959 &  8.01 & [[2680, 2977], [2873, 4261]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  liar &   0.838500 & 0.541005 &   0.592991 & 0.564480 & 0.578384 &  0.35 & [[2893, 2764], [3107, 4027]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  liar &   0.919375 & 0.539833 &   0.592747 & 0.559013 & 0.575386 &  0.56 & [[2917, 2740], [3146, 3988]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  liar &   1.000000 & 0.520522 &   0.588101 & 0.468321 & 0.521420 &  3.55 & [[3317, 2340], [3793, 3341]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  liar &   0.998500 & 0.480807 &   0.609410 & 0.192459 & 0.292532 &  5.31 &  [[4777, 880], [5761, 1373]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  liar &   0.998125 & 0.477132 &   0.613544 & 0.168909 & 0.264893 &  3.83 &  [[4898, 759], [5929, 1205]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  liar &   0.980500 & 0.475803 &   0.624927 & 0.150407 & 0.242458 &  2.56 &  [[5013, 644], [6061, 1073]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  liar &   0.999375 & 0.474318 &   0.618360 & 0.150126 & 0.241597 &  4.24 &  [[4996, 661], [6063, 1071]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  liar &   0.999625 & 0.470174 &   0.603358 & 0.146061 & 0.235188 &  3.46 &  [[4972, 685], [6092, 1042]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  liar &   0.999375 & 0.465093 &   0.627622 & 0.100645 & 0.173472 &  4.05 &   [[5231, 426], [6416, 718]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  liar &   0.998200 & 0.462044 &   0.625372 & 0.088450 & 0.154980 &  2.31 &   [[5279, 378], [6503, 631]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  liar &   1.000000 & 0.448440 &   0.665272 & 0.022288 & 0.043130 &  3.18 &    [[5577, 80], [6975, 159]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  liar &   1.000000 & 0.446642 &   0.637255 & 0.018223 & 0.035432 &  4.97 &    [[5583, 74], [7004, 130]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  liar &   1.000000 & 0.445939 &   0.659864 & 0.013597 & 0.026645 &  4.07 &     [[5607, 50], [7037, 97]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  liar &   0.997500 & 0.444140 &   0.666667 & 0.006728 & 0.013322 &  2.61 &     [[5633, 24], [7086, 48]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False).to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 2.222195863723755 seconds)\n",
      "Saved vector 0 in 5.955408573150635 seconds\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.0s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.3s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.0s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.1s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   6.5s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   6.6s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.9s\n",
      "content_count finished in 77.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.796545</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.792741</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.780287</td>\n",
       "      <td>0.764588</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466891</td>\n",
       "      <td>0.604930</td>\n",
       "      <td>0.127278</td>\n",
       "      <td>0.210307</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name split  train_acc       acc  precision    recall        f1  \\\n",
       "0  content_count   val        1.0  0.783000   0.796545  0.788973  0.792741   \n",
       "1  content_count  test        1.0  0.776000   0.780287  0.764588  0.772358   \n",
       "2  content_count  liar        1.0  0.466891   0.604930  0.127278  0.210307   \n",
       "\n",
       "    time                                              model  \n",
       "0  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "1  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "2  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500]},#[500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(grid, \"content_count\")]),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, X_liar, y_liar) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)\n",
    "# best params\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
    "    if tests == None:\n",
    "        tests = Test_statistic()\n",
    "    for file, name in files:\n",
    "        print(f\"Proccessing: {name}\")\n",
    "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "        data = pd.read_csv(file, usecols=cols_to_read)\n",
    "        print(\"Read data into dataframe\")\n",
    "\n",
    "        for col, entry_name in cols_to_test:\n",
    "            for func, model, func_name in vec_funcs:\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
    "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
    "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
    "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "importlib.reload(pp)\n",
    "\n",
    "def test_on_liar(test, file):\n",
    "    liar_data = pp.apply_pipeline_pd_tqdm(pd.read_csv(file), [(pp.Binary_labels_LIAR(), 'label', 'type_binary')])\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for row in info_list:\n",
    "        model_name = row[-1]\n",
    "        model = test.metrics[test.metrics[\"name\"] == model_name][\"model\"].values[0]\n",
    "        vectorizer = test.metrics[test.metrics[\"name\"] == model_name][\"vectorizer\"].values[0]\n",
    "        X = vectorizer.transform(liar_data[\"statement_combined\"].values)\n",
    "        #print(liar_data[\"type_binary\"].astype(int).value_counts())\n",
    "        metrics = pd.concat([mt.get_predict_metrics(model, X, liar_data[\"type_binary\"].astype(int), name=model_name), metrics])\n",
    "\n",
    "        \n",
    "    return metrics.sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/sample/dataset_unbalanced_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.12475, conspiracy: 0.112, junksci: 0.0165, hate: 0.01025, unreliable: 0.03825, bias: 0.15625, satire: 0.01725, reliable: 0.257, clickbait: 0.03075, political: 0.237\n",
      "True: 2099, Fake: 1901\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.098, junksci: 0.01, hate: 0.008, unreliable: 0.044, bias: 0.174, satire: 0.01, reliable: 0.25, clickbait: 0.044, political: 0.254\n",
      "True: 274, Fake: 226\n",
      "Distribution of test with size 500:\n",
      "fake: 0.134, conspiracy: 0.128, junksci: 0.026, hate: 0.01, unreliable: 0.056, bias: 0.134, satire: 0.012, reliable: 0.244, clickbait: 0.034, political: 0.222\n",
      "True: 250, Fake: 250\n",
      "File: ../datasets/sample/dataset_balanced_types_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 1200, Fake: 2800\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.112, junksci: 0.006, hate: 0.008, unreliable: 0.05, bias: 0.166, satire: 0.014, reliable: 0.27, clickbait: 0.026, political: 0.24\n",
      "True: 268, Fake: 232\n",
      "Distribution of test with size 500:\n",
      "fake: 0.132, conspiracy: 0.096, junksci: 0.014, hate: 0.01, unreliable: 0.044, bias: 0.158, satire: 0.01, reliable: 0.244, clickbait: 0.022, political: 0.27\n",
      "True: 268, Fake: 232\n",
      "File: ../datasets/sample/dataset_balanced_bin_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.13175, conspiracy: 0.11775, junksci: 0.017, hate: 0.0105, unreliable: 0.0405, bias: 0.165, satire: 0.0175, reliable: 0.18275, clickbait: 0.022, political: 0.16425\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.088, conspiracy: 0.088, junksci: 0.016, hate: 0.008, unreliable: 0.034, bias: 0.138, satire: 0.014, reliable: 0.22, clickbait: 0.038, political: 0.202\n",
      "True: 307, Fake: 193\n",
      "Distribution of test with size 500:\n",
      "fake: 0.122, conspiracy: 0.096, junksci: 0.016, hate: 0.008, unreliable: 0.05, bias: 0.116, satire: 0.006, reliable: 0.214, clickbait: 0.026, political: 0.202\n",
      "True: 293, Fake: 207\n",
      "File: ../datasets/sample/dataset_reliable_fake_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.5, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.5, clickbait: 0.0, political: 0.0\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.306, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.694, clickbait: 0.0, political: 0.0\n",
      "True: 347, Fake: 153\n",
      "Distribution of test with size 500:\n",
      "fake: 0.302, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.698, clickbait: 0.0, political: 0.0\n",
      "True: 349, Fake: 151\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_cleaned.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "for file in [unbalanced, balanced_types, balanced_bin, balanced_reliable_fake]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "nbformat": 4,
   "nbformat_minor": 2
}