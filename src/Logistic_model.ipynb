{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.apply_pipeline(\n",
    "    \"../datasets/big/shuffled.csv\", \n",
    "    [(pp.Binary_labels(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/big/dataset_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nans\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/sample/dataset_unbalanced_1M.csv\",\n",
    "    [(pp.Delete_nan(), 'content_title'),\n",
    "     (pp.Delete_nan(), 'content_domain'),\n",
    "     (pp.Delete_nan(), 'content_authors'),\n",
    "     (pp.Delete_nan(), 'content_domain_authors_title')],\n",
    "     new_file=\"../datasets/sample/dataset_unbalanced_1M_.csv\",\n",
    "     progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 645049.29it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 630807.78it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 641900.16it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 661270.10it/s]\n",
      "100%|██████████| 20000/20000 [00:00<00:00, 636083.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entries read: 100000\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/big/dataset.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_unbalanced_100K.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "#pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "#                                    out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check distribution of labels (just to show that everything works)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 768849.38it/s]\n",
      "100%|██████████| 10000/10000 [00:18<00:00, 539.50it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 18019.53it/s]\n",
      "100%|██████████| 10000/10000 [00:08<00:00, 1205.96it/s]\n",
      "100%|██████████| 10000/10000 [01:02<00:00, 161.14it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 82612.20it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 169198.84it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 15205.85it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 124929.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 51255.62it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6579.82it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 497940.71it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 167259.01it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 38457.73it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37036.88it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 37947.78it/s]\n",
      "100%|██████████| 10000/10000 [00:00<00:00, 26879.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 200000 rows\n",
      "finish time: 97.20679092407227\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting liar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data = pd.read_csv(\"../datasets/big/combined_cleaned_bin.csv\")\n",
    "X_liar =  liar_data[\"statement_combined\"].values\n",
    "y_liar = liar_data[\"type_binary\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models (other than logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive_bayes finished in 3.50 seconds\n",
      "random_forest finished in 102.68 seconds\n",
      "decision_tree finished in 28.78 seconds\n",
      "ada_boost finished in 30.19 seconds\n",
      "svm finished in 73.44 seconds\n",
      "passive_aggressive finished in 79.77 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>val</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.831410</td>\n",
       "      <td>0.881325</td>\n",
       "      <td>0.781120</td>\n",
       "      <td>0.828203</td>\n",
       "      <td>78.51</td>\n",
       "      <td>[[42504, 5472], [11387, 40637]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>val</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.805740</td>\n",
       "      <td>0.816596</td>\n",
       "      <td>0.808089</td>\n",
       "      <td>0.812320</td>\n",
       "      <td>1.36</td>\n",
       "      <td>[[38534, 9442], [9984, 42040]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>val</td>\n",
       "      <td>0.523091</td>\n",
       "      <td>0.520750</td>\n",
       "      <td>0.520505</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.684648</td>\n",
       "      <td>16.82</td>\n",
       "      <td>[[51, 47925], [0, 52024]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>svm</td>\n",
       "      <td>val</td>\n",
       "      <td>0.522355</td>\n",
       "      <td>0.520040</td>\n",
       "      <td>0.520201</td>\n",
       "      <td>0.996886</td>\n",
       "      <td>0.683654</td>\n",
       "      <td>40.51</td>\n",
       "      <td>[[142, 47834], [162, 51862]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>val</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.280870</td>\n",
       "      <td>0.419391</td>\n",
       "      <td>27.01</td>\n",
       "      <td>[[44930, 3046], [37412, 14612]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>val</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.595420</td>\n",
       "      <td>0.827500</td>\n",
       "      <td>0.280870</td>\n",
       "      <td>0.419391</td>\n",
       "      <td>28.44</td>\n",
       "      <td>[[44930, 3046], [37412, 14612]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>test</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.829650</td>\n",
       "      <td>0.881638</td>\n",
       "      <td>0.779928</td>\n",
       "      <td>0.827670</td>\n",
       "      <td>78.51</td>\n",
       "      <td>[[42057, 5492], [11543, 40908]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>test</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.807920</td>\n",
       "      <td>0.821667</td>\n",
       "      <td>0.809479</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>1.36</td>\n",
       "      <td>[[38334, 9215], [9993, 42458]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>test</td>\n",
       "      <td>0.523091</td>\n",
       "      <td>0.524960</td>\n",
       "      <td>0.524746</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.688306</td>\n",
       "      <td>16.82</td>\n",
       "      <td>[[45, 47504], [0, 52451]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>svm</td>\n",
       "      <td>test</td>\n",
       "      <td>0.522355</td>\n",
       "      <td>0.524210</td>\n",
       "      <td>0.524432</td>\n",
       "      <td>0.996911</td>\n",
       "      <td>0.687303</td>\n",
       "      <td>40.51</td>\n",
       "      <td>[[132, 47417], [162, 52289]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>test</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.594690</td>\n",
       "      <td>0.830267</td>\n",
       "      <td>0.285657</td>\n",
       "      <td>0.425068</td>\n",
       "      <td>27.01</td>\n",
       "      <td>[[44486, 3063], [37468, 14983]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>test</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.594690</td>\n",
       "      <td>0.830267</td>\n",
       "      <td>0.285657</td>\n",
       "      <td>0.425068</td>\n",
       "      <td>28.44</td>\n",
       "      <td>[[44486, 3063], [37468, 14983]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>random_forest</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.523091</td>\n",
       "      <td>0.557736</td>\n",
       "      <td>0.557736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.716085</td>\n",
       "      <td>16.82</td>\n",
       "      <td>[[0, 5657], [0, 7134]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, max_featu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>svm</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.522355</td>\n",
       "      <td>0.557736</td>\n",
       "      <td>0.557736</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.716085</td>\n",
       "      <td>40.51</td>\n",
       "      <td>[[0, 5657], [0, 7134]]</td>\n",
       "      <td>SVC(kernel='linear', max_iter=10)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>naive_bayes</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.817462</td>\n",
       "      <td>0.529435</td>\n",
       "      <td>0.561704</td>\n",
       "      <td>0.711382</td>\n",
       "      <td>0.627744</td>\n",
       "      <td>1.36</td>\n",
       "      <td>[[1697, 3960], [2059, 5075]]</td>\n",
       "      <td>MultinomialNB()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>passive_aggressive</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.875345</td>\n",
       "      <td>0.472520</td>\n",
       "      <td>0.580391</td>\n",
       "      <td>0.195823</td>\n",
       "      <td>0.292841</td>\n",
       "      <td>78.51</td>\n",
       "      <td>[[4647, 1010], [5737, 1397]]</td>\n",
       "      <td>PassiveAggressiveClassifier()</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>decision_tree</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.443437</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>27.01</td>\n",
       "      <td>[[5625, 32], [7087, 47]]</td>\n",
       "      <td>DecisionTreeClassifier(max_depth=1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ada_boost</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.595579</td>\n",
       "      <td>0.443437</td>\n",
       "      <td>0.594937</td>\n",
       "      <td>0.006588</td>\n",
       "      <td>0.013032</td>\n",
       "      <td>28.44</td>\n",
       "      <td>[[5625, 32], [7087, 47]]</td>\n",
       "      <td>(DecisionTreeClassifier(max_depth=1, random_st...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name split  train_acc       acc  precision    recall  \\\n",
       "0  passive_aggressive   val   0.875345  0.831410   0.881325  0.781120   \n",
       "0         naive_bayes   val   0.817462  0.805740   0.816596  0.808089   \n",
       "0       random_forest   val   0.523091  0.520750   0.520505  1.000000   \n",
       "0                 svm   val   0.522355  0.520040   0.520201  0.996886   \n",
       "0       decision_tree   val   0.595579  0.595420   0.827500  0.280870   \n",
       "0           ada_boost   val   0.595579  0.595420   0.827500  0.280870   \n",
       "1  passive_aggressive  test   0.875345  0.829650   0.881638  0.779928   \n",
       "1         naive_bayes  test   0.817462  0.807920   0.821667  0.809479   \n",
       "1       random_forest  test   0.523091  0.524960   0.524746  1.000000   \n",
       "1                 svm  test   0.522355  0.524210   0.524432  0.996911   \n",
       "1       decision_tree  test   0.595579  0.594690   0.830267  0.285657   \n",
       "1           ada_boost  test   0.595579  0.594690   0.830267  0.285657   \n",
       "2       random_forest  liar   0.523091  0.557736   0.557736  1.000000   \n",
       "2                 svm  liar   0.522355  0.557736   0.557736  1.000000   \n",
       "2         naive_bayes  liar   0.817462  0.529435   0.561704  0.711382   \n",
       "2  passive_aggressive  liar   0.875345  0.472520   0.580391  0.195823   \n",
       "2       decision_tree  liar   0.595579  0.443437   0.594937  0.006588   \n",
       "2           ada_boost  liar   0.595579  0.443437   0.594937  0.006588   \n",
       "\n",
       "         f1   time                 confusion_matrix  \\\n",
       "0  0.828203  78.51  [[42504, 5472], [11387, 40637]]   \n",
       "0  0.812320   1.36   [[38534, 9442], [9984, 42040]]   \n",
       "0  0.684648  16.82        [[51, 47925], [0, 52024]]   \n",
       "0  0.683654  40.51     [[142, 47834], [162, 51862]]   \n",
       "0  0.419391  27.01  [[44930, 3046], [37412, 14612]]   \n",
       "0  0.419391  28.44  [[44930, 3046], [37412, 14612]]   \n",
       "1  0.827670  78.51  [[42057, 5492], [11543, 40908]]   \n",
       "1  0.815528   1.36   [[38334, 9215], [9993, 42458]]   \n",
       "1  0.688306  16.82        [[45, 47504], [0, 52451]]   \n",
       "1  0.687303  40.51     [[132, 47417], [162, 52289]]   \n",
       "1  0.425068  27.01  [[44486, 3063], [37468, 14983]]   \n",
       "1  0.425068  28.44  [[44486, 3063], [37468, 14983]]   \n",
       "2  0.716085  16.82           [[0, 5657], [0, 7134]]   \n",
       "2  0.716085  40.51           [[0, 5657], [0, 7134]]   \n",
       "2  0.627744   1.36     [[1697, 3960], [2059, 5075]]   \n",
       "2  0.292841  78.51     [[4647, 1010], [5737, 1397]]   \n",
       "2  0.013032  27.01         [[5625, 32], [7087, 47]]   \n",
       "2  0.013032  28.44         [[5625, 32], [7087, 47]]   \n",
       "\n",
       "                                               model  \n",
       "0                      PassiveAggressiveClassifier()  \n",
       "0                                    MultinomialNB()  \n",
       "0  (DecisionTreeClassifier(max_depth=1, max_featu...  \n",
       "0                  SVC(kernel='linear', max_iter=10)  \n",
       "0                DecisionTreeClassifier(max_depth=1)  \n",
       "0  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "1                      PassiveAggressiveClassifier()  \n",
       "1                                    MultinomialNB()  \n",
       "1  (DecisionTreeClassifier(max_depth=1, max_featu...  \n",
       "1                  SVC(kernel='linear', max_iter=10)  \n",
       "1                DecisionTreeClassifier(max_depth=1)  \n",
       "1  (DecisionTreeClassifier(max_depth=1, random_st...  \n",
       "2  (DecisionTreeClassifier(max_depth=1, max_featu...  \n",
       "2                  SVC(kernel='linear', max_iter=10)  \n",
       "2                                    MultinomialNB()  \n",
       "2                      PassiveAggressiveClassifier()  \n",
       "2                DecisionTreeClassifier(max_depth=1)  \n",
       "2  (DecisionTreeClassifier(max_depth=1, random_st...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "info_list = [(\n",
    "    \"../datasets/sample/dataset_unbalanced_100K.csv\", \"content_combined\", mt.create_count_vector, [\n",
    "        (MultinomialNB(), \"naive_bayes\"),\n",
    "        (RandomForestClassifier(max_depth=1), \"random_forest\"), #25\n",
    "        (DecisionTreeClassifier(max_depth=1), \"decision_tree\"),\n",
    "        (AdaBoostClassifier(n_estimators=1), \"ada_boost\"), #2\n",
    "        (SVC(kernel='linear', max_iter=10), \"svm\"),\n",
    "        #(KNeighborsClassifier(n_neighbors=2, algorithm='kd_tree'), \"knn\"), #15\n",
    "        (PassiveAggressiveClassifier(), \"passive_aggressive\")\n",
    "        ])\n",
    "]\n",
    "\n",
    "test_stats_base = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_100K.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_base)\n",
    "test_stats_base.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_count finished in 370.79 seconds\n",
      "content_count_hyper finished in 367.48 seconds\n",
      "content_count_balanced_types finished in 216.27 seconds\n",
      "content_count_balanced_bin finished in 361.70 seconds\n",
      "content_count_reliable_fake finished in 364.93 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>val</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.873580</td>\n",
       "      <td>0.892032</td>\n",
       "      <td>0.861237</td>\n",
       "      <td>0.876364</td>\n",
       "      <td>366.08</td>\n",
       "      <td>[[42553, 5423], [7219, 44805]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.872150</td>\n",
       "      <td>0.891333</td>\n",
       "      <td>0.858969</td>\n",
       "      <td>0.874852</td>\n",
       "      <td>369.13</td>\n",
       "      <td>[[42528, 5448], [7337, 44687]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>val</td>\n",
       "      <td>0.900223</td>\n",
       "      <td>0.870240</td>\n",
       "      <td>0.896534</td>\n",
       "      <td>0.848402</td>\n",
       "      <td>0.871804</td>\n",
       "      <td>360.26</td>\n",
       "      <td>[[42902, 5092], [7884, 44122]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>val</td>\n",
       "      <td>0.916062</td>\n",
       "      <td>0.837590</td>\n",
       "      <td>0.912774</td>\n",
       "      <td>0.762503</td>\n",
       "      <td>0.830899</td>\n",
       "      <td>215.31</td>\n",
       "      <td>[[43858, 3813], [12428, 39901]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>val</td>\n",
       "      <td>0.977344</td>\n",
       "      <td>0.713530</td>\n",
       "      <td>0.810682</td>\n",
       "      <td>0.587953</td>\n",
       "      <td>0.681583</td>\n",
       "      <td>363.52</td>\n",
       "      <td>[[40693, 7160], [21487, 30660]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>test</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.871240</td>\n",
       "      <td>0.890248</td>\n",
       "      <td>0.860613</td>\n",
       "      <td>0.875179</td>\n",
       "      <td>366.08</td>\n",
       "      <td>[[41984, 5565], [7311, 45140]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.870110</td>\n",
       "      <td>0.889033</td>\n",
       "      <td>0.859659</td>\n",
       "      <td>0.874100</td>\n",
       "      <td>369.13</td>\n",
       "      <td>[[41921, 5628], [7361, 45090]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>test</td>\n",
       "      <td>0.900223</td>\n",
       "      <td>0.867520</td>\n",
       "      <td>0.895768</td>\n",
       "      <td>0.845953</td>\n",
       "      <td>0.870148</td>\n",
       "      <td>360.26</td>\n",
       "      <td>[[42364, 5165], [8083, 44388]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>test</td>\n",
       "      <td>0.916062</td>\n",
       "      <td>0.838080</td>\n",
       "      <td>0.912156</td>\n",
       "      <td>0.762871</td>\n",
       "      <td>0.830861</td>\n",
       "      <td>215.31</td>\n",
       "      <td>[[44038, 3830], [12362, 39770]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>test</td>\n",
       "      <td>0.977344</td>\n",
       "      <td>0.716530</td>\n",
       "      <td>0.812541</td>\n",
       "      <td>0.593343</td>\n",
       "      <td>0.685854</td>\n",
       "      <td>363.52</td>\n",
       "      <td>[[40709, 7139], [21208, 30944]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.900499</td>\n",
       "      <td>0.475881</td>\n",
       "      <td>0.582692</td>\n",
       "      <td>0.212363</td>\n",
       "      <td>0.311280</td>\n",
       "      <td>369.13</td>\n",
       "      <td>[[4572, 1085], [5619, 1515]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_hyper</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.897783</td>\n",
       "      <td>0.473849</td>\n",
       "      <td>0.583540</td>\n",
       "      <td>0.197785</td>\n",
       "      <td>0.295436</td>\n",
       "      <td>366.08</td>\n",
       "      <td>[[4650, 1007], [5723, 1411]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_balanced_bin</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.900223</td>\n",
       "      <td>0.462278</td>\n",
       "      <td>0.585447</td>\n",
       "      <td>0.122932</td>\n",
       "      <td>0.203197</td>\n",
       "      <td>360.26</td>\n",
       "      <td>[[5036, 621], [6257, 877]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_reliable_fake</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.977344</td>\n",
       "      <td>0.462825</td>\n",
       "      <td>0.596056</td>\n",
       "      <td>0.114382</td>\n",
       "      <td>0.191932</td>\n",
       "      <td>363.52</td>\n",
       "      <td>[[5104, 553], [6318, 816]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count_balanced_types</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.916062</td>\n",
       "      <td>0.450004</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.032801</td>\n",
       "      <td>0.062375</td>\n",
       "      <td>215.31</td>\n",
       "      <td>[[5522, 135], [6900, 234]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           name split  train_acc       acc  precision  \\\n",
       "0           content_count_hyper   val   0.897783  0.873580   0.892032   \n",
       "0                 content_count   val   0.900499  0.872150   0.891333   \n",
       "0    content_count_balanced_bin   val   0.900223  0.870240   0.896534   \n",
       "0  content_count_balanced_types   val   0.916062  0.837590   0.912774   \n",
       "0   content_count_reliable_fake   val   0.977344  0.713530   0.810682   \n",
       "1           content_count_hyper  test   0.897783  0.871240   0.890248   \n",
       "1                 content_count  test   0.900499  0.870110   0.889033   \n",
       "1    content_count_balanced_bin  test   0.900223  0.867520   0.895768   \n",
       "1  content_count_balanced_types  test   0.916062  0.838080   0.912156   \n",
       "1   content_count_reliable_fake  test   0.977344  0.716530   0.812541   \n",
       "2                 content_count  liar   0.900499  0.475881   0.582692   \n",
       "2           content_count_hyper  liar   0.897783  0.473849   0.583540   \n",
       "2    content_count_balanced_bin  liar   0.900223  0.462278   0.585447   \n",
       "2   content_count_reliable_fake  liar   0.977344  0.462825   0.596056   \n",
       "2  content_count_balanced_types  liar   0.916062  0.450004   0.634146   \n",
       "\n",
       "     recall        f1    time                 confusion_matrix  \\\n",
       "0  0.861237  0.876364  366.08   [[42553, 5423], [7219, 44805]]   \n",
       "0  0.858969  0.874852  369.13   [[42528, 5448], [7337, 44687]]   \n",
       "0  0.848402  0.871804  360.26   [[42902, 5092], [7884, 44122]]   \n",
       "0  0.762503  0.830899  215.31  [[43858, 3813], [12428, 39901]]   \n",
       "0  0.587953  0.681583  363.52  [[40693, 7160], [21487, 30660]]   \n",
       "1  0.860613  0.875179  366.08   [[41984, 5565], [7311, 45140]]   \n",
       "1  0.859659  0.874100  369.13   [[41921, 5628], [7361, 45090]]   \n",
       "1  0.845953  0.870148  360.26   [[42364, 5165], [8083, 44388]]   \n",
       "1  0.762871  0.830861  215.31  [[44038, 3830], [12362, 39770]]   \n",
       "1  0.593343  0.685854  363.52  [[40709, 7139], [21208, 30944]]   \n",
       "2  0.212363  0.311280  369.13     [[4572, 1085], [5619, 1515]]   \n",
       "2  0.197785  0.295436  366.08     [[4650, 1007], [5723, 1411]]   \n",
       "2  0.122932  0.203197  360.26       [[5036, 621], [6257, 877]]   \n",
       "2  0.114382  0.191932  363.52       [[5104, 553], [6318, 816]]   \n",
       "2  0.032801  0.062375  215.31       [[5522, 135], [6900, 234]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count\"), \n",
    "                                                              (LogisticRegression(max_iter=300, C=0.1), \"content_count_hyper\")]),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_types\")]),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_bin\")]),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_reliable_fake\")]),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_tfidf_pseudo finished in 17.06 seconds\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    \n",
    "     (unbalanced, \"content_combined\", mt.create_tdfidf_vector_unigram, [(LogisticRegression(max_iter=300), \"content_tfidf\")]),\n",
    "     #                                                                   (LogisticRegression(max_iter=300, C=250), \"content_tfidf_hyper_1\"),\n",
    "     #                                                                   (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "     (unbalanced, \"content_combined\", mt.create_tdfidf_vector_bigram, [#(LogisticRegression(max_iter=300), \"content_tfidf_bi\"),\n",
    "                                                                       (LogisticRegression(max_iter=300, C=250), \"content_tfidf_bi_hyper_1\")]),\n",
    "]\n",
    "\n",
    "#test_stats_tdidf = mt.Test_statistic()\n",
    "\n",
    "#mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, tests=test_stats_tdidf)\n",
    "test_stats_tdidf.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "info_list = [\n",
    "    \n",
    "     (\"../datasets/sample/dataset_unbalanced_100K.csv\", \"content_combined\", mt.create_tdfidf_vector_trigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_tdidf = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_tri.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors_tri.pickle\", info_list, tests=test_stats_tdidf)\n",
    "test_stats_tdidf.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 108.31220841407776 seconds)\n",
      "Saved vector 0 in 440.462379693985 seconds\n",
      "Creating vector 1 (data read in 106.4322395324707 seconds)\n",
      "Saved vector 1 in 427.0809762477875 seconds\n",
      "Creating vector 2 (data read in 102.18216276168823 seconds)\n",
      "Saved vector 2 in 409.81818413734436 seconds\n",
      "Creating vector 3 (data read in 105.79786205291748 seconds)\n",
      "Saved vector 3 in 438.70478320121765 seconds\n",
      "content_title_count finished in 385.34 seconds\n",
      "content_domain_count finished in 337.14 seconds\n",
      "content_authors_count finished in 335.44 seconds\n",
      "all_count finished in 343.60 seconds\n",
      "all_count_hyper_1 finished in 336.64 seconds\n",
      "all_count_hyper_2 finished in 343.65 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>confusion_matrix</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.996040</td>\n",
       "      <td>0.996939</td>\n",
       "      <td>0.995444</td>\n",
       "      <td>0.996191</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[47817, 159], [237, 51787]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.995730</td>\n",
       "      <td>0.996325</td>\n",
       "      <td>0.995464</td>\n",
       "      <td>0.995894</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[47785, 191], [236, 51788]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.994480</td>\n",
       "      <td>0.995152</td>\n",
       "      <td>0.994233</td>\n",
       "      <td>0.994692</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[47724, 252], [300, 51724]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.993980</td>\n",
       "      <td>0.994614</td>\n",
       "      <td>0.993811</td>\n",
       "      <td>0.994212</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[47696, 280], [322, 51702]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>0.938097</td>\n",
       "      <td>0.912329</td>\n",
       "      <td>0.925033</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[44844, 3132], [4561, 47463]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>val</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.882290</td>\n",
       "      <td>0.894089</td>\n",
       "      <td>0.877710</td>\n",
       "      <td>0.885824</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[42567, 5409], [6362, 45662]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.996060</td>\n",
       "      <td>0.996869</td>\n",
       "      <td>0.995615</td>\n",
       "      <td>0.996242</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[47385, 164], [230, 52221]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.995690</td>\n",
       "      <td>0.996317</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.995890</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[47356, 193], [238, 52213]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.994520</td>\n",
       "      <td>0.995324</td>\n",
       "      <td>0.994223</td>\n",
       "      <td>0.994773</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[47304, 245], [303, 52148]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.994150</td>\n",
       "      <td>0.995226</td>\n",
       "      <td>0.993613</td>\n",
       "      <td>0.994419</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[47299, 250], [335, 52116]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.920630</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>0.909630</td>\n",
       "      <td>0.923209</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[44352, 3197], [4740, 47711]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>test</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.881610</td>\n",
       "      <td>0.894046</td>\n",
       "      <td>0.878382</td>\n",
       "      <td>0.886145</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[42089, 5460], [6379, 46072]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_title_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.911011</td>\n",
       "      <td>0.483699</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>0.267452</td>\n",
       "      <td>0.366219</td>\n",
       "      <td>383.68</td>\n",
       "      <td>[[4279, 1378], [5226, 1908]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_authors_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.946640</td>\n",
       "      <td>0.448988</td>\n",
       "      <td>0.563798</td>\n",
       "      <td>0.053266</td>\n",
       "      <td>0.097336</td>\n",
       "      <td>334.12</td>\n",
       "      <td>[[5363, 294], [6754, 380]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_1</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.449222</td>\n",
       "      <td>0.604215</td>\n",
       "      <td>0.036165</td>\n",
       "      <td>0.068245</td>\n",
       "      <td>335.35</td>\n",
       "      <td>[[5488, 169], [6876, 258]]</td>\n",
       "      <td>LogisticRegression(C=250, max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_domain_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999764</td>\n",
       "      <td>0.443280</td>\n",
       "      <td>0.622642</td>\n",
       "      <td>0.004626</td>\n",
       "      <td>0.009183</td>\n",
       "      <td>335.69</td>\n",
       "      <td>[[5637, 20], [7101, 33]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.442342</td>\n",
       "      <td>0.517241</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.004188</td>\n",
       "      <td>342.31</td>\n",
       "      <td>[[5643, 14], [7119, 15]]</td>\n",
       "      <td>LogisticRegression(max_iter=300)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>all_count_hyper_2</td>\n",
       "      <td>liar</td>\n",
       "      <td>0.999620</td>\n",
       "      <td>0.442186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>342.35</td>\n",
       "      <td>[[5656, 1], [7134, 0]]</td>\n",
       "      <td>LogisticRegression(C=0.1, max_iter=300)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name split  train_acc       acc  precision    recall  \\\n",
       "0      all_count_hyper_2   val   0.999620  0.996040   0.996939  0.995444   \n",
       "0              all_count   val   0.999941  0.995730   0.996325  0.995464   \n",
       "0      all_count_hyper_1   val   0.999982  0.994480   0.995152  0.994233   \n",
       "0   content_domain_count   val   0.999764  0.993980   0.994614  0.993811   \n",
       "0  content_authors_count   val   0.946640  0.923070   0.938097  0.912329   \n",
       "0    content_title_count   val   0.911011  0.882290   0.894089  0.877710   \n",
       "1      all_count_hyper_2  test   0.999620  0.996060   0.996869  0.995615   \n",
       "1              all_count  test   0.999941  0.995690   0.996317  0.995462   \n",
       "1      all_count_hyper_1  test   0.999982  0.994520   0.995324  0.994223   \n",
       "1   content_domain_count  test   0.999764  0.994150   0.995226  0.993613   \n",
       "1  content_authors_count  test   0.946640  0.920630   0.937200  0.909630   \n",
       "1    content_title_count  test   0.911011  0.881610   0.894046  0.878382   \n",
       "2    content_title_count  liar   0.911011  0.483699   0.580645  0.267452   \n",
       "2  content_authors_count  liar   0.946640  0.448988   0.563798  0.053266   \n",
       "2      all_count_hyper_1  liar   0.999982  0.449222   0.604215  0.036165   \n",
       "2   content_domain_count  liar   0.999764  0.443280   0.622642  0.004626   \n",
       "2              all_count  liar   0.999941  0.442342   0.517241  0.002103   \n",
       "2      all_count_hyper_2  liar   0.999620  0.442186   0.000000  0.000000   \n",
       "\n",
       "         f1    time                confusion_matrix  \\\n",
       "0  0.996191  342.35    [[47817, 159], [237, 51787]]   \n",
       "0  0.995894  342.31    [[47785, 191], [236, 51788]]   \n",
       "0  0.994692  335.35    [[47724, 252], [300, 51724]]   \n",
       "0  0.994212  335.69    [[47696, 280], [322, 51702]]   \n",
       "0  0.925033  334.12  [[44844, 3132], [4561, 47463]]   \n",
       "0  0.885824  383.68  [[42567, 5409], [6362, 45662]]   \n",
       "1  0.996242  342.35    [[47385, 164], [230, 52221]]   \n",
       "1  0.995890  342.31    [[47356, 193], [238, 52213]]   \n",
       "1  0.994773  335.35    [[47304, 245], [303, 52148]]   \n",
       "1  0.994419  335.69    [[47299, 250], [335, 52116]]   \n",
       "1  0.923209  334.12  [[44352, 3197], [4740, 47711]]   \n",
       "1  0.886145  383.68  [[42089, 5460], [6379, 46072]]   \n",
       "2  0.366219  383.68    [[4279, 1378], [5226, 1908]]   \n",
       "2  0.097336  334.12      [[5363, 294], [6754, 380]]   \n",
       "2  0.068245  335.35      [[5488, 169], [6876, 258]]   \n",
       "2  0.009183  335.69        [[5637, 20], [7101, 33]]   \n",
       "2  0.004188  342.31        [[5643, 14], [7119, 15]]   \n",
       "2  0.000000  342.35          [[5656, 1], [7134, 0]]   \n",
       "\n",
       "                                     model  \n",
       "0  LogisticRegression(C=0.1, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0  LogisticRegression(C=250, max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "0         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=0.1, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1  LogisticRegression(C=250, max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "1         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=250, max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2         LogisticRegression(max_iter=300)  \n",
       "2  LogisticRegression(C=0.1, max_iter=300)  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_title_count\")]),\n",
    "    (unbalanced, \"content_domain\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_domain_count\")]),\n",
    "    (unbalanced, \"content_authors\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_authors_count\")]),\n",
    "    (unbalanced, \"content_domain_authors_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"all_count\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=250), \"all_count_hyper_1\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=0.1), \"all_count_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_meta = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, tests=test_stats_meta)\n",
    "test_stats_meta.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"all_count_hyper_1\"\n",
    "\n",
    "metrics = test_stats_simple.metrics\n",
    "metrics_test = metrics[metrics[\"split\"] == \"test\"]\n",
    "cm = metrics_test[metrics_test[\"name\"] == model_name][\"confusion_matrix\"].values[0] * 100\n",
    "sns.heatmap(cm, annot=True, cmap=\"Blues\", xticklabels=[\"Fake\",\"True\"], yticklabels=[\"Fake\",\"True\"], fmt=\"d\", annot_kws={\"size\": 12})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llrrrrrlll}\n",
      "\\toprule\n",
      "                        name & split &  train\\_acc &      acc &  precision &   recall &       f1 &  time &             confusion\\_matrix &                                   model \\\\\n",
      "\\midrule\n",
      "                   all\\_count &   val &   1.000000 & 0.915000 &   0.918406 & 0.920152 & 0.919278 &  4.07 &       [[431, 43], [42, 484]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &   val &   1.000000 & 0.907000 &   0.912381 & 0.910646 & 0.911513 &  4.97 &       [[428, 46], [47, 479]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &   val &   0.997500 & 0.903000 &   0.911708 & 0.903042 & 0.907354 &  2.61 &       [[428, 46], [51, 475]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &   val &   1.000000 & 0.901000 &   0.905123 & 0.906844 & 0.905983 &  3.18 &       [[424, 50], [49, 477]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &   val &   0.999375 & 0.851000 &   0.845872 & 0.876426 & 0.860878 &  4.05 &       [[390, 84], [65, 461]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &   val &   0.965250 & 0.830000 &   0.815603 & 0.874525 & 0.844037 &  8.01 &      [[370, 104], [66, 460]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &   val &   0.978250 & 0.825000 &   0.805217 & 0.880228 & 0.841054 & 16.49 &      [[362, 112], [63, 463]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &   val &   0.919375 & 0.823000 &   0.822551 & 0.846008 & 0.834114 &  0.56 &       [[378, 96], [81, 445]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &   val &   1.000000 & 0.810000 &   0.808824 & 0.836502 & 0.822430 &  3.55 &      [[370, 104], [86, 440]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &   val &   0.999375 & 0.809000 &   0.820268 & 0.815589 & 0.817922 &  4.24 &       [[380, 94], [97, 429]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &   val &   0.980500 & 0.807000 &   0.817143 & 0.815589 & 0.816365 &  2.56 &       [[378, 96], [97, 429]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &   val &   0.998125 & 0.803000 &   0.802239 & 0.825336 & 0.813623 &  3.83 &      [[373, 106], [91, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_count &   val &   0.998500 & 0.802000 &   0.805970 & 0.821293 & 0.813559 &  5.31 &      [[370, 104], [94, 432]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &   val &   0.838500 & 0.782000 &   0.767361 & 0.840304 & 0.802178 &  0.35 &      [[340, 134], [84, 442]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &   val &   0.998200 & 0.764000 &   0.862319 & 0.666045 & 0.751579 &  2.31 &      [[407, 57], [179, 357]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &   val &   0.999625 & 0.684000 &   0.753589 & 0.596591 & 0.665962 &  3.46 &     [[369, 103], [213, 315]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  test &   1.000000 & 0.914000 &   0.911824 & 0.915493 & 0.913655 &  4.97 &       [[459, 44], [42, 455]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  test &   1.000000 & 0.907000 &   0.908907 & 0.903421 & 0.906155 &  4.07 &       [[458, 45], [48, 449]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  test &   1.000000 & 0.901000 &   0.899598 & 0.901408 & 0.900503 &  3.18 &       [[453, 50], [49, 448]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  test &   0.997500 & 0.901000 &   0.904472 & 0.895372 & 0.899899 &  2.61 &       [[456, 47], [52, 445]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  test &   0.978250 & 0.820000 &   0.781528 & 0.885312 & 0.830189 & 16.49 &      [[380, 123], [57, 440]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  test &   0.919375 & 0.828000 &   0.816764 & 0.843058 & 0.829703 &  0.56 &       [[409, 94], [78, 419]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  test &   0.965250 & 0.822000 &   0.794824 & 0.865191 & 0.828516 &  8.01 &      [[392, 111], [67, 430]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  test &   0.999375 & 0.819000 &   0.829167 & 0.800805 & 0.814739 &  4.05 &       [[421, 82], [99, 398]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  test &   1.000000 & 0.808000 &   0.804391 & 0.810865 & 0.807615 &  3.55 &       [[405, 98], [94, 403]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  test &   0.998500 & 0.806000 &   0.808554 & 0.798793 & 0.803644 &  5.31 &      [[409, 94], [100, 397]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  test &   0.980500 & 0.807000 &   0.817992 & 0.786720 & 0.802051 &  2.56 &      [[416, 87], [106, 391]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  test &   0.838500 & 0.790000 &   0.759494 & 0.845070 & 0.800000 &  0.35 &      [[370, 133], [77, 420]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  test &   0.999375 & 0.802000 &   0.804481 & 0.794769 & 0.799595 &  4.24 &      [[407, 96], [102, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  test &   0.998125 & 0.798000 &   0.804481 & 0.788423 & 0.796371 &  3.83 &      [[403, 96], [106, 395]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  test &   0.998200 & 0.775000 &   0.878173 & 0.661568 & 0.754635 &  2.31 &      [[429, 48], [177, 346]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  test &   0.999625 & 0.695000 &   0.750594 & 0.612403 & 0.674493 &  3.46 &     [[379, 105], [200, 316]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           content\\_tfidf\\_tri &  liar &   0.978250 & 0.549292 &   0.583020 & 0.673816 & 0.625138 & 16.49 & [[2219, 3438], [2327, 4807]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "            content\\_tfidf\\_bi &  liar &   0.965250 & 0.542647 &   0.588699 & 0.597281 & 0.592959 &  8.01 & [[2680, 2977], [2873, 4261]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_2 &  liar &   0.838500 & 0.541005 &   0.592991 & 0.564480 & 0.578384 &  0.35 & [[2893, 2764], [3107, 4027]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "               content\\_tfidf &  liar &   0.919375 & 0.539833 &   0.592747 & 0.559013 & 0.575386 &  0.56 & [[2917, 2740], [3146, 3988]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_tfidf\\_hyper\\_1 &  liar &   1.000000 & 0.520522 &   0.588101 & 0.468321 & 0.521420 &  3.55 & [[3317, 2340], [3793, 3341]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "               content\\_count &  liar &   0.998500 & 0.480807 &   0.609410 & 0.192459 & 0.292532 &  5.31 &  [[4777, 880], [5761, 1373]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "  content\\_count\\_balanced\\_bin &  liar &   0.998125 & 0.477132 &   0.613544 & 0.168909 & 0.264893 &  3.83 &  [[4898, 759], [5929, 1205]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "         content\\_count\\_hyper &  liar &   0.980500 & 0.475803 &   0.624927 & 0.150407 & 0.242458 &  2.56 &  [[5013, 644], [6061, 1073]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "         content\\_title\\_count &  liar &   0.999375 & 0.474318 &   0.618360 & 0.150126 & 0.241597 &  4.24 &  [[4996, 661], [6063, 1071]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      " content\\_count\\_reliable\\_fake &  liar &   0.999625 & 0.470174 &   0.603358 & 0.146061 & 0.235188 &  3.46 &  [[4972, 685], [6092, 1042]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "       content\\_authors\\_count &  liar &   0.999375 & 0.465093 &   0.627622 & 0.100645 & 0.173472 &  4.05 &   [[5231, 426], [6416, 718]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "content\\_count\\_balanced\\_types &  liar &   0.998200 & 0.462044 &   0.625372 & 0.088450 & 0.154980 &  2.31 &   [[5279, 378], [6503, 631]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "        content\\_domain\\_count &  liar &   1.000000 & 0.448440 &   0.665272 & 0.022288 & 0.043130 &  3.18 &    [[5577, 80], [6975, 159]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_1 &  liar &   1.000000 & 0.446642 &   0.637255 & 0.018223 & 0.035432 &  4.97 &    [[5583, 74], [7004, 130]] & LogisticRegression(C=250, max\\_iter=300) \\\\\n",
      "                   all\\_count &  liar &   1.000000 & 0.445939 &   0.659864 & 0.013597 & 0.026645 &  4.07 &     [[5607, 50], [7037, 97]] &        LogisticRegression(max\\_iter=300) \\\\\n",
      "           all\\_count\\_hyper\\_2 &  liar &   0.997500 & 0.444140 &   0.666667 & 0.006728 & 0.013322 &  2.61 &     [[5633, 24], [7086, 48]] & LogisticRegression(C=0.1, max\\_iter=300) \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False).to_latex(index=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector 0 (data read in 2.222195863723755 seconds)\n",
      "Saved vector 0 in 5.955408573150635 seconds\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.0s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=200, max_iter=500; total time=   5.3s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.0s\n",
      "[CV] END ................................C=250, max_iter=500; total time=   6.1s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   6.5s\n",
      "[CV] END ................................C=300, max_iter=500; total time=   5.8s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.4s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   6.6s\n",
      "[CV] END ................................C=350, max_iter=500; total time=   5.9s\n",
      "content_count finished in 77.97 seconds\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>split</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>acc</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "      <th>time</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>content_count</td>\n",
       "      <td>val</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.783000</td>\n",
       "      <td>0.796545</td>\n",
       "      <td>0.788973</td>\n",
       "      <td>0.792741</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>content_count</td>\n",
       "      <td>test</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.776000</td>\n",
       "      <td>0.780287</td>\n",
       "      <td>0.764588</td>\n",
       "      <td>0.772358</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>content_count</td>\n",
       "      <td>liar</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.466891</td>\n",
       "      <td>0.604930</td>\n",
       "      <td>0.127278</td>\n",
       "      <td>0.210307</td>\n",
       "      <td>77.94</td>\n",
       "      <td>GridSearchCV(cv=3, estimator=LogisticRegressio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            name split  train_acc       acc  precision    recall        f1  \\\n",
       "0  content_count   val        1.0  0.783000   0.796545  0.788973  0.792741   \n",
       "1  content_count  test        1.0  0.776000   0.780287  0.764588  0.772358   \n",
       "2  content_count  liar        1.0  0.466891   0.604930  0.127278  0.210307   \n",
       "\n",
       "    time                                              model  \n",
       "0  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "1  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  \n",
       "2  77.94  GridSearchCV(cv=3, estimator=LogisticRegressio...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500]},#[500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(grid, \"content_count\")]),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, X_liar, y_liar) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)\n",
    "# best params\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_files(files, cols_to_test, vec_funcs, tests = None):\n",
    "    if tests == None:\n",
    "        tests = Test_statistic()\n",
    "    for file, name in files:\n",
    "        print(f\"Proccessing: {name}\")\n",
    "        cols_to_read = list(list(zip(*cols_to_test))[0]) + [\"type_binary\", \"set\"]\n",
    "        data = pd.read_csv(file, usecols=cols_to_read)\n",
    "        print(\"Read data into dataframe\")\n",
    "\n",
    "        for col, entry_name in cols_to_test:\n",
    "            for func, model, func_name in vec_funcs:\n",
    "                X_train, X_val, X_test, y_train, y_val, y_test = split_data(data, col, \"type_binary\")\n",
    "                X_train_vec, X_val_vec, X_test_vec = func(X_train, X_val, X_test)\n",
    "                print(f\"Vectorized {entry_name} with {func_name}\")\n",
    "                tests.test_baseline(X_train_vec, X_val_vec, y_train, y_val, name=f\"{entry_name}_{name}_{func_name}\", model=model)\n",
    "    return tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "importlib.reload(pp)\n",
    "\n",
    "def test_on_liar(test, file):\n",
    "    liar_data = pp.apply_pipeline_pd_tqdm(pd.read_csv(file), [(pp.Binary_labels_LIAR(), 'label', 'type_binary')])\n",
    "\n",
    "    metrics = pd.DataFrame()\n",
    "    for row in info_list:\n",
    "        model_name = row[-1]\n",
    "        model = test.metrics[test.metrics[\"name\"] == model_name][\"model\"].values[0]\n",
    "        vectorizer = test.metrics[test.metrics[\"name\"] == model_name][\"vectorizer\"].values[0]\n",
    "        X = vectorizer.transform(liar_data[\"statement_combined\"].values)\n",
    "        #print(liar_data[\"type_binary\"].astype(int).value_counts())\n",
    "        metrics = pd.concat([mt.get_predict_metrics(model, X, liar_data[\"type_binary\"].astype(int), name=model_name), metrics])\n",
    "\n",
    "        \n",
    "    return metrics.sort_values(by=\"f1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File: ../datasets/sample/dataset_unbalanced_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.12475, conspiracy: 0.112, junksci: 0.0165, hate: 0.01025, unreliable: 0.03825, bias: 0.15625, satire: 0.01725, reliable: 0.257, clickbait: 0.03075, political: 0.237\n",
      "True: 2099, Fake: 1901\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.098, junksci: 0.01, hate: 0.008, unreliable: 0.044, bias: 0.174, satire: 0.01, reliable: 0.25, clickbait: 0.044, political: 0.254\n",
      "True: 274, Fake: 226\n",
      "Distribution of test with size 500:\n",
      "fake: 0.134, conspiracy: 0.128, junksci: 0.026, hate: 0.01, unreliable: 0.056, bias: 0.134, satire: 0.012, reliable: 0.244, clickbait: 0.034, political: 0.222\n",
      "True: 250, Fake: 250\n",
      "File: ../datasets/sample/dataset_balanced_types_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.1, conspiracy: 0.1, junksci: 0.1, hate: 0.1, unreliable: 0.1, bias: 0.1, satire: 0.1, reliable: 0.1, clickbait: 0.1, political: 0.1\n",
      "True: 1200, Fake: 2800\n",
      "Distribution of val with size 500:\n",
      "fake: 0.108, conspiracy: 0.112, junksci: 0.006, hate: 0.008, unreliable: 0.05, bias: 0.166, satire: 0.014, reliable: 0.27, clickbait: 0.026, political: 0.24\n",
      "True: 268, Fake: 232\n",
      "Distribution of test with size 500:\n",
      "fake: 0.132, conspiracy: 0.096, junksci: 0.014, hate: 0.01, unreliable: 0.044, bias: 0.158, satire: 0.01, reliable: 0.244, clickbait: 0.022, political: 0.27\n",
      "True: 268, Fake: 232\n",
      "File: ../datasets/sample/dataset_balanced_bin_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.13175, conspiracy: 0.11775, junksci: 0.017, hate: 0.0105, unreliable: 0.0405, bias: 0.165, satire: 0.0175, reliable: 0.18275, clickbait: 0.022, political: 0.16425\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.088, conspiracy: 0.088, junksci: 0.016, hate: 0.008, unreliable: 0.034, bias: 0.138, satire: 0.014, reliable: 0.22, clickbait: 0.038, political: 0.202\n",
      "True: 307, Fake: 193\n",
      "Distribution of test with size 500:\n",
      "fake: 0.122, conspiracy: 0.096, junksci: 0.016, hate: 0.008, unreliable: 0.05, bias: 0.116, satire: 0.006, reliable: 0.214, clickbait: 0.026, political: 0.202\n",
      "True: 293, Fake: 207\n",
      "File: ../datasets/sample/dataset_reliable_fake_cleaned.csv ----------------------------------\n",
      "Distribution of train with size 4000:\n",
      "fake: 0.5, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.5, clickbait: 0.0, political: 0.0\n",
      "True: 2000, Fake: 2000\n",
      "Distribution of val with size 500:\n",
      "fake: 0.306, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.694, clickbait: 0.0, political: 0.0\n",
      "True: 347, Fake: 153\n",
      "Distribution of test with size 500:\n",
      "fake: 0.302, conspiracy: 0.0, junksci: 0.0, hate: 0.0, unreliable: 0.0, bias: 0.0, satire: 0.0, reliable: 0.698, clickbait: 0.0, political: 0.0\n",
      "True: 349, Fake: 151\n"
     ]
    }
   ],
   "source": [
    "def get_distribution(data, is_percentage=True, col = \"type\"):\n",
    "    for i, label in enumerate(pp.labels):\n",
    "        if is_percentage:\n",
    "            percent = len(data[data[col] == label]) / (data.shape[0])\n",
    "        else:\n",
    "            percent = len(data[data[col] == label])\n",
    "        print(f\"{label}: {percent}\", end=\"\")\n",
    "        print(\", \", end=\"\") if i != len(pp.labels) - 1 else _\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_cleaned.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_cleaned.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_cleaned.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_reliable_fake_cleaned.csv\"\n",
    "\n",
    "for file in [unbalanced, balanced_types, balanced_bin, balanced_reliable_fake]:\n",
    "    data = pd.read_csv(file)\n",
    "    print(f\"File: {file} ----------------------------------\")\n",
    "    # find distribution of labels\n",
    "    for i, set_name in enumerate([\"train\", \"val\", \"test\"]):\n",
    "        set = data[data[\"set\"] == i]\n",
    "        print(f\"Distribution of {set_name} with size {set.shape[0]}:\")\n",
    "        get_distribution(set)\n",
    "        print(f\"\\nTrue: {len(set[set['type_binary'] == True])}, Fake: {len(set[set['type_binary'] == False])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
