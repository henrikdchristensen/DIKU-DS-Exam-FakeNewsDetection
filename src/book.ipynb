{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\madsv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "import transformers as ppb # pytorch-pretrained-bert\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import models as ml\n",
    "\n",
    "import importlib"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pp.apply_pipeline(\"../datasets/sample/dataset.csv\", None, get_batch=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:57<00:00, 173.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# freq plot\n",
    "importlib.reload(pp)\n",
    "unique_words = pp.Generate_unique_word_list()\n",
    "_ = pp.apply_pipeline_pd_tqdm(data, [\n",
    "    (pp.Tokenizer(), 'content'),\n",
    "    (unique_words, 'content')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words.plot_most_frequent(50)\n",
    "unique_words.plot_frequency_line(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:00<00:00, 82.92it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(data):\n",
    "    stopwords_lst = stopwords.words('english') + [\"NUM\",\"DATE\",\"URL\",\"EMAIL\"]\n",
    "    cleaned_data = pp.apply_pipeline_pd_tqdm(data, [\n",
    "            (pp.binary_labels(), 'type'),\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "        ])\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "\n",
    "cleaned_data = Clean_data(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:30<00:00, 331.78it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Get_unique_words(cleaned_data, high, low):\n",
    "    unique_words = pp.Generate_unique_word_list()\n",
    "    pp.apply_pipeline_pd_tqdm(cleaned_data, [\n",
    "        (unique_words, \"content\")\n",
    "    ])\n",
    "    return unique_words\n",
    "\n",
    "unique_words = Get_unique_words(cleaned_data, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_words.get_freqs())\n",
    "print(unique_words.get_most_frequent(1000))\n",
    "unique_words.plot_most_frequent(50)\n",
    "unique_words.plot_frequency_line(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words (features): 20924\n"
     ]
    }
   ],
   "source": [
    "unique_words_list = unique_words.get_unique_words(0,1)\n",
    "print(\"words (features):\", len(unique_words_list))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words representation BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:01<00:00, 55.05it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Vectorize_content(cleaned_data, unique_words):\n",
    "    vectors = pp.apply_pipeline_pd_tqdm(cleaned_data, [\n",
    "        (pp.Create_word_vector(unique_words), \"content\")\n",
    "    ])\n",
    "    return vectors\n",
    "\n",
    "\n",
    "vectors = Vectorize_content(cleaned_data, unique_words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_split(data, test_size = 0.4):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(csr_matrix(np.stack(data[\"content\"])), data[\"type\"].astype(int), test_size=test_size, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vector, X_test_vector, y_train_vector, y_test_vector = test_split(vectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words with term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:27<00:00, 359.72it/s]\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(pp)\n",
    "vectors_normalized = pp.apply_pipeline_pd_tqdm(vectors, [(pp.Normalize(), \"content\")])\n",
    "X_train_vector_normalized, X_test_vector_normalized, y_train_vector_normalized, y_test_vector_normalized = test_split(vectors_normalized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TfidfVectorizer performs the following operations:\n",
    "* <strong>Tokenization:</strong> It breaks the text into individual words or tokens.\n",
    "* <strong>Counting:</strong> It counts the number of occurrences of each token in each document.\n",
    "* <strong>Normalization:</strong> It calculates the frequency of each token in each document by dividing the count by the total number of tokens in the document.\n",
    "* <strong>Weighting:</strong> It applies the Tfidf weighting scheme to each token in each document. The Tfidf weight of a token in a document is proportional to its frequency in the document, but inversely proportional to its frequency in the corpus (i.e., the collection of all documents)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  (10000, 18)\n",
      "Vocabulary:  109205  words\n",
      "Shape of vect:  (10000, 109205)\n",
      "Done vectorizing data!\n"
     ]
    }
   ],
   "source": [
    "def Vectorize(file, to_csv_file_name=None):\n",
    "\n",
    "    cleaned_data = pp.apply_pipeline(file, [\n",
    "            (pp.binary_labels(), 'type'),\n",
    "            (pp.Clean_data(), 'content')\n",
    "        ], \n",
    "        get_batch=True, \n",
    "        batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"Shape of data: \", cleaned_data.shape)\n",
    "\n",
    "    #TODO: how to avoid leaking information from the test set? \n",
    "    vect = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "    vect.fit(cleaned_data['content'])\n",
    "    \n",
    "    print(\"Vocabulary: \", len(vect.vocabulary_), \" words\")\n",
    "    content_tfidf = vect.transform(cleaned_data['content'])\n",
    "    print(\"Shape of vect: \", content_tfidf.shape)\n",
    "\n",
    "    vectorized_data = {\n",
    "        \"X\": content_tfidf,\n",
    "        \"y\": cleaned_data['type']\n",
    "    }\n",
    "\n",
    "    if to_csv_file_name != None:\n",
    "        content_tfidf_df = pd.DataFrame(content_tfidf.todense(),columns = vect.get_feature_names_out())\n",
    "        content_tfidf_df.to_csv(to_csv_file_name, index=False)\n",
    "\n",
    "    print(\"Done vectorizing data!\")\n",
    "    return vectorized_data\n",
    "\n",
    "vectorized_data = Vectorize(\"../datasets/sample/dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(vectorized_data[\"X\"], vectorized_data[\"y\"].astype(int), test_size=0.40, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim of tfidf vectorized data:  (6000, 109205) <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Dim of own vectorized data:  (6000, 20924) <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Dim of tfidf vectorized data: \", X_train.shape, type(X_train))\n",
    "print(\"Dim of own vectorized data: \", X_train_vector.shape, type(X_train_vector))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def support_vector_classifier(X_train, X_test, y_train, y_test):\n",
    "    # Define the classifier classes\n",
    "    svc = SVC(kernel='linear')\n",
    "\n",
    "    # Fit the model\n",
    "    svc.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    svc_pred = svc.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    print(\"svc accuracy: \" + str(accuracy_score(y_test, svc_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc accuracy: 0.808\n"
     ]
    }
   ],
   "source": [
    "support_vector_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "svc accuracy: 0.7545\n"
     ]
    }
   ],
   "source": [
    "support_vector_classifier((X_train_vector), (X_test_vector), y_train_vector, y_test_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_neighbors_classifier(X_train, X_test, y_train, y_test):\n",
    "    # Define the classifier classes\n",
    "    k_nearest = KNeighborsClassifier(n_neighbors=15, weights='distance')\n",
    "\n",
    "    # Fit the model\n",
    "    k_nearest.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    k_nearest_pred = k_nearest.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    print(\"k_nearest accuracy:\", accuracy_score(y_test, k_nearest_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_nearest accuracy: 0.46975\n"
     ]
    }
   ],
   "source": [
    "k_neighbors_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_nearest accuracy: 0.55525\n"
     ]
    }
   ],
   "source": [
    "k_neighbors_classifier(X_train_vector, X_test_vector, y_train_vector, y_test_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passive aggresive classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def passive_aggressive_classifier(X_train, X_test, y_train, y_test):\n",
    "    # Define the classifier classes\n",
    "    passive_aggressive = PassiveAggressiveClassifier()\n",
    "\n",
    "    # Fit the model\n",
    "    passive_aggressive.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    passive_aggressive_pred = passive_aggressive.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    print(\"passive_aggressive accuracy:\", accuracy_score(y_test, passive_aggressive_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passive_aggressive accuracy: 0.80425\n"
     ]
    }
   ],
   "source": [
    "passive_aggressive_classifier(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "passive_aggressive accuracy: 0.764\n"
     ]
    }
   ],
   "source": [
    "passive_aggressive_classifier(X_train_vector, X_test_vector, y_train_vector, y_test_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of classes in train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "counts:  {0: 2423, 1: 3577}\n",
      "count freq {0: 0.4038333333333333, 1: 0.5961666666666666}\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "total = sum(counts)\n",
    "print(\"counts: \", dict(zip(unique, counts)))\n",
    "print(\"count freq\", dict(zip(unique, counts/total)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = pp.apply_pipeline(\"../datasets/sample/dataset.csv\", [(pp.binary_labels(), \"type\")], get_batch=True, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All TF 2.0 model weights were used when initializing DistilBertModel.\n",
      "\n",
      "All the weights of DistilBertModel were initialized from the TF 2.0 model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1771 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "def distilBERT(data):\n",
    "    content = data['content']\n",
    "\n",
    "    #Tokenizer and model input\n",
    "    pretrained_weights = 'distilbert-base-uncased'\n",
    "    tokenizer = ppb.DistilBertTokenizer.from_pretrained(pretrained_weights)\n",
    "    model = ppb.DistilBertModel.from_pretrained(pretrained_weights, from_tf=True)\n",
    "\n",
    "    #Tokenize input\n",
    "    tokenized = content.apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    model.eval()\n",
    "\n",
    "    #Pad input so that all sequences are of the same size:\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    padded = padded[:,:32]\n",
    "\n",
    "    # Tell embedding model to disregard pad tokens\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    \n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "      model = model.cuda()\n",
    "      device = torch.device(\"cuda\")\n",
    "\n",
    "    # Convert input to a pytorch tensor\n",
    "    input = torch.tensor(np.array(padded), device=device)\n",
    "    attention_mask = torch.tensor(attention_mask, device=device)\n",
    "\n",
    "    # Embed sequences (processing in batches to avoid memory problems)\n",
    "    batch_size= 200\n",
    "    embeddings = []\n",
    "\n",
    "    for start_index in range(0, input.shape[0], batch_size):\n",
    "      with torch.no_grad():\n",
    "        # Call embedding model\n",
    "        embedding = model(input[start_index:start_index+batch_size], \n",
    "                          attention_mask=attention_mask[start_index:start_index+batch_size])[0][:,0,:]\n",
    "        embeddings.append(embedding)\n",
    "    embeddings = torch.cat(embeddings)   # concatenate all batch outputs back into one tensor\n",
    "\n",
    "    # Move embeddings back to numpy\n",
    "    embeddings = embeddings.cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "  \n",
    "embeddings = distilBERT(data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "multi layer peceptron accuracy: 0.788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\madsv\\miniconda3\\envs\\penguin\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "labels = data_embeddings['type'].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "layers = [1,2,3,4,5]\n",
    "layer_sizes = [2,5,8,11,14]\n",
    "tuple_list = []\n",
    "\n",
    "for layer_size in layer_sizes:\n",
    "    for layer in layers:\n",
    "        tuple_list.append((layer_size,) * layer)\n",
    "        \n",
    "inputs = {'hidden_layer_sizes': tuple_list}\n",
    "\n",
    "# Define the classifier classes\n",
    "MLP = MLPClassifier()\n",
    "\n",
    "\n",
    "#Gridsearch\n",
    "cross_val = GridSearchCV(MLP, inputs, n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "# Fit the model\n",
    "cross_val.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "MLP_pred = cross_val.predict(X_test)\n",
    "\n",
    "print(\"multi layer peceptron accuracy:\", accuracy_score(y_test, MLP_pred))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6000, 109205)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X_train, X_test, y_train, y_test):\n",
    "    # Define the classifier classes\n",
    "    neural_network = MLPClassifier(hidden_layer_sizes=(5000,500,50), max_iter=10000, verbose=True)\n",
    "\n",
    "    # Fit the model\n",
    "    neural_network.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    neural_network_pred = neural_network.predict(X_test)\n",
    "\n",
    "    # Evaluate performance\n",
    "    print(\"neural_network accuracy:\", accuracy_score(y_test, neural_network_pred))\n",
    "\n",
    "neural_network(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "25/25 [==============================] - 1s 14ms/step - loss: 0.7602 - accuracy: 0.4812 - val_loss: 0.6951 - val_accuracy: 0.4950\n",
      "Epoch 2/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6971 - accuracy: 0.5163 - val_loss: 0.6920 - val_accuracy: 0.5050\n",
      "Epoch 3/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6875 - accuracy: 0.5450 - val_loss: 0.6879 - val_accuracy: 0.5450\n",
      "Epoch 4/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.6837 - accuracy: 0.5600 - val_loss: 0.6853 - val_accuracy: 0.5200\n",
      "Epoch 5/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.6760 - accuracy: 0.5650 - val_loss: 0.6792 - val_accuracy: 0.5600\n",
      "Epoch 6/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.6615 - accuracy: 0.6087 - val_loss: 0.6704 - val_accuracy: 0.5700\n",
      "Epoch 7/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.6305 - accuracy: 0.6637 - val_loss: 0.6490 - val_accuracy: 0.6750\n",
      "Epoch 8/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.5838 - accuracy: 0.7188 - val_loss: 0.6269 - val_accuracy: 0.6600\n",
      "Epoch 9/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.5061 - accuracy: 0.8175 - val_loss: 0.5927 - val_accuracy: 0.7000\n",
      "Epoch 10/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.4304 - accuracy: 0.8788 - val_loss: 0.5714 - val_accuracy: 0.7100\n",
      "Epoch 11/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.3614 - accuracy: 0.8863 - val_loss: 0.5401 - val_accuracy: 0.7650\n",
      "Epoch 12/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.3068 - accuracy: 0.8988 - val_loss: 0.5310 - val_accuracy: 0.7350\n",
      "Epoch 13/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2557 - accuracy: 0.9225 - val_loss: 0.5082 - val_accuracy: 0.7650\n",
      "Epoch 14/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.2092 - accuracy: 0.9450 - val_loss: 0.5420 - val_accuracy: 0.7050\n",
      "Epoch 15/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1799 - accuracy: 0.9538 - val_loss: 0.5106 - val_accuracy: 0.7350\n",
      "Epoch 16/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.1610 - accuracy: 0.9500 - val_loss: 0.5095 - val_accuracy: 0.7800\n",
      "Epoch 17/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1332 - accuracy: 0.9712 - val_loss: 0.5084 - val_accuracy: 0.7550\n",
      "Epoch 18/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1292 - accuracy: 0.9588 - val_loss: 0.5166 - val_accuracy: 0.7550\n",
      "Epoch 19/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.1106 - accuracy: 0.9750 - val_loss: 0.5274 - val_accuracy: 0.7550\n",
      "Epoch 20/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0947 - accuracy: 0.9825 - val_loss: 0.5345 - val_accuracy: 0.7600\n",
      "Epoch 21/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0842 - accuracy: 0.9850 - val_loss: 0.5399 - val_accuracy: 0.7700\n",
      "Epoch 22/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0797 - accuracy: 0.9800 - val_loss: 0.5459 - val_accuracy: 0.7900\n",
      "Epoch 23/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0794 - accuracy: 0.9737 - val_loss: 0.6072 - val_accuracy: 0.7300\n",
      "Epoch 24/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0688 - accuracy: 0.9837 - val_loss: 0.5833 - val_accuracy: 0.7750\n",
      "Epoch 25/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0639 - accuracy: 0.9875 - val_loss: 0.5806 - val_accuracy: 0.7850\n",
      "Epoch 26/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0553 - accuracy: 0.9875 - val_loss: 0.6066 - val_accuracy: 0.7650\n",
      "Epoch 27/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0502 - accuracy: 0.9912 - val_loss: 0.6485 - val_accuracy: 0.7300\n",
      "Epoch 28/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0446 - accuracy: 0.9937 - val_loss: 0.6149 - val_accuracy: 0.7800\n",
      "Epoch 29/30\n",
      "25/25 [==============================] - 0s 8ms/step - loss: 0.0410 - accuracy: 0.9937 - val_loss: 0.6187 - val_accuracy: 0.7850\n",
      "Epoch 30/30\n",
      "25/25 [==============================] - 0s 7ms/step - loss: 0.0378 - accuracy: 0.9975 - val_loss: 0.6293 - val_accuracy: 0.7800\n",
      "7/7 [==============================] - 0s 3ms/step - loss: 0.6293 - accuracy: 0.7800\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6293168067932129, 0.7799999713897705]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load dataset\n",
    "df = pp.apply_pipeline(\"../datasets/sample/dataset.csv\", [(pp.binary_labels(), \"type\")], get_batch=True, batch_size=1000)\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(df[\"content\"])\n",
    "sequences = tokenizer.texts_to_sequences(df[\"content\"])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=1000, truncating=\"post\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, df[\"type\"].astype(int), test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=10000, output_dim=16, input_length=1000),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"sigmoid\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=30, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18e8ad9e04aa29451ad941865f411deac86078108f00415026b2da5e6b8d8358"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
