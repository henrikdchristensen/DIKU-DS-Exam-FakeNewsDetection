{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from ast import literal_eval\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import csr_matrix, vstack, load_npz, save_npz\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import torch\n",
    "\n",
    "import pipeline as pp\n",
    "import model_tests as mt\n",
    "\n",
    "import importlib\n",
    "import math\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you should be very careful with choosing the correct file."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preproccessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Covert types to binary labels in LIAR - either True (reliable) or False (fake news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/large/cleaned_file.csv\", \n",
    "    [(pp.Binary_labels_LIAR(), 'type', 'type_binary')], \n",
    "    new_file=\"../datasets/large/cleaned_file_bin.csv\", \n",
    "    progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the follwoing input files:\n",
    "* All are unbalanced\n",
    "* The test and validation set are balanced according to the types (e.g. satire, reliable...), and the test set is unbalanced\n",
    "* The test and validation set are balanced according to the binary classes, and the test set is unbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of rows to train the model\n",
    "BATCH_SIZE = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "from_file = \"../datasets/large/cleaned_file.csv\"\n",
    "\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.1,0.1,0.1], [False, False, False], \n",
    "                                    out_file=\"../datasets/large/dataset_unbalanced_100k.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.5 ,0.1,0.1], [True, False, False], \n",
    "                                   out_file=\"../datasets/sample/dataset_balanced_types.csv\", get_frame=False)\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False],\n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_bin.csv\", get_frame=False, classes=[True,False], type_col=\"type_binary\")\n",
    "pp.get_dataframe_with_distribution(from_file, BATCH_SIZE, [0.8,0.1,0.1], [True, False, False], \n",
    "                                    out_file=\"../datasets/sample/dataset_balanced_reliable_fake.csv\", get_frame=False, classes=[\"reliable\", \"fake\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the files. This may have already been done by running some of the other notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(pp)\n",
    "\n",
    "def Clean_data(file, new_file):\n",
    "    stopwords_lst = stopwords.words('english')\n",
    "    pp.apply_pipeline(file, [\n",
    "            # binary labels\n",
    "            (pp.Binary_labels(), 'type', 'type_binary'),\n",
    "            # Clean content\n",
    "            (pp.Clean_data(), 'content'),\n",
    "            (pp.Tokenizer(), \"content\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"content\"),\n",
    "            (pp.Stem(), \"content\"),\n",
    "            (pp.Combine_Content(), \"content\", \"content_combined\"),\n",
    "            # Clean authors\n",
    "            (pp.Clean_author(), \"authors\"),\n",
    "            # Clean title\n",
    "            (pp.Clean_data(), 'title'),\n",
    "            (pp.Tokenizer(), \"title\"),\n",
    "            (pp.Remove_stopwords(stopwords_lst), \"title\"),\n",
    "            (pp.Stem(), \"title\"),\n",
    "            (pp.Combine_Content(), \"title\"),\n",
    "            # Clean domain\n",
    "            (pp.Clean_domain(), 'domain'),\n",
    "            # Combine columns (used as features)\n",
    "            (pp.Join_str_columns([\"content_combined\", \"authors\"]), None, \"content_authors\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"title\"]), None, \"content_title\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\"]), None, \"content_domain\"),\n",
    "            (pp.Join_str_columns([\"content_combined\", \"domain\", \"authors\", \"title\"]), None, \"content_domain_authors_title\")\n",
    "        ],\n",
    "        new_file=new_file,\n",
    "        progress_bar=True,\n",
    "    )\n",
    "\n",
    "#Clean_data(\"../datasets/sample/dataset_unbalanced.csv\", \"../datasets/sample/dataset_unbalanced_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_types.csv\", \"../datasets/sample/dataset_balanced_types_cleaned.csv\")\n",
    "#Clean_data(\"../datasets/sample/dataset_balanced_bin.csv\", \"../datasets/sample/dataset_balanced_bin_cleaned.csv\")\n",
    "Clean_data(\"../datasets/sample/dataset_reliable_fake.csv\", \"../datasets/sample/dataset_reliable_fake_cleaned.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete nans for the combined columns. This is nescesarry for some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete nans\n",
    "pp.apply_pipeline(\n",
    "    \"../datasets/sample/dataset_unbalanced_1M.csv\",\n",
    "    [(pp.Delete_nan(), 'content_title'),\n",
    "     (pp.Delete_nan(), 'content_domain'),\n",
    "     (pp.Delete_nan(), 'content_authors'),\n",
    "     (pp.Delete_nan(), 'content_domain_authors_title')],\n",
    "     new_file=\"../datasets/sample/dataset_unbalanced_1M_.csv\",\n",
    "     progress_bar=True\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the logistic model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting liar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data = pd.read_csv(\"../datasets/liar_dataset/cleaned/combined_cleaned.csv\")\n",
    "X_liar =  liar_data[\"statement_combined\"].values\n",
    "y_liar = liar_data[\"label_binary\"].astype(int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing models (other than logistic) - may take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "info_list = [(\n",
    "    \"../datasets/large/dataset_unbalanced_1M.csv\", \"content_no_swords_combined\", mt.create_count_vector, [\n",
    "        (MultinomialNB(), \"naive_bayes\"),\n",
    "        (RandomForestClassifier(max_depth=5), \"random_forest\"), #25\n",
    "        (DecisionTreeClassifier(max_depth=2), \"decision_tree\"),\n",
    "        (AdaBoostClassifier(n_estimators=2), \"ada_boost\"), #2\n",
    "        (SVC(kernel='linear', max_iter=10), \"svm\"),\n",
    "        (KNeighborsClassifier(n_neighbors=2, algorithm='kd_tree'), \"knn\"), #15\n",
    "        (PassiveAggressiveClassifier(), \"passive_aggressive\")\n",
    "        ])\n",
    "]\n",
    "\n",
    "test_stats_base = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/large/dataset_count_vectors.pickle\", info_list, tests=test_stats_base)\n",
    "test_stats_base.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing logistic model on different label distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/large/dataset_unbalanced_1M.csv\"\n",
    "balanced_types = \"../datasets/sample/dataset_balanced_types_1M.csv\"\n",
    "balanced_bin = \"../datasets/sample/dataset_balanced_bin_1M.csv\"\n",
    "balanced_reliable_fake = \"../datasets/sample/dataset_balanced_reliable_fake_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count\"), \n",
    "                                                              (LogisticRegression(max_iter=300, C=0.1), \"content_count_hyper\"),\n",
    "                                                              (LogisticRegression(max_iter=300, C=250), \"content_count_hyper\")]),\n",
    "    (balanced_types, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_types\")]),\n",
    "    (balanced_bin, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_balanced_bin\")]),\n",
    "    (balanced_reliable_fake, \"content_combined\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_count_reliable_fake\")]),\n",
    "]\n",
    "\n",
    "test_stats_simple = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors.pickle\", info_list, tests=test_stats_simple)\n",
    "test_stats_simple.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing logistic models with tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "info_list = [\n",
    "      (\"../datasets/large/dataset_unbalanced_1M.csv\", \"content_combined\", mt.create_tdfidf_vector_unigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_uni\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_uni_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_uni_hyper_2\")]),\n",
    "     (\"../datasets/large/dataset_unbalanced_1M.csv\", \"content_combined\", mt.create_tdfidf_vector_bigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_bi\")]),\n",
    "     (\"../datasets/sample/dataset_unbalanced_10K.csv\", \"content_combined\", mt.create_tdfidf_vector_trigram, [\n",
    "        (LogisticRegression(max_iter=300), \"content_tfidf_tri\"),\n",
    "        (LogisticRegression(max_iter=300, C=250), \"content_tfidf_tri_hyper_1\"),\n",
    "        (LogisticRegression(max_iter=300, C=0.1), \"content_tfidf_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_tdidf = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_tdidf_vectors.pickle\", info_list, tests=test_stats_tdidf)\n",
    "test_stats_tdidf.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing baseline model with meta fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced_1M.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_title_count\")]),\n",
    "    (unbalanced, \"content_domain\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_domain_count\")]),\n",
    "    (unbalanced, \"content_authors\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"content_authors_count\")]),\n",
    "    (unbalanced, \"content_domain_authors_title\", mt.create_count_vector, [(LogisticRegression(max_iter=300), \"all_count\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=250), \"all_count_hyper_1\"), \n",
    "                                                                          (LogisticRegression(max_iter=300, C=0.1), \"all_count_hyper_2\")]),\n",
    "]\n",
    "\n",
    "test_stats_meta = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, X_liar, y_liar)\n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/dataset_count_vectors_meta.pickle\", info_list, tests=test_stats_meta)\n",
    "test_stats_meta.metrics.sort_values(by=[\"split\",\"f1\"], ascending=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"content_count_hyper\"\n",
    "\n",
    "metrics = test_stats_simple.metrics\n",
    "metrics_test = metrics[metrics[\"split\"] == \"test\"]\n",
    "cm = metrics_test[metrics_test[\"name\"] == model_name][\"confusion_matrix\"].values[0]\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "sns.heatmap([[tp, fn],[fp, tn]], annot=True, cmap=\"Blues\", xticklabels=[\"Fake\",\"True\"], yticklabels=[\"Fake\",\"True\"], fmt=\"d\", annot_kws={\"size\": 12})\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter tuning - the best found was C=300 and max_iter=700. The code down below takes around 5 hours to run for 1M entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(mt)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator  = LogisticRegression(),\n",
    "    param_grid = {\"C\": [200, 250, 300, 350], \"max_iter\": [500]},#[500, 600, 700, 800]},\n",
    "    cv         = 3,\n",
    "    scoring    = ['f1'],\n",
    "    refit      = 'f1',\n",
    "    verbose    = 2\n",
    ")\n",
    "\n",
    "unbalanced = \"../datasets/sample/dataset_unbalanced.csv\"\n",
    "\n",
    "info_list = [\n",
    "    (unbalanced, \"content_combined\", mt.create_count_vector, [(grid, \"content_count\")]),\n",
    "]\n",
    "\n",
    "test_stats_hyper_opt = mt.Test_statistic()\n",
    "\n",
    "mt.create_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, X_liar, y_liar) \n",
    "mt.test_vectors_from_infolist(\"../datasets/sample/hyper_opt.pickle\", info_list, tests=test_stats_hyper_opt)\n",
    "test_stats_hyper_opt.metrics.sort_values(by=\"f1\", ascending=False)\n",
    "# best params\n",
    "print(grid.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "penguin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
